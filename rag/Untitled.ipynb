{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97a498e-65e7-4a27-90de-3f19fa925753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24541\\AppData\\Local\\uv\\cache\\archive-v0\\aN1IsAeqbfnq70OlXzgs5\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7509250b-17b0-4edb-8c55-c8bfd62eb1a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 你已经完成的流程：chunks + embeddings + chroma + embedding_model + cross_encoder + gemini_client 都在前面初始化了\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 你已经完成的流程：chunks + embeddings + chroma + embedding_model + cross_encoder + gemini_client 都在前面初始化了\n",
    "from typing import List\n",
    "\n",
    "def split_into_chunks(doc_file: str) -> List[str]:\n",
    "    with open(doc_file, 'r',encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return [chunk for chunk in content.split(\"##\")]\n",
    "\n",
    "# 主程序部分\n",
    "chunks = split_into_chunks(\"README.md\")\n",
    "\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"[{i}] {chunk}\\n\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    embedding = embedding_model.encode(chunk)\n",
    "    return embedding.tolist()\n",
    "\n",
    "test_embedding = embed_chunk(\"test\")\n",
    "print(len(test_embedding))\n",
    "print(test_embedding)\n",
    "\n",
    "embeddings = [embed_chunk(chunk) for chunk in chunks]\n",
    "print(len(embeddings))\n",
    "print(embeddings[0])\n",
    "\n",
    "import chromadb\n",
    "\n",
    "chromadb_client = chromadb.EphemeralClient()\n",
    "chromadb_collection = chromadb_client.get_or_create_collection(name=\"default\")\n",
    "\n",
    "def save_embeddings(chunks:List[str], embeddings:List[List[float]]) -> None:\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    chromadb_collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "save_embeddings(chunks,embeddings)\n",
    "\n",
    "def retrieve(query: str, top_k: int) -> List[str]:\n",
    "    query_embedding = embed_chunk(query)\n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"distances\"]  # 建议带上距离，方便观察\n",
    "    )\n",
    "    return results['documents'][0]\n",
    "\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query:str, retrieved_chunks:List[str],top_k:int) -> List[str]:\n",
    "    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    pairs=[(query,chunk) for chunk in retrieved_chunks]\n",
    "    scores=cross_encoder.predict(pairs)\n",
    "\n",
    "    chunk_with_score_list=[(chunk,score)\n",
    "                           for chunk,score in zip(retrieved_chunks, scores)]\n",
    "    chunk_with_score_list.sort(key=lambda pair:pair[1],reverse=True)\n",
    "    return [chunk for chunk,_ in chunk_with_score_list][:top_k]\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "google_client = genai.Client()\n",
    "\n",
    "def generate(query: str, chunks: List[str]) -> str:\n",
    "    relevant_info = \"\\n\\n\".join(chunks)\n",
    "\n",
    "    prompt = f\"\"\"You're a knowledge assistant, please answer questions according to the user's request and the following information.\n",
    "\n",
    "User's question:\n",
    "{query}\n",
    "\n",
    "Relevant information:\n",
    "{relevant_info}\n",
    "\n",
    "Please answer according to the information mentioned above. Do not make up information.\"\"\"\n",
    "\n",
    "    print(f\"{prompt}\\n\\n---\\n\")\n",
    "\n",
    "    response = google_client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def rag_agent_fn(state: Dict[str, str]) -> Dict[str, str]:\n",
    "    query = state[\"input\"]\n",
    "\n",
    "    # 检索\n",
    "    retrieved_chunks = retrieve(query, top_k=5)\n",
    "\n",
    "    # rerank\n",
    "    reranked_chunks = rerank(query, retrieved_chunks, top_k=3)\n",
    "\n",
    "    # 生成答案\n",
    "    answer = generate(query, reranked_chunks)\n",
    "\n",
    "    return {\"response\": answer}\n",
    "\n",
    "rag_agent = RunnableLambda(rag_agent_fn)\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(dict)\n",
    "graph_builder.add_node(\"rag_agent\", rag_agent)\n",
    "graph_builder.set_entry_point(\"rag_agent\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "result = graph.invoke({\"input\": \"How to use metatdenovo?\"})\n",
    "print(result[\"response\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a5433-1e49-401e-bc68-29b36760cb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
