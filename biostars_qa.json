[
  {
    "url": "https://www.biostars.org/p/9612990/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612993/",
    "title": "Best way to install R packages on an HPC?",
    "question": "Hi everyone,\nI have been trying to practice single cell analysis using R, and I realized that most of my time goes to installing packages using BiocManager::install or install.packages. A single package can take as long as 45 minutes.\nI am using an HPC to do this. When I've tried using mamba install to install packages, I always run into the error:\nEnvironmentNotWritableError: The current user does not have write permissions to the target environment.\nDoes anyone else know how to quickly install R packages on an HPC, or a way around this \"not writable\" error?\nThank you.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612992/",
    "title": "How do I generate a Manhattan plot like this?",
    "question": "Hi all,\nI'm trying to generate a Manhattan plot similar to this one I found online: https://plottie.art/plots/5372\nIt looks cleaner than the default ones I’ve seen — especially in terms of layout, significance threshold lines, and how the top SNPs are annotated.\nI’m wondering:\nWhat R or Python packages can produce plots like this?\nAny tips on customizing axis ticks, colors by chromosome, or labeling top hits?\nWould appreciate any guidance or example code. Thanks!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/225812/",
    "title": "The Biostar Handbook. A bioinformatics e-book for beginners.",
    "question": "The Biostar Handbook collection now includes five volumes of straight-up, no-nonsense data analysis from the trenches!\nWhat's new:\nMarch 2023: The Biostar Workflows has been expanded . Added the 10-minute RNA-Seq chapters.\nPast changes:\nNovember 2022: The Art of Bioinformatics scripting has been modernized.\nApril 2022: A new volume: Biostar Workflows in the seriers. Learn how automate bioinformatics analysis\nMarch 2021: A new volume Coronavirus Genome Analysis. Learn about viral genome analysis.\nJanuary 2020: A new volume RNA-Seq by Example. Detailed step by step guides to RNA-Seq.\nMay 2019: A new volume **The Art of Bioinformatics scripting released. Lear how to script like a pro.\nJanuary 2018: A rewrite of the Biostar Handbook. New installation instructions, many chapters rewritten.\nAll new content is always included with the subscription.\nDecember 1st, 2016: Original Announcement\nAnnounced almost 18 months ago, the Biostar Handbook has now been published. It delivers simple, concise, and relevant information for those looking to understand the field of bioinformatics as a data science.\nIt is a comprehensive, practical handbook that aims to cover (though it is not quite there yet) all major application areas of bioinformatics.\nRead the Biostar Handbook\nGet the Biostar Handbook\nSpecial thanks go to Biostar users GenoMax , shenwei356 and Jeremy Leipzig who have contributed entire pages or sections to the book.\nOnly now that the book released - as I am looking at 713 pages of do I start to realize just how big Bioinformatics has gotten in the past few years. And we're still missing entire subdomains of it: Metagenomics, Assembly, ChIP-Seq. But fear not we'll handle those too in this coming year.\nSpread the word, let others know - I think there is no other resource like it. I like to call it data analysis with attitude, where reproducibility means not following letter by letter, but doing it better, faster and simpler.\nLet me invite anyone that wishes to contribute to do so. It is easy, and simple, Markdown based publishing. And there is so much more that could be done and will be done. Be a part of it! We are independent, self published, self supported. Chart your own course, bring your own ideas and goals to fruition or just enjoy being a part of a creative process.",
    "answers": [
      "A huge metagenomics project in my Lab failed just because my boss refused to employ a bioinformatician. Of course I didn't know anything in bioinformatics apart from the word itself, so I couldn't save the project.\nThen I decided, even if it means life or death, am gonna settle down and learn bioinformatics. Yes, I got started, and day by day, my confidence is growing steadily thanks to \"The Biostar Handbook\" which I purchased 3 months ago. I think we need to add something like \"Teach Yourself Bioinformatics\" to the title of this book, because for sure, that's what am doing with this book. I would highly recommend it for anyone passionate about starting a career in bioinformatics.\nAnd I must say that I would even pay more for it because the information in there is not just worth those peanuts you pay to get the book. It's worth a career! And now there is also this huge Biostar community with answers to almost every question I as a starter may have, and you get these answers instantly as if there is a robot sitting on the other side with answers ready to respond to your questions. This gives you the feeling that not matter what, you will always get the help you need.\nThen there are these guys complaining that the \"book should have been for free\", and those claiming that the \"Biostar community is diminishing the power of Bioinformatics cores\". Come on guys, give us a break. You buy a book for only and only $35 and enjoy all those privileges for two good years and you can't say thank you? This is ridiculous!",
      "Congratulations! I've been using Biostars for past 6+ months so I just bought the book. I'm intermediate-savvy with bioinformatics (I can usually figure out what to do if I search on Biostars or Google but I don't know all the codes off my head) so this book will be great in getting my skills up-to-date.\nI already like that you included a section of Unix commands - that's something a lot of bioinformatic workshops don't go in enough depths about.\nWhat would be the best way for contributing? Does GitBook work like Github? I think it would be great if there is a section on maybe WGCNA (or at least mention it). Maybe this is not applicable to everyone but majority of people I've talked to (and many are wet-lab biologists) really want to be able to take RNA-seq data and put into WGCNA but tutorials available are pretty abstract (UCLA tutorial requires some experiences, for instance).\nOverall an amazing tool! It's like an early Christmas gift :-)",
      "There is a GitHub repository that tracks issues, problems and suggestions for the book.\nIf you experience any type of technical problem please visit the site below to create a new issue.\nhttps://github.com/biostars/biostar-handbook-issues/issues",
      "I've been following this for the last couple of months and really glad to see it released! It was an instant buy for me as a wet-lab phd student interested in bioinformatics. I was wondering if it is possible to supply an epub version alongside mobi for those without kindle e-readers? Also is there a plan to perhaps have an online repository on github or similar for code examples and workflows from the book? This could be similar to Vince Buffalo's Bioinformatics Data Skills book. Overall I can't wait to get stuck into reading it, i'm sure it will save me and many others hours of time searching for help online, so congratulations and thanks again to all involved for all of the hard work putting this together!",
      "I am the first person in my lab to attempt bioinformatics analysis using command line (with zero training). This book has been extremely helpful, covering some important basic concepts and moves on the complex & practical analysis.\nThe most brilliant thing about this book, is that it is unbelievably fun to read! People in the office caught me laughing out loud while 'studying' bioinformatics several times. The author has a very interesting character! He is passionate about what he does, while occasionally pokes fun of the irrational reality. It really feels like having a tutor telling me things beyond the textbook. Thank you for making me clueless attempt surprising enjoyable.",
      "February, 2020: RNA-Seq by Example\nA new book has been released in the Biostar Handbook series:\nRNA-Seq by Example\nA step by step guide through the process of performing an RNA-Seq data analysis.\nAs always all new content is included with the subscription.",
      "March, 2020: Coronavirus Genome Analysis\nA new book has been released in the Biostar Handbook series:\nCoronavirus Genome Analysis\nThe book introduces readers to the practical aspects of investigating data from a viral outbreak.",
      "March 2023: Major update to Biostar Workflows\nAnd lo, a new update to the Biostar Workflows was revealed, and it didst contain wondrous code for functional analysis and enrichment, along with automated workflows to ease the burden of the diligent researcher.\nThe new edition includes reproducible data analysis workflows that allow you to perform an RNA-Seq analysis in just 10 minutes! That's right! You'll be a scientist on the go, 10 minutes from anywhere - just like in \"Oh Brother, Where Art Thou?\" - minus the hair pomade, of course.\n• In the Airway RNA-Seq in 10 minutes we take a published research paper and demonstrate that a small subset of the sequencing data can perfectly reproduce the gene expression effect of dexamethasone on smooth airway muscles\n• In contrast, in the Presenilin RNA-Seq in 10 minutes we can run the same standardized workflow on a study for Alzheimer's disease; alas we find that the results are not all that reproducible.\nAs usual, each step is meticulously documented and explained.\nAnd that's not all - the Art of Bioinformatics Scripting has been completely reformulated and typeset, making it an even greater value and resource for your career.",
      "This looks great, impressed to see chapters dedicated to things that you can only usually get a handle on from hours or days of google-fu (was looking forward to checking out sratools, entrez and bioawk).\nEDIT: I originally had a comment on empty sections, but I'm throwing it out because it's totally misleading. I was totally wrong, and as Istvan pointed out I've skipped straight over the section on analysis and inadvertently found myself in the tools section.",
      "In https://read.biostarhandbook.com/ontology/sequence-ontology.html page, URL=https://raw.githubusercontent.com/The-Sequence-Ontology/SO-Ontologies/master/so-xp-simple.obo curl $URL > so.obo doesn't work. There is no file in the URL. Please edit this in the next update version. :)",
      "Well done Istvan for this great job. It's absolutely a valuable resource for current and future students! but why not delivering a course on Coursera or edX based on this book!",
      "In the Gene ontology section of this book, there is no file in the locate: http://geneontology.org/gene-associations/gene_association.goa_human.gz Please check, thanks.",
      "Is correct to try to cite the handbook? If yes, how would I cite it?\nBWT, what about de novo transcriptome assembly? Would be great to include a section for that.\nFor fastq-dump, I see that the Handbook uses version 2.5.2. I remember a notice from SRA-Toolkit's github page saying that one should upgrade because of the https update on the NCBI platform. I see also that the \"--split-files\" option for paired-end data is used, what about \"--split-3\" option? I have noticed some subtle differences (and better results for using Trinity) when I use \"--split-3\" option instead of \"--split-files\" option.\nThanks in advance.",
      "The book is great, but I would like to have some information related to ChIP- seq as well",
      "I just purchased the book, but I have Windows 7. Can anyone help me download the right programs needed to work through the book that are available on Windows 7?",
      "Great book!\nIn the section How to visualize genomic variation (What would realistic and good data look like?) it says that the script simulate-experimental-data.sh will generate a file called results.bam. It actually generates a file called align.bam.\nIn the section Variant effect prediction (How do I use snpEff?) the link http://data.biostarhandbook.com/variant/find-ebola-variants.sh results in a file not found error. Please check, thanks.",
      "November 1st, 2019: The Art of Bioinformatics Scripting\nThe Biostar Handbook has grown huge :-) it is now close to 1000 pages! To manage this complexity we have started reworking the various chapters into independent books.\nThe reorganization will allow readers to more easily locate the information that they need. It will allow us to design and formulate specific training plans that are customized to specific needs. The first book section that has been reworked covers Unix Scripting and is titled:\nThe Art of Bioinformatics Scripting\nAs always all new content is included with your subscription.",
      "April 2022: Biostar Workflows\nA new book has been released in the Biostar Handbook series:\nBiostar Workflows\nThe book presents bioinformatics automation in the context of modular makefiles.\nSeveral published analyses have been documented and solved via the modular structure.\nThe Biostar Handbook collection now includes five different volumes, as always everything is included in a single subscription.",
      "Has Biostar now become a sales and advertising platform for 'for-profit' books and guides?\nIs Biostar the proper forum for a book, that by the sound of it, is based on the free content that is found without charge in this educational platform. Although, I applaud the efforts of the writers and editors of this book to aggregate the material for the book, I am questioning the idea of having a booklet that is a pet project for some being advertised on such an open source environment.",
      "This is amazing! Coming from software engineering, I'm pretty new to this \"bio\" part of bioinformatics. I'm going to buy it as soon as I can,",
      "thanks a lot for this amazing book and i am enjoying reading the online version. i want to know if possible to add some chapter such as :\nstudy of microrna-seq in model and non model organism.\nstudy of RNA-seq in non model organism.\nthanks a lot for your hard work.",
      "I love it! Thanks for an amazing book - and the courses in bioinformatics and python!!!\nWhen following the courses, is there a forum/thread to discuss the different lectures and topics?",
      "What are the pre-requisites to be able to use the book (and the training) for self-teaching?",
      "Hi guys, the book is great! Is there any plan to extend the metagenomic part? It would be nice to include other tools and approaches for both wgs and 16S metagenome In case you can suggest other tutorials outside the book I would also be happy to go over them! Thank you!",
      "Hi, The Biostar Handbook is one of the best sources I have encountered in my long journey as a molecular biologist transitioning almost full-time to the field of bioinformatics. I am very interested to know you opinion about the Reactome database, I noticed that it is not covered as one of the tools for functional analysis and pathway analysis. I have used the online database https://reactome.org and recently I learned of the ReactomePA R package and I find it to be extremely user friendly but also most importantly comprehensive including several model organisms. It would be helpful for me if you could comment on your opinion on this particular database. Thank you!",
      "This book has been an amazing resource for me. I would love to have updates that would include the analysis of Ribo-Seq data or usage of python packages. Biopython, anaconda, conda, miniconda etc. Some suggestions are:\nuORF-Tools https://uorf-tools.readthedocs.io/en/latest/index.html\nplastid https://plastid.readthedocs.io/en/latest/#\nThank you for your consideration."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612965/",
    "title": "ATAC seq: from peaks to differential analysis ??",
    "question": "Hi,\nI’m fairly new to ATAC-seq and have successfully run MACS2 separately for each of my samples. I now have individual *.narrowPeak files as output.\nMy experimental design looks like this:\nSample_ID     Cell_type     Condition     Donor\n\nSample_1      T_cells       Tumor         Donor_1\nSample_2      T_cells       Normal        Donor_1\nSample_3      T_cells       Tumor         Donor_2\nSample_4      T_cells       Normal        Donor_2\n...\nSample_11     Dendritics    Tumor         Donor_10\nSample_12     Dendritics    Normal        Donor_10\nSample_13     Dendritics    Tumor         Donor_11\nSample_14     Dendritics    Normal        Donor_11\nAs you can see, I have two cell types (T_cells and Dendritics), and for each donor, I have paired Tumor and Normal samples.\nMy goal is to perform a differential accessibility analysis (Tumor vs Normal), accounting for both Donor and Cell_type. I’m also interested in comparing Tumor (T_cells) vs Tumor (Dendritics).\nI heard that it is possible to use DESeq2 for ATAC-seq data, so my design will look like this: https://bioconductor.org/packages/3.21/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#group-specific-condition-effects-individuals-nested-within-groups\nI have a few questions:\n1) Can I use the exact same code from the vignette, as typically done for RNA-seq data? Or are there any parameters or steps specific to ATAC-seq that I should consider?\n2) I’m struggling with how to convert my individual *.narrowPeak files into a count matrix. Do you have any recommendations or tools to help with this step?\n3) Are there alternative methods to DESeq2 that would be better suited for this kind of analysis? I guess limma should work the same no ?\nThank you in advance for your help !",
    "answers": [
      "2:\n# make saf file:\nawk 'OFS=\"\\t\" {print $1\":\"$2+1\"-\"$3, $1, $2+1, $3, \"+\"}' ${sample}_peaks.narrowPeak > featureCounts_peaks.saf\n\n$featureCounts -a featureCounts_peaks.saf \\\n    -F SAF \\\n    --read2pos 5 \\\n    -p \\\n    -o ${peak_all_dir}${sample}_countMatrix.txt ${rmBL_dir}*.bam\n${rmBL_dir}*.bam: all bam files for generating narrowPeaks\n3: DEseq2 is oke. design matrix and param depends on research question, not on the ATAC-seq or RNA-seq as they are all readcount type",
      "I also recommend converting the peak to SAF file as QX shows, although I just count fragments centered on cut site. There's little difference either way though.\nFor more guidance on creating a count matrix, you do need to create a common set of peaks first. Two main ways to do this, either take the intersection of peaks present in all samples or the union.\nI use bedtools, e.g.\n# N just represents number of total samples.\nbedtools mutliinter -i ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | awk '$4 == N' | bedtools sort -i - | bedtools merge -i - > All_Samples_Intersection.bed\nOr,\ncat  ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | bedtools sort -i - | bedtools merge -i - > All_Samples_Union.bed\nThen you can convert that bed into SAF and use featurecounts to count reads from each sample and treat similar to RNA-seq.\nOne thing I would suggest, I usually use a low count filter before moving forward with DE analysis. With ATAC-seq, there's usually more noise (I use relatively lax peak calling), so I tend to use a higher count threshold."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612986/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9595989/",
    "title": "Cut&Run replicates handling",
    "question": "Is there a CUT&RUN peakcaller that supports replicates? If not, what is your recommended approach for handling replicates?\nIt appears that all available peak callers, such as SEACR, are designed to call peaks from single pull-down experiments.\nOur Experiment: We have two conditions, Untreated (UTR) and Treatment (TREAT), each with two biological replicates. Each condition also has respective INPUT data for normalization.\nWe have completed:\nQuality Control (QC)\nAlignment\nDuplicate marking/removal\nSpike-in Calibration\nNext, we would like to proceed to peak calling and differential analysis. We would appreciate your recommendations on suitable peak callers and the best practices for handling replicates.",
    "answers": [
      "You can use nf-core cutandrun pipeline, it supports replicates in the step of consensus peaks calculations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612981/",
    "title": "Unable to extract count matrices from multi-layered Seurat object",
    "question": "Hello, I have a merged R object with 18 samples\nall_combined <- merge(\n  sample_1MI1_so,\n  y = c(\n    sample_1MI3_so,\n    sample_2MI1_so,\n    sample_2MI3_so,\n    sample_3C_MI2_so,\n    sample_3MI1_so,\n    sample_3MI3_so,\n    sample_4D_MI2_so,\n    sample_4MI1_so,\n    sample_4MI3_so,\n    sample_5MI1_so,\n    sample_5MI3_so,\n    sample_6MI1_so,\n    sample_6MI3_so,\n    sample_7MI1_so,\n    sample_7MI3_so,\n    sample_8MI1_so,\n    sample_8MI3_so\n  ),\n  add.cell.ids = c(\n    \"sample_1MI1_so\",\n    \"sample_1MI3_so\",\n    \"sample_2MI1_so\",\n    \"sample_2MI3_so\",\n    \"sample_3C_MI2_so\",\n    \"sample_3MI1_so\",\n    \"sample_3MI3_so\",\n    \"sample_4D_MI2_so\",\n    \"sample_4MI1_so\",\n    \"sample_4MI3_so\",\n    \"sample_5MI1_so\",\n    \"sample_5MI3_so\",\n    \"sample_6MI1_so\",\n    \"sample_6MI3_so\",\n    \"sample_7MI1_so\",\n    \"sample_7MI3_so\",\n    \"sample_8MI1_so\",\n    \"sample_8MI3_so\"\n  ),\n  merge.data = TRUE\n)\nView object\nall_combined\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 55 layers present: counts.1, counts.2, counts.3, counts.4, counts.5, counts.6, counts.7, counts.8, counts.9, counts.10, counts.11, counts.12, counts.13, counts.14, counts.15, counts.16, counts.17, counts.18, data.1, scale.data.1, data.2, scale.data.2, data.3, scale.data.3, data.4, scale.data.4, data.5, scale.data.5, data.6, scale.data.6, data.7, scale.data.7, data.8, scale.data.8, data.9, scale.data.9, data.10, scale.data.10, data.11, scale.data.11, data.12, scale.data.12, data.13, scale.data.13, data.14, scale.data.14, data.15, scale.data.15, data.16, scale.data.16, data.17, scale.data.17, data.18, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nIn the all_combined object, I', trying to extract the counts matrices from it, but I'm not sure how to do this? Below is what I've tried\nall_combined_count_matrix <- LayerData(object = all_combined, assay = \"RNA\", layer = \"counts\")\nWarning: multiple layers are identified by counts.1 counts.2 counts.3 counts.4 counts.5 counts.6 counts.7 counts.8 counts.9 counts.10 counts.11 counts.12 counts.13 counts.14 counts.15 counts.16 counts.17 counts.18\n only the first layer is used\n\n# only 1st sample counts matrix is extracted....want to extract all 18 counts matrices...all_combined object contains 18 seurat objects merged into one\n\n`\nManually combine all 18 count matrices into one\n# List all layers that start with \"counts\"\nall_counts_layers <- Layers(all_combined[[\"RNA\"]])\nall_counts_layers <- counts_layers[grepl(\"^counts\", all_counts_layers)]\n\n# # view all_count_layers\n#  [1] \"counts.1\"  \"counts.2\"  \"counts.3\"  \"counts.4\"  \"counts.5\"  \"counts.6\"  \"counts.7\"  \"counts.8\"  \"counts.9\"  \"counts.10\" \"counts.11\" \"counts.12\" \"counts.13\" \"counts.14\"\n# [15] \"counts.15\" \"counts.16\" \"counts.17\" \"counts.18\"\n\n\n\n# Extract each layer and combine\nall_count_matrices <- lapply(all_counts_layers, function(layer) {\n  LayerData(all_combined, assay = \"RNA\", layer = layer)\n})\n\n\n# View all_count_matrices (prints out all 18 count matrices. NOTE: each matrices has different number of rows/genes and different number of columns. However, total rows adds up to 23,447 genes and total columns add to 135,704 cells)\n# all_count_matrices\n\n\n\n# Combine into one gene x cell matrix\nall_combined_counts <- do.call(cbind, all_count_matrices)\n\n# Error in cbind.Matrix(x, y, deparse.level = 0L) : \n#   number of rows of matrices must match\nAny advice on how to do this effectively would be greatly appreciated. I'm using Seurat v5.3.0",
    "answers": [
      "Alright, I fixed the issue by joining the counts layers in all_combined. I saved it to a new seurat object. Hopefully this helps someone else!\nall_combined_join_layers <- JoinLayers(all_combined)\nThis joins the 18 counts layers into one layer (and also the 18 normalized data layers)\nall_combined_join_layers\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 21 layers present: data, counts, scale.data.1, scale.data.2, scale.data.3, scale.data.4, scale.data.5, scale.data.6, scale.data.7, scale.data.8, scale.data.9, scale.data.10, scale.data.11, scale.data.12, scale.data.13, scale.data.14, scale.data.15, scale.data.16, scale.data.17, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nI then extracted the counts matrix from all_combined_join_layers\nall_combined_count_matrix_2 <- LayerData(object = all_combined_join_layers, assay = \"RNA\", layer = \"counts\")\n\nall_combined_count_matrix_2\n\n23477 x 135704 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n\nENSMMUG00000023296 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692             . . . . . . . . . 1 . . . . . 1 2 . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612961/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612955/",
    "title": "open target platform SNP to Gene qurey",
    "question": "Hello, open target genetics is using the function to map the gene for snp by doing a snp to gene query. However, maybe because it was renewed, the old code doesn’t seem to work. Below is a part of the query statement.\nsearch_query <- \"\nquery searchRsId($rsId: String!) {\nsearch(queryString: $rsId) {\nvariants {\nid\n}\n}\n}\n\"\n\nquery_string <-\"\nquery v2g($variantId: String!) {\ngenesForVariant(variantId: $variantId) {\ngene {\nsymbol\nid\n}\nvariant\noverallScore\ndistances {\nsourceId\naggregatedScore\ntissues {\ndistance\n}\n}\n}\n}\"\nI wonder if it’s not available now or if the code has been changed. Thank you.",
    "answers": [
      "Did you see this announcement:\nOpen Targets Genetics has been superseded by the Open Targets Platform 25.03 release with updated genetics data and analyses. Open Targets Genetics is no longer maintained and will be deprecated on 9 July 2025.\nLooks like you will need to use https://platform.opentargets.org/ and the new API."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612956/",
    "title": "usage of ChromHMM and Segway",
    "question": "I want to use ChromHMM and Segway to identify chromatin states or genomic segmentation based on existing models. Where are these models stored and what form these models are, and how can I use them for direct calculations? ChromHMM(https://ernstlab.github.io/ChromHMM/); Segway(https://segway.hoffmanlab.org/)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/215568/",
    "title": "Which tools do you recommend me use to get the maximum information of a protein?",
    "question": "Hello everybody!!\nI have begun to study bioinformatics two month ago and I'm a little bit newbie in this field. In any case I've been always working with proteins, for that reason I'd like to know which tools do you recommend me to study them.\nFor example to know this kind of things:\nWhich hypothetical cofactors can join them?\nA 3D protein structure compared to other\nIs it possible to \"model\" the protein?",
    "answers": [
      "For modeling, check out the AlphaFold Server for predicting protein structure."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9470187/",
    "title": "Tools for predicting protein function?",
    "question": "I am trying to gain insight into the potential function of a list of candidate effector protein sequences I have identified. I have searched using NCBI BLASTP against the nr database, performed GO analysis, searched for known domains using InterProScan and blasted them against the PHI-Base database, but still, the majority are annotated as “hypothetical proteins” and have no known protein domains.\nI am struggling to decide where to go next to try to gain insight into their potential function. I have considered trying to predict the 3d structure, but I would have to do this for 72 sequences and then what? I can’t find a database to search protein structures against.\nDoes anyone have any suggestions where to go next? Are there other papers, tools, or approaches you could suggest to gain more insight into potential function?\nThanks,\nJamie",
    "answers": [
      "Though unlikely, it is possible that all your proteins are uncharacterized and have no known domains. It is possible that some of them are not proteins at all. You may want to try the links below if you haven't already done so.\nhttps://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi\nhttps://pfam.xfam.org/\nhttp://smart.embl-heidelberg.de/\nhttps://toolkit.tuebingen.mpg.de/tools/hhpred",
      "Have you read the wikicrow entry on the gene? Wikicrow is an LLM generated jumping off point, so while you should treat the information directly presented with caution (as in a wikipedia article), the compiled links and evidence are very helpful."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9578484/",
    "title": "Map genome positions onto protein coordinates?",
    "question": "I am looking for a way to do the following\n1) reliably find a protein structure e.g. pdb file or pre-computed alphafold results that is associated with a particular gene/transcript isoform. I found a way to do this somewhat for human genes using biomart, but i'd like to be able to do this for 'any species' (reason: i make tools, and I want to allow people to use my tool on any species of interest).\n2) find a way to map genome coordinates onto that protein structure (3d position is relevant, but i guess just knowing the index into the 1d amino acid chain gets you most of the way there?). I feel like this is something variant annotation tools do, but is there a small purposeful code tool that does this instead of full fledged 'variant annotation'? my current way of doing things just looks at gff, takes every three letters of the CDS features, increments into the amino acid count, but I have a feeling this is not the most reliable way of doing things.\nfootnote: my gene to pdb structure biomart query i found...useful for now, but would be interested in finding a similar thing for other species http://useast.ensembl.org/biomart/martview/643c564ac8b632a4791ea866fb79f8e5?VIRTUALSCHEMANAME=default&ATTRIBUTES=hsapiens_gene_ensembl.default.feature_page.ensembl_gene_id|hsapiens_gene_ensembl.default.feature_page.ensembl_gene_id_version|hsapiens_gene_ensembl.default.feature_page.ensembl_transcript_id|hsapiens_gene_ensembl.default.feature_page.ensembl_transcript_id_version|hsapiens_gene_ensembl.default.feature_page.pdb&FILTERS=&VISIBLEPANEL=attributepanel",
    "answers": [
      "In R:\nhttps://bioconductor.org/packages/devel/bioc/vignettes/ensembldb/inst/doc/coordinate-mapping.html",
      "You can use NCBI gene table, e.g., https://www.ncbi.nlm.nih.gov/gene/346689?report=gene_table. In iCn3D, the isoforms, exons and genomic positions are shown for AlphaFold or PDB structures, e.g., https://structure.ncbi.nlm.nih.gov/icn3d/share.html?pA3pPu7LxdiuZDVX7",
      "You can use ensemble VEP to get the protein coding changes.\nYou can then use the Genomics 2 Proteins portal to map these changes to protein structures (Interactive Module for PDB, AlphaFold, and uploaded structures)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9545017/",
    "title": "Online tool for creating figures of gene with its features",
    "question": "Is there an online and free tool for creating figures of gene with its features? Simple figures, like a bar representing the gene and smaller bars inside it representing motifs.\nThank you",
    "answers": [
      "Yes, check out the Genomics 2 Proteins portal. The tool allow you to map any feature, including domains onto a protein structure and capture high resolution images of both the features in sequence and structure.\nYou should also check out the UniProt Structure viewer and RCSB PDB Sequence Structure viewer. These won't let you capture a high resolution image of the sequence, but will for the structure. You may not be able to highlight all domains with these, but you could highlight 1 domain at a time in each of these tools and capture a high resolution image.\nOffline, you can use the PyMOL viewer. You will need to annotate the domains though, so this may not address your needs."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9579375/",
    "title": "Simple tools for creation of figures of gene and protein structure",
    "question": "Is there an online/simple and free tool for creating figures of gene and protein with its features/domains? Simple figures, like a bar representing the gene?protein and smaller bars inside it representing motifs/domains.\nThank you",
    "answers": [
      "There is a whole list of programs in that group:\nhttps://cmdcolin.github.io/awesome-genome-visualization/?latest=true&tag=Protein\nNot in the list:\nhttps://github.com/LKremer/showTree\nUp until a month ago this was working (I used it for a recent paper), but not any more:\nhttp://pfam-legacy.xfam.org/generate_graphic\nThe closest replacement for that functionality:\nhttps://github.com/ProteinsWebTeam/domain-gfx",
      "UCSC Genome Browser",
      "Yes, check out the Genomics 2 Proteins portal. The tool allow you to map any feature, including domains onto a protein structure and capture high resolution images of both the features in sequence and structure.\nYou should also check out the UniProt Structure viewer and RCSB PDB Sequence Structure viewer. These won't let you capture a high resolution image of the sequence, but will for the structure. You may not be able to highlight all domains with these, but you could highlight 1 domain at a time in each of these tools and capture a high resolution image.\nOffline, you can use the PyMOL viewer. You will need to annotate the domains though, so this may not address your needs."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9465733/",
    "title": "To create amino acid substitution in protein structure",
    "question": "i have 3-D structure of protein (PDB format), and in that structure i want to create an amino acid substitution at specific position of protein and after that i need to know the global minimum energy of mutated structure.\nPlease suggest any software regarding above problem or any method i can follow\nthank you",
    "answers": [
      "Changing a single amino-acid is not that difficult (for example, see Swiss-Model or Modeller), but what you want appears to be more rigorous. I suggest FoldX or Rosetta, but beware that both of them have steep learning curves. FoldX has an explanation here and for Rosetta there are several, but you can probably start here.",
      "For more intensive analysis, you can consider MD simulations of the mutant protein where you relax the structure after mutation using tools like GROMACs or VMD"
    ]
  },
  {
    "url": "https://www.biostars.org/p/48/",
    "title": "Where Can I Get The Secondary Structure Of A Protein?",
    "question": "As in the title... I have a protein and I would like to know its secundary structure. I couldn't find it in uniprot, althought I tought they had annotations for it there. In the end I have used a predictor (jpred) but there it should be a database somewhere.",
    "answers": [
      "If you have the PDB file then you can use the standard tool called DSSP , it is supposed to be the gold standard for obtaining secondary structure. In case you just have sequence then I personally prefer PSIPRED , it takes evolutionary information into account to predict the secondary structure . According to CASP evaluation it is one of the best secondary structure predictor available.",
      "I think you found the best answer yourself: use a predictor! There are several out there...\nYou suggest that there should be a Secondary Structure Database. I'm not sure that makes much sense, let me explain my point of view (which may not be that of everyone): most often, the data that is found in databases is the \"state of knowledge\" of the described object, based on experimentation.\nThat may be the case for secondary structures of proteins, but only in the case where the said proteins have been crystalized. In those cases, it is not only the secondary structures but also the tertiary structures (with the caveat that the crystal structure of a protein does not prove \"all\" states that a protein may take in real \"dynamic\" physiological conditions).\nFor all those proteins that have not been crystalized, then we can only rely on predictions. And I use them quite frequently: they are extremely useful! But as far as I know, no prediction is accepted as fact. They're \"educated guesses\" that are often correct, but sometimes wrong. The results may differ from one prediction method to another. Also they change each time the algorithms are improved...\nIf there was a database of predicted secondary structures, people would likely take them for granted (make the equation prediction = fact) which would be quite \"unscientific\".\nI think such a resource would be more of a hindrance than an asset to the scientific community...",
      "If it is only one sequence you may try PSIPRED server. If you need to work on a large sequence dataset, better to install PSIPRED locally. PSIPRED runs are typically computational intensive.",
      "Protein structure prediction is a complex issue that is likely to require multiple approaches. There are many methods/tools listed at the Expert Protein Analysis System website",
      "May be a little bit dated, but let me blow my own trumpet (collection of links).\nhttp://openwetware.org/wiki/Wikiomics:Bioinfo_tutorial#Protein_localization_and_structure_prediction",
      "If you do have the protein structure (PDB file), Stride is also a good option for assigning the secondary structure.",
      "If you want to obtain domains as well as the annotations that come along, you can do it locally with an RPS-BALST. Here for example to obtain Pfam annotations:\nrpsblast -i \".$InputPath.\"/\".$item.\" -d ~/Bioinfo/cdd/Pfam -e 0.000000000001 -o \".$elemt[0].\"_Pfam.rpsblast -T T -m 7\n-i = the input path\n-d = the database path\n-e = the e-value cut-off value\n-o = the output name\n-T T and -m 7 = to have the output in XML format\nYou can download all the databases from CDD. You'll obtain external source databases like Pfam, SMART, COG, PRK, TIGRFAM.",
      "You have got all the answers needed for your query. The structure of the protein generally comes after the X-Ray crystallography (which crystallized 80% of the protein structure existed) or NMR technology.\nProtein Data Bank (PDB) - It's the best and reliable options to find out all the structure available for your protein.\nPymol - Visualization and modelling in a 3D. You can visualize either by uploading PDB file or searching through Load structure Plugin.",
      "If you want to predict the secondary structure from a protein 3D structure DSSP is one of the best algorithm. following is the link for the same. http://swift.cmbi.ru.nl/gv/dssp/",
      "Jalview (http://www.jalview.org/) uses the fast and pretty accurate Jpred secondary structure predictor, there is a video on this on YouTube https://youtu.be/z5cVjR9Q3Mw",
      "UniProt will show the secondary structure for areas of the protein with available SIFTS secondary structures (from PDBs). If there is no PDB available, you can use the G2P portal to see the secondary structure from the predicted AlphaFold structure. Note that you should consider this secondary structure prediction alongside the reported AlphaFold confidence."
    ]
  },
  {
    "url": "https://www.biostars.org/p/97515/",
    "title": "Database Of Structurally Derived Protein Features",
    "question": "I have a list of a thousand residues identified on uniprot proteins. I need to gather features of these residues for a classifier. Some features require protein structures. I have a pipeline for finding appropriate structures using the protein model portal, and calculating structure based features (pka, solvent accessibility, disorder, local charge, etc) using published tools. It took a lot of work to scrape the structures and trouble shoot the published tools. I would have rather used pre computed data, but I could not find such a database.\nAre you aware of a protein feature database that spans all uniprot proteins? If not, do you know why such a resource has not been built?",
    "answers": [
      "I wrote a feature generation program/script a while ago in python that can do a lot of this, it's very fast and local, so just run it against the fastas of the proteins you want.\n(I'm currently working on a much upgraded version).\nHere's the source code.\nhttp://www.protonet.cs.huji.ac.il/neuropid/code/index.php\nhttp://www.protonet.cs.huji.ac.il/neuropid/code/local_SLEEK_FeatureGen+_new.py\nDan\n(Used in: http://neuropid.cs.huji.ac.il/ and the articles).",
      "A few databases that have protein structural features are UniProt, the RCSB PDB, and the G2P portal\nStructural features from each database:\nUniProt: PDB SIFTS secondary structures, AlphaFold confidence\nRCSB PDB: UniProt features, Hydropathy, Disorder, Occupancy, RSR, Buried Residues, etc\nG2P portal: UniProt features, Hydropathy, Molar mass, Secondary Structure (AlphaFold only), pockets (AlphaFold only), ASA (AlphaFold only), Disulfide bridge/H-bonds/Vander waals/Salt bridges (PDB and AlphaFold)"
    ]
  },
  {
    "url": "https://www.biostars.org/p/211648/",
    "title": "Protein structure and snps",
    "question": "Dear all,\nI'm looking for a tool that predicts the protein's structure after inducing mutations (user input). Are you aware of such approach? A similar example is MutationMapper which gives the 3D structure if it is known.\nThanks",
    "answers": [
      "There is a tradeoff between speed and information for a mutation: (1) Fast, visual: tools like MutationMapperenter link description here and the G2P portal for visualizing location of mutations. (2) Stability predictors including FoldX, Rosetta, and ThermoMPNN: predict the impact of a mutation physically, taking up to a few seconds per mutation. (3) MDSimulation: Long running simulation of a protein, often taking minutes hours to model the dynamics for a protein with a mutation, performance heavily affected by size of the protein. Tools like GROMACS and VMD can be used for this."
    ]
  },
  {
    "url": "https://www.biostars.org/p/17444/",
    "title": "Modelling Effect Of Mutation On Protein Structure",
    "question": "Hi,\nI am interested in structural changes in my protein due to mutation(s). I have WT pdb structure and I would like to model structure after single or multiple mutations . I would like to use something more fancy than just choosing from rotamer library. I was thinking about RosettaBackrub or Modeller or maybe there is another tool?\nThx for any suggestions!",
    "answers": [
      "FoldX - http://foldx.crg.es/ - will calculate a new structure and have a go at estimating the stability change if you give it a pdb file and list of mutations. It's a downloadable program, which is fairly straightforward to use once you get started.",
      "Modeller isn't likely to give you any hints. Since it's homology modelling, the entire protein, bar the mutation(s), is going to retain exactly the same structural features. Rosetta Backrub does seem a much better alternative for this.\nIf you can, get your hands on a MD package like GROMACS and run a moderately long simulation, depending on the size of your protein, so you can derive some conclusions out of it, although you won't see any spectacular unfolding or rearrangement. This will take a bit more time (a few days running, another few analyzing the results) but will probably give you a much more detailed picture of the influence of the mutations in the dynamics of the protein.\nIn the case of protein-protein complexes, I would look at the latest rounds of CAPRI and decide on who to choose to model the complex.",
      "For a quick visual analysis of how a mutation relates to or features in 3D, the Genomics 2 Proteins portal is designed for this purpose. Other tools including VarMap can streamline mutation effect predictions.\nFor a stability prediction, FoldX (mentioned above) and Rosetta are very popular. Recently ThermoMPNN has been introduced as a high performing deep learning algorithm, although it's important to understand the strengths and weaknesses of each approach.\nFor longer running MD simulations, you could use GROMACS (described above) or other MD software."
    ]
  },
  {
    "url": "https://www.biostars.org/p/278740/",
    "title": "Comparison of protein structures",
    "question": "For my undergraduate research project i'm trying to compare the structures of some proteins, specifically transcription factors crucial for dehydration tolerance in plants, in the hope of finding differences that make each protein more or less dehydration tolerant than the others. Is there any database or a site where i can find information regarding this?",
    "answers": [
      "You can start by visualizing protein structures in a variety of different tools, including offline tools like PyMOL (has an online version as well), VMD, and many web tools using MolStar (e.g. the G2P portal).\nYou can get secondary structure information using DSSP, and pocket information using SiteMap or fpocket (which is open source)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/180322/",
    "title": "Tools to display mutations in a protein's structure",
    "question": "I want to graphically display a number of point mutations in a protein graphically. I am aware of protein-structure-predicting tools (https://en.wikipedia.org/wiki/List_of_protein_secondary_structure_prediction_programs) but I have not heard of any tools that exist for displaying multiple mutations on a single protein.\nI would ideally like to great an interactive graphic that would allow the viewer to see each mutation's location highlighted on wild-type structure and be able to select one or more mutations then view their predicted impact on the protein's structure. But simply high-lighting the location of a list of mutations with a different color is my primary goal.",
    "answers": [
      "cbioportals mutationmapper has an option for this. Once you load your mutation data and submit, you can click on 3D structure for interactive 3D structure with mutations highlighted.",
      "The Genomics 2 Proteins portal's Interactive Mapping module is designed for this purpose. You can upload sets of mutations or experimental readouts from a screen.\nYou can change colors, etc, map other features from other variant and feature databases, and export the final output to PyMOL as well."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612938/",
    "title": "Should I perform integration to correct batch effects?",
    "question": "I have two single-cell RNA-seq samples: one from a liver metastasis of a wild-type tumor cell line, and the other from a liver metastasis of a drug-resistant tumor cell line. I want to compare the differences between the tumor regions in these two conditions.The experimental and sequencing workflows for both samples were completely identical (the only variable was the time).\nWhen performing dimensionality reduction and clustering, should I apply integration to remove batch effects? I'm concerned that integration might also eliminate some true biological differences between the two tumor types.\nAny suggestions or best practices would be greatly appreciated!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612924/",
    "title": "Different comparisons in RNASeq Analysis",
    "question": "Hi, I am in bit of a confusion regarding different comparisons in RNASeq data analysis.\nHere is what i have and already done.\nDataset 1: I have a mutant-strain and its wildtype. these were used to infact plant. Then RNA was extracted and sequenced with Illumina Paired-end 150bp sequencing with 3-replicates each\nAnalysis 1: DEGs analysis of plant-infection-mutants(A) vs plant-infection-wildtype(B) (A-vs-B)\nQuality check of fastq files with FastQC\nQuality trimming with FastP ( I did not perform any Quality Trimming as Data already seemed quality trimmed)\nRead Split to get reference specific reads using bbsplit from bbmap package (with this i got plant specific reads and mutant-strain specific reads)\nReads alignment using Hisat2 version 2.2.1 to ref-genome using paired-end reads\nQuantification using featureCounts with paired-end flags\nDEG analysis using DESeq2\nDataset 2: Again, a mutant-strain and its wildtype. these were grown in flask. Then RNA was extracted and sequenced with Illumina Single-end 75bp sequencing with 4-replicates each. (this is an old data)\nAnalysis 2: DEGs analysis of flask-grown-mutants(C) vs flask-grown-wildtype(D) (C-vs-D)\nQuality check of fastq files with FastQC\nQuality trimming with FastP ( I did not perform any Quality Trimming as Data already seemed quality trimmed)\nReads alignment using Hisat2 version 2.2.1 to ref-genome using paired-end reads\nQuantification using featureCounts with paired-end flags\nDEG analysis using DESeq2\nNow their are two more analysis which i want to do.\nAnalysis 3: DEGs analysis of plant-infection-mutants(A) vs flask-grown-mutants(C) (A-vs-C)\nFor Analysis-3 i did try this approach so far,\nextracted the plant-infection-mutants(A) featurecounts data from the Analysis-1 featurecount matrix which is based on illumina paired-end 150bp sequencing (first 6 columns GeneID Chr Start End Strand Length + 3 columns which contain mutant expression values in plants)\nextracted the flask-grown-mutants(C) feature counts data from the Analysis-2 featurecount matrix which is based on illumina single-end 75bp sequencing (first 6 columns GeneID Chr Start End Strand Length + 4 columns which contain mutant expression values in flask)\nMerged both based on the GeneIDs, (Note: A has 3 replicates, C has 4 replicates)\nDEG analysis using DESeq2 using the same commands as in A-vs-B and C-vs-D\nAnalysis 4: DEGs analysis of plant-infection-wildtype(B) vs flask-grown-wildtype(D) (C-vs-D)\nFor this i went with similar approach to Analysis-3\nQuestions\nFor analysis-1 and analysis-2 is my approach correct ? As far as i know, DESeq2 itself performs Median Ratio Normalization (MRN) so i didnot perfrom any other normalization.\nI am confused about the analysis-3 and analysis-4. are they correct? or the Different sequence-type (paired vs single), difference in read-length (150bp-x2 vs 75bp-x1) will have any technical or batch effect ?\nif analysis 3 and 4 are not correct, what should i do ? do i need to normalize them? by what method ?\nAny other thoughts or points you have to raise.\nYour thoughts and suggestions will be really helpful.\nRegards",
    "answers": [
      "or the Different sequence-type (paired vs single), difference in read-length (150bp-x2 vs 75bp-x1) will have any technical or batch effect ?\nYes, they absolutely will. This is fixable by trimming the paired one to be 75 bases, and just using the R1 fastq for aligning. Different instruments should not introduce too many artifacts. But the more substantial problem is that they were prepped on different dates. This will introduce a pretty big batch effect. I'm not sure analyses 3 and 4 are worth doing, because of the batch effect.",
      "For analysis 3, you could think about something like:\n(Plant Mut vs. Plant WT) vs (Flask Mut vs. Flask WT)\nIn doing so you are internally controling each MUT using the corresponding WT."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612929/",
    "title": "Querying on non-canonical bacterial IDs",
    "question": "I am trying to automate a query for non-canonical bacterial protein IDs in NCBI.\nThe IDs are not standard refseq (e.g. NP_*) but instead start with HDK (e.g. GenBank: HDK9254199.1)\nThey exist in NCBI (https://www.ncbi.nlm.nih.gov/protein/HDK9254199) but I have not yet been able to recover them programmatically.\nIn R,\nlibrary(rentrez)\nsearch <- entrez_search(db = \"protein\", term = \"HDK9254199\")\nsummary <- entrez_summary(db = \"protein\", id= \"HDK9254199\")\nsearch <- entrez_search(db = \"nuccore\", term = \"HDK9254199\")\nsummary <- entrez_summary(db = \"nuccore\", id= \"HDK9254199\")\nAll yield nothing. The summary error our and the searches yield nothing.\nIn a perfect world my colleague would have used the reference genome. But, they have already ordered an expensive library corresponding to these accession terms.\nIs there any way to map these HDK accessions back to canonical refseq terms?",
    "answers": [
      "I don't think there are any other records, including what you call canonical, for this protein.\nMaybe this will help you, using Entrez Direct:\nefetch -id HDK9254199 -db protein -format fasta\nThe screen output:\n>HDK9254199.1 TPA: RluA family pseudouridine synthase [Staphylococcus aureus USA100-NRS382]\nMETYEFNITDKEQTGMRVDKLLPELNNDWSRNQIQDWIKAGLVVANDKVVKSNYKVKLNDHIVVTEKEVV\nEADILPENLNLDIYYEDDDVAVVYKPKGMVVHPSPGHYTNTLVNGLMYQIKNLSGINGEIRPGIVHRIDM\nDTSGLLMVAKNDIAHRGLVEQLMDKSVKRKYIALVHGNIPHDYGTIDAPIGRNKNDRQSMAVVDDGKEAV\nTHFNVLEHFKDYTLVECQLETGRTHQIRVHMKYIGFPLVGDPKYGPKKTLDIGGQALHAGLIGFEHPVTG\nEYIERHAELPQDFEDLLDTIRKRDA\nAssuming you have all the IDs of interest in IDs.txt:\ncat IDs.txt | xargs -i efetch -id {} -db protein -format fasta >> proteins.fas"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612922/",
    "title": "Distribution of assayed SNPs per sample",
    "question": "I received a plink/vcf file with a lot of samples genotyped with many different SNP chips, both in size (varying from 50K to 1.5 million) and in platforms (different companies).\nI need to find common SNPs across samples.\nThe file itself has SNP IDs in the format of CHR:BP, therefore I cannot use this to infer which SNP comes from which platform. According to my logic, one could do this by filtering out SNPs with missing genotyping calls (./.), however when I did this, I ended up having very small number of SNPs in common. Also, some individuals might be genotyped for some SNP, but ended up having missed call, so I think this is not a good way to do it. I tried PLINK's --missing command which reports the overall missing genotype calls per individual and sample. This is informative, however, I need to know exact SNPs that are common across individuals.\nIs there a way to find this out?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612921/",
    "title": "Learn Python for Biological Data Analysis – Online Course this September!",
    "question": "Dear all, Are you a biologist interested in learning how to code and analyse your data using Python?\nWe’re excited to announce our upcoming online course: Introduction to Python Programming for Biologists\nDates: 29 September – 2 October\nOnline | 2–8 PM Berlin time daily\nCourse website: https://www.physalia-courses.org/courses-workshops/python24/\nThis hands-on, beginner-friendly course is designed for life scientists with little or no coding experience. Across four intensive days, you’ll learn:\nPython basics, data types, and coding structure\nHow to manipulate and analyse biological data\nUse of libraries like BioPython, NumPy, and pandas\nData visualisation tools like matplotlib, seaborn, and plotly\nHow to organise reusable code and build small projects for your own research",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612919/",
    "title": "How to calculate partial correlation controlling cancer types",
    "question": "Hello Biostars,\nI just wrote this tutorial on how to calculate partial correlation. https://divingintogeneticsandgenomics.com/post/partial-cor/\nIt was for my understanding of using linear models to calculate correlation. In the end, I used a real biological example (CRISPR screen data from Depmap) to demonstrate the usage.\nHope it is helpful for you too!\nBest, Tommy aka crazyhottommy",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612917/",
    "title": "SAIGE-GENE error",
    "question": "Hi,\nI'm trying to run SAIGE-GENE for a genomic dataset on HPC. I created a sparseGRM, but I get some segmentation error when I run the step1:\n*** caught segfault ***\naddress 0xc18c000, cause 'memory not mapped'\nSegmentation fault (core dumped)\nWhat I’ve eliminated as causes: Incorrect sample ID ordering or mismatch; Corrupted or improperly formatted sparse GRM matrix; Mismatch between phenotype, genotype, and GRM sample sets.\nDoes anyone have any experience running SAIGE/SAIGE-GENE? And any suggestion what may cause to this error? Thanks",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612915/",
    "title": "IGV track display problem: interval rectangle from BED file appears above all tracks in white, overlapping each other",
    "question": "If I load a BED file, it shows the interval rectangles above all other tracks, and if I have multiple loaded and the same interval is in each, the rectangles overlap each other, all at the top, no matter how I rearrange the tracks. The feature name appears where expected, wherever the track is in the list and at that position.\nThis happened suddenly, I think after updating my system (Manjaro). I installed the latest version of IGV from the Arch User Repository, but it still has the same problem. I was thinking maybe it's a problem with the Java version being used.\nHas anyone seen this and have insight into what is causing it, and how to revert it to the usual behavior where features appear with their tracks?\nNote: I also tried searching for the feature name in the box, e.g. Chr1-154, but it didn't bring anything up. Maybe because the names are prefixed by the chromosome name, or related to the display problem?",
    "answers": [
      "The problem was the filename suffix, it's meaningful in how IGV treats the file. It is .pk here, but changed to .bed and it works."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612913/",
    "title": "Y-axis break in Volcano Plot",
    "question": "res_lumbar <-read.csv(\"res_lumbar_results_new.csv\",header=TRUE)\nsig_lumbar<- subset(res_lumbar, padj < 0.1 & abs(log2FoldChange) >= 1)\n\nup_lumbar <- subset(sig_lumbar, log2FoldChange >= 1)\n\ndown_lumbar<- subset(sig_lumbar, log2FoldChange <= -1)\n\ncat(\"Lumbar Upregulated:\", nrow(up_lumbar), \"\\n\")\n\ncat(\"Lumbar  Downregulated:\", nrow(down_lumbar), \"\\n\")\n\nEnhancedVolcano(res_lumbar,\n                lab = NA,\n                x = \"log2FoldChange\",\n                y = \"padj\",\n                title = \"Volcano Plot: Thoracic CT vs IT\",\n                xlab = \"Log2 Fold Change\",\n                ylab = \"-log10(p-value)\",\n                pCutoff = 0.05,\n                FCcutoff = 1,\n                pointSize = 1,\n                colAlpha = 0.75,\n                legendPosition = \"right\",\n                legendLabSize = 12,\n                legendIconSize = 4,\n                drawConnectors = FALSE,\n                ylim=c(0,130)\n)\nThis is the code i have used. While plotting the Volcano plots, due to few outliers, my whole plot is getting skewed, can anyone please suggest me how to include nreaks in the y-axis. Any help is appreciated",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612911/",
    "title": "How to rank a gene list using correlation coefficient and p-values for GSEA/clusterProfiler?",
    "question": "I have two datasets: a bulk RNAseq dataset for different samples, and a viability dataset with scores giving the effect of a treatment on sample.\nFrom this dataset, I calculated the correlation of the different gene expression profiles with the viability scores for each treatment. So for each treatment, I have a list of genes and their correlation with the treatment-viability, and a p-value.\nI want to rank this list as input for GSEA (previously used gseKEGG in ClusterProfiler). What is the best way to rank this gene list?\nPossibilities I could imagine:\ncorrelation coefficient (from +1 to -1)\nsign(correlation coefficient) * -log10(p-value)\np-value (from 0 to 1)\nUsing solely the correlation coefficient would be easier to interpret, as a high correlation would directly correspond to the effect of treatment on viability (gene correlated with sensitivity or resistance to treatment)\nI would not really know how to interpret solely the p-value.\nAnd using the combination formula with correlation coeff. and p-value would become more interpretable, but also a bit messy (gene significantly correlated with sensitivity or resistance to treatment). I drew on this possible formula, as I've seen it used for ranking DE (sign(FoldChange) * -log10(p) ), but I've also seen some critique of this as well.\nOverall, I can't really find a good source for how to rank genes for GSEA outside of differential expression.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612900/",
    "title": "Nanopore data analysis",
    "question": "Hi, I have Nanopore data, and the goal is to identify differentially methylated regions. I already have aligned BAM files with MM/ML tags. Does this approach make sense:\nConvert BAMs to BED using modbam2bed\nAggregate % methylation (e.g., with bedtools)\nCompare conditions using DSS in R?\nAlso, is it common to filter bam_pass reads by quality before analysis? And since I have ~500 BAM files per sample (about 150 GB total), should I merge them before converting to BED? Thanks in advance!",
    "answers": [
      "Merge bams - yes. (why do you have so many ?). Merging fastqs before creating bams is easier (cat *.fastq.gz ) than bam merging.\n150gb - yes, typical\nmodkit should be able to handle that amount of data, just try it. It's certainly more up to date than modbamtobed"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9575464/",
    "title": "What is the difference between norm --multiallelics -any versus --atomize?",
    "question": "Hello, forgive my ignorance-\nSuppose input.vcf contains a complex multiallelic site.\nWhat is the difference between\nbcftools norm --multiallelics -any -f hg38.fa input.vcf\nversus\nbcftools norm --atomize -f hg38.fa input.vcf\nI understand what --multiallelics -any does but not sure what is going on with --atomize. In the documentation it says \"Decompose complex variants, e.g. split MNVs into consecutive SNVs.\". I do not understand what this means for a multiallelic site.\nIf someone has a good example that would help clarify, that would be great.\nThanks in advance.",
    "answers": [
      "I don't think atomization compares to norm with respect to multiallelic sites. You can see an example of atomatization on a multi-allelic site in the example under --atom-overlaps option documentation:\n# Before atomization:\n    100  CC  C,GG   1/2\n\n    # After:\n    #   bcftools norm -a .\n    100  C   G      ./1\n    100  CC  C      1/.\n    101  C   G      ./1\nNormalization would just give you 2 records (I can't tell offhand what the GT field would be):\n100 CC GG\n100 CC C\nOnly the ALT field is split and the REF/POS are altered only in certain cases. MNVs are not split into SNVs - CC>GG remains CC>GG. I think when atomize is used MNVs will be split, so you get 2 C>G entries instead of one CC>GG entry. Note that this split would happen even if that record were not multiallelic.\nSide note: I wonder if they meant bcftools norm -a --atom-overlaps . and not bcftools norm -a ., but that's not today's problem."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612907/",
    "title": "checking the sequences that share origin",
    "question": "Hi all,\nI have a library of sequencing data that is very diverse. However, due to nature of sequencing machine, that it is hard to say whether a pair of sequences are from the same origin sequence (due to technical error) or actually different based on the (random) designs.\nOne can set a threshold based on the hamming distance, that smaller than, let say 2 hamming score, is actually just the result of technical error. But from my understanding, the different in some bases can also due to mutation during culture (biological impact) or many other factors. Also, setting a threshold is subjective.\nCan anyone suggest some ways or any modeling that take some parameters to check this?",
    "answers": [
      "Without the presence of UMI in data, it would be difficult to say with certainty that a pair of sequences share an identical original fragment, are from two identical independent fragments from the library or are because of sequencer making an error (or more)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612899/",
    "title": "Assembling Genomes with Oxford Nanopore: A Hands-On Guide to Long-Read Sequencing",
    "question": "Dear all,\nAre you working with long-read sequencing data or planning to dive into genome assembly using Oxford Nanopore Technologies? Join us this 29 September–2 October 2025 for the Physalia online course: Assembling Genomes with Oxford Nanopore: A Hands-On Guide to Long-Read Sequencing\nCourse website: https://www.physalia-courses.org/courses-workshops/course59/\nThis course will guide you step-by-step through the process of transforming raw Nanopore data into fully assembled, polished, and quality-checked genomes—whether or not you have short-read data or a reference genome.\nWith a mix of theory and hands-on practice, you’ll work with real datasets and apply state-of-the-art tools for basecalling, assembly, polishing, and QC.\nFor the full list of our courses and workshops, please visit: https://www.physalia-courses.org/courses-workshops/",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612898/",
    "title": "Bambu on illumina short reads sequencing",
    "question": "Hi all,\nI’m analyzing RNA-seq data generated with Illumina short reads from Singapore Consortium and I’m interested in using Bambu for transcript variant quantification. I have already used it for Nanopore long-read sequencing but I have very few TPM for my target.\nEven though most of the examples and tutorials I’ve seen seem to be based on long-read data (e.g. PacBio or Oxford Nanopore) I was wondering If it is possible or appropriate to use Bambu with Illumina short-read data.\nPs I tried and it says \"Bambu reports a new model with ROC AUC 0.914 and PR AUC 0.589 on my Illumina data, compared to 0.66 and 0.113 for the ONT pretrained model. Is this performance sufficient for reliable isoform quantification, or should I switch to Salmon?\"",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612891/",
    "title": "Isoquant Major Exon Elongation",
    "question": "Hi all, long time lurker first time poster. I've been running Isoquant to assign Nanopore reads to one transcript isoform, and even though they align, the vast majority of my reads are assigned \"inconsistent_non_intronic\". I look at the individual read assignments, and most of the reads have \"mono_exon_match,terminal_position_match_precise:0,major_exon_elongation:599\" under the \"assignment_events\" column.\nFrom the Isoquant docs, this means \"major_exon_elongation_5/3 - significant exon extension at 5'/3' end (exceeding 30bp);\". My question is, does the 599 mean how long the reads are extended past either the 5' or 3' end? Has anyone else run into this sort of problem with inconsistently assigned reads?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612890/",
    "title": "bcftools run-roh error",
    "question": "Hi,\nI am trying to use the bcftools run-roh functionality on a set of 5 vcf files. I used the bcftools roh command and was able to get lists of roh for each sample individually, but now I want to see any regions hared across the 5 samples. I keep running into the following error:\nError: The FORMAT/PL tag not found in the header, consider running with -G\nEven though this tag exists in all the vcf files. I can't really find any other documentation on bcftools run-roh either to troubleshoot this. If anyone has any experience with this, it would be very helpful, thank you!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612888/",
    "title": "Workshop: Virtual Single-Cell Transcriptomics, June 24–26",
    "question": "We still have space available in our upcoming Single-Cell Transcriptomics workshop (virtual but live instruction – June 24–26). This hands-on workshop will guide participants through the complete single-cell RNA-seq workflow, including raw data processing, quality control, clustering (UMAP), cell type identification, and sample integration using Jupyter notebooks with Python and R.\nLearn more & register here: bioinformatics.uconn.edu/cbc-workshops/\nWHERE: Virtual (Zoom) WHEN: 10:00 AM – 2:00 PM EST COST: $400 (UConn affiliates including UConn Health) $500 (External participants)\nQuestions? E-mail: cbcsupport@helpspotmail.com",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612879/",
    "title": "How can I find KO IDs for ORF sequences from kofam_scan ?",
    "question": "Hi,\nI set up the environment in HCC, and my FASTA file contains 98 sequences. I’ve tried running it several times changing time duration in SLURM script without any success. This is my SLURM script.\n#!/bin/bash\n#SBATCH --job-name=kofamscan\n#SBATCH --output=kofamscan.out\n#SBATCH --error=kofamscan.err\n#SBATCH --time=5:59:00\n#SBATCH --mem=32G\n#SBATCH --cpus-per-task=8\n\n\nsource ~/miniconda3/etc/profile.d/conda.sh\nconda activate kofamscan_env\n\n./exec_annotation \\\n  -o kofam_output.txt \\\n  -f detail-tsv \\\n  -p profiles/ \\\n  -k ko_list \\\n  --cpu 8 \\\n  test.faa\nIt keeps giving the following error, I tried changing differenet cpu allocation too.\n**“slurmstepd: error: *** JOB 10654468 ON c2023 CANCELLED AT 2025-06-10T21:46:45 DUE TO TIME LIMIT *”\nWhat should I do? What could be the issue?",
    "answers": [
      "Please don't post the same content in multiple places.\nI tried changing differenet cpu allocation too.\nAs I wrote in the other comment this is an issue of time not CPU.\nCANCELLED AT 2025-06-10T21:46:45 DUE TO TIME LIMIT *\nYou are asking for one min less than 6 h in this request and the job is getting killed once that limit is reached. Increase the time request to --time=1-0 (this would be one day) and see if that is enough. Adjust as needed/permitted by your local allocation.\nI’ve tried running it several times changing time duration\nIf you are only allowed 5 h and 59 min max then this job can't be completed in that time limit.\nBTW: The title of this post does not have a direct connection to the question you asked.",
      "Already answered in your other post.\nI suggest you inquire about the SLURM time limit and set it to a maximum value allowed. This would be 6 days:\n#SBATCH --time=6-00:00:00\nSuggestions:\nHow do you know that the same error is occurring in response to GenoMax suggestion when you didn't allow several days for this to run?\nWhy don't you try the same script on your shortest sequence and make sure that everything works?\nWhy not ask for more than 8 CPUs?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612875/",
    "title": "VEP all genes are skipped due to invalid types",
    "question": "Hi,\nI'm currently running VEP (local mode) on human data using hs1 genome with a GTF file from refseq using the following command :\nperl vep --refseq -i my_file.vcf -o my_file_VEP.txt --gtf hs1.ncbiRefSeq.gtf_sorted.gtf.gz --fasta hs1.fa\nI have the following warning message for all genes when I run the command (here just 1 line example) :\n WARNING: Parent entries with the following IDs were not found or skipped due to invalid types: TRPC5OS, TRPC5, RTL4, ALG13\nI'm quite sure it concern every genes because I obtain only intergenic_variant in the my_file_VEP.txt (~ 5000 lines of variants) .\nAny idea why I have this kind of results ?\nIn this documentation page I read that in ensembl GTF file there is a line describing gene that doesn't exist in refseq GTF file :\n1    Ensembl gene        1000  5000  . + . ID=gene1;Name=GENE1\nIs it the reason ?\nBest,",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612865/",
    "title": "Isoform analysis after quantification with Salmon/Star",
    "question": "Hello all, I have been given with STAR aligned BAM files, so I can use STAR/SALMON to get the transcript level counts. Basically I need to focus on the isoform level changes , so what are some good tools to explore that? And which one will be better? Counting with STAR or SALMON ?",
    "answers": [
      "It depends on what level you would like your analysis to take place on. In general, if you are interested in complete isoforms, then Salmon gives you isoform level quantification, where as STAR doesn't. However many tools that operate on alternate splciing level, require either exon or junction counts, or BAM files as input, which would mean using STAR.\nDecide what you are intersted in:\nDifferential Transcript Expression (DTE): This simply compares the level of each transcript in each sample. So if a gene has two isoforms iso1 and iso2, and they are measured in two conditions A and B, DTE would give you two numbers: LFC[iso1] = count[iso1,condA]/count[iso1,condB] and LFC[iso2] = count[iso2,condA]/count[iso2,condB]. That is iso1 and iso2 are treated as if they are entirely independent of each other, and their fold changes measured seperately. Any resulting log fold change could be due to changes in transcription, or changes in splicing or transcript stability. The most common tools here are edgeR/limma/DESeq2, and you would want Salmon quantification.\nDifferential Transcript Usage (DTU): This compares the fraction of a gene that is an isoform in one condition to the fraction of a gene that is that isoform in the other gene. In the example abouve it is DTU[gene] = (count[iso1,condA]/count[iso2,condA])/(count[iso1, condB]/count[iso2, condB]). Any log fold change here could be due to a switch from one isoform to another, (i.e. iso1 down, iso2 up), such as a change in splicing, or it could be due to a differnce in the level of only one of the isoforms (e.g. iso2 is destablised, but iso1 is unaffected). It may or may not be accompanied by a overall change in the level of the gene, but that will not show up in the analysis. The most common tools here are IsoformSwitchAnalyzeR/DEXSeq(on transcript counts)/limma::diffSplice. Again, you want Salmon quantification. You would also want to generate your Salmon quantification with pseduoreplication.\nDifferential Exon/junction Usage (DEU): Here the inclusion ratio of each indevidual exon in each of isoform 1 and 2 is compared between the conditions. Again, this won't distinguish between a switch from one form to the other and just one form changing levels but not the other. Although I've reffered to it as DEU, the same principle applies to junctions. The upside here is that you get results on indvidual events, like exon inclusion or skipping. There is also much less usage of inferential statistics on isoform levels from reads that could come from multiple transcripts - it just uses straight forward counts, so in that way the results are more reliable. They can however be hard to interpret. If an exon is 10 times more included in condition A than condition B, that might seem exciting, but not when it turns out that both the inclusion and exclusion forms only make up 10% of all transcripts from a gene. Top tools here are rMATS (uses BAM files as input), SUPPA2, MISO, DEXSeq (on exon or junction counts).\nWhich is best probably depends on what you want out of your analysis.",
      "The following papers show the advantages of Salmon with Gibbs resampling, followed by edgeR's catchSalmon, glmQLFit, glmQLFTest (for DTE) and diffSplice (for DTU):\nBaldoni PL#, Chen L#, Li M, Chen Y, Smyth GK (2025). Dividing out quantification uncertainty enables assessment of differential transcript usage with diffSplice. bioRxiv https://doi.org/10.1101/2025.04.07.647659\nBaldoni PL, Chen L, Smyth GK (2024). Faster and more accurate assessment of differential transcript expression with Gibbs sampling and edgeR v4. NAR Genomics and Bioinformatics 6(4), lqae151.\nBaldoni PL#, Chen Y#, Hediyeh-zadeh S, Liao Y, Dong X, Ritchie ME, Shi W, Smyth GK (2024). Dividing out quantification uncertainty allows efficient assessment of differential transcript expression with edgeR. Nucleic Acids Research 52(3), e13. https://doi.org/10.1093/nar/gkad1167"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612867/",
    "title": "Confidence in CIBERSORT (LM22) Deconvolution Output for Rare Cell Types",
    "question": "Hi everyone,\nI’m working on deconvolution of bulk RNA-seq data using CIBERSORT with the LM22 signature matrix to estimate immune cell proportions. My supervisor asked a very valid and challenging question that I’d like to get your input on:\nHow believable or reliable are the output proportions from CIBERSORT, especially for rare cell types like GD?\nIn my current analysis, I’ve followed a methodology similar to the one used in:\nA Characterization of the immune cell landscape in CRC: Clinical implications of tumour-infiltrating leukocytes in early- and late-stage CRC\nHowever, I also came across this recent tool and paper:\nReCIDE (Robust Estimation of Cell Type Proportions by Integrating Single-Reference-Based Deconvolutions), which emphasizes that rare cell type estimates (typically <2%) may not be reliable when using single-reference-based methods like CIBERSORT.\nThis leads me to a few key questions I would really appreciate feedback on:\nHow much confidence can we have in CIBERSORT results for rare cell types (e.g., <2% estimated proportion)?\nAre there better-suited approaches or additional validation steps for rare cell types in bulk RNA-seq deconvolution?\nHas anyone tried integrating CIBERSORT with other methods like ReCIDE or using single-cell data to improve deconvolution accuracy for rare populations?\nI’d really value hearing how others in the field handle these limitations, especially when trying to interpret or validate rare cell population estimates from deconvolution analyses.\nThanks in advance!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612866/",
    "title": "Significant reads loss after fastp",
    "question": "Hy everyone when I first did my fastqc I observed that phred score was not very good in my data and especially phred score of R2 was terrible, so I did fastp. When I read the fastp report I noticed that about half of my reads have been discarded, now this makes me worried whether I used the right set of parameters or not? how will this now affect my alignment and subsequent steps?? its impact on gene coverage?? Before filtering the read length was 35-76 bp and now it has reduced to 50-76 bps. let me tell you I used the same fastp command for R1 and R2 of all samples. WHAT SHOULD I DO NOW? SHOULD I PROCEED FURTHER OR CHANGE MY FASTP PARAMETERS?\nthis was the fastp command that i used\n fastp \\\n    -i ${sample}_R1.fastq.gz \\\n    -I ${sample}_R2.fastq.gz \\\n    -o trimmed_fastq/${sample}_R1.trimmed.fastq.gz \\\n    -O trimmed_fastq/${sample}_R2.trimmed.fastq.gz \\\n    --adapter_sequence (provided)\n    --adapter_sequence_r2 (it was poly-g sequence)\n    --detect_adapter_for_pe \\\n    --correction \\\n    --trim_poly_g \\\n    --trim_poly_x \\\n    --poly_g_min_len 10 \\\n    --poly_x_min_len 10 \\\n    --cut_front \\\n    --cut_tail \\\n    --cut_right \\\n    --cut_window_size 5 \\\n    --cut_mean_quality 20 \\\n    --qualified_quality_phred 20 \\\n    --unqualified_percent_limit 30 \\\n    --n_base_limit 3 \\\n    --length_required 50 \\\n    --low_complexity_filter \\\n    --complexity_threshold 30 \\\n    --overrepresentation_analysis \\\n    --thread 8 \\\n    --html fastp_reports/${sample}_fastp.html \\\n    --json fastp_reports/${sample}_fastp.json",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612864/",
    "title": "Shinyapps Server Down?",
    "question": "Is it only me, or are all the Shinyapps showing Internal Server Error?",
    "answers": [
      "https://downforeveryoneorjustme.com/\nSite may have recovered since you posted the original message but https://www.shinyapps.io/ is up now."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612862/",
    "title": "Data of microarray",
    "question": "I am trying to learn how to use this application, but I do not have any data. Could you give me a file format .tif or .txt, to test this application? Thank you so much.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612860/",
    "title": "How to run GSVA on scRNA-Seq analysis on multiple clusters and groups to compare per tumor sample?",
    "question": "Hello ,\nI’m working on a single-cell RNA-seq dataset where I want to compare pathway activity across different cell groups (e.g., tumor, immune, other) within each tumor sample. These cell groups each include multiple clusters (e.g., macrophages, NK cells for the immune group). I would like to use GSVA or ssGSEA for pathway analysis. I am not sure which algorithm is suitable.\nI’ve tried two approaches:\nRun GSVA on the full matrix using GetAssayData(seurat_obj, slot = \"scale.data\") then perform grouping in post-analysis visualization with a heatmap.\nAggregate expression using AggregateExpression() by orig.ident and cell_group, then run GSVA on the resulting matrix, but I’m unsure if it's the correct approach.\nEdit: I have not been able to perform the first method, but I have been able to perform the second. I am still not sure which one is more appropriate. Isn't performing pseudo bulking defeat the purpose of scRNASeq, since you return to a bulk-like state?\nPreviously, I have used the ssGSEA parameter on the whole matrix obtained from GetAssay function, however as I understand it, the algorithm will perform the analysis on every single cell of the matrix. The analysis was terminated because it hasn't finished after nearly one week of processing. I am sure I am doing something wrong but I am not sure why. This method was inspired by this GitHub response suggesting the GetAssay option.\nThe biological question is this: what are the different pathways expressed inside each cell groups (tumor, immune, other) where each cell group contains multiple clusters of celltypes (macrophages, NK cells for immune group for example), for each tumor sample? These cell groups are chosen based on their previous cluster identification.\nBecause online guides on GSVA pertaining to single-cell are so scarce, I am not sure whether it is appropriate to make our cell groups before or after the GSVA analysis and whether or not to input the matrix of pseudo-bulk genes obtained from the AggregateExpression function, or the raw scaled.data assay situated which is normalized and scaled previously in a previous Seurat process, obtaining it from the GetAssayData(layer = \"scale.data\") function.\nHere the relevant code:\nobject <- GetAssayData(object = seurat_obj, assay = \"RNA\", layer = \"scale.data\")\n# ssgsea_object <- ssgseaParam(object, geneSets = gene_sets)\n\ngsva_object <- gsvaParam(object, geneSets = gene_sets)\ngsva_results <- GSVA::gsva(param = gsva_object)\nAnother alternative is using the matrix obtained from pseudobulk using AggregateExpression such as:\n avg_exp_group <- AggregateExpression(seurat_obj, group.by = c(\"orig.ident\", \"cell_group\"))\n gsva_object <- gsvaParam(avg_exp_group$RNA, geneSets = gene_sets)\n gsva_results <- GSVA::gsva(param = gsva_object, expr = object, gset.idx.list = gene_sets)\nI am not sure which way is the correct way or if there is a methodological misstep in any of those since the official GSVA vignette doesn't really mention anything about scRNA-Seq. Asking chatGPT ironically gives two different responses on two different computers, one suggesting the first and another the second approach. Also just to be sure, should I use the counts or the scaled.data in layers?\nFor the list of genes:\n# HALLMARK: HYPOXIA\nhallmark <- msigdbr(species = species, collection = \"H\")\nhallmark_hypoxia <- hallmark %>% filter(gs_name == \"HALLMARK_HYPOXIA\")\n[...]\n# Assuming I have multiple gene sets from msigdbr\ngene_sets <- list(\nHALLMARK_HYPOXIA = unique(hallmark_hypoxia$gene_symbol),\nHALLMARK_GLYCOLYSIS = unique(hallmark_glycolysis$gene_symbol)\n)\nMany thanks in advance, Minh-Anh\nEdit2: It turns out I have wrongly taken inspiration from the GitHub code:\n GSVA::gsva(expr = object, gset.idx.list = geneset, ...)\nThe function arguments have changed, thus the arguments were inappropriate and the function never resolved. I have now been able to perform both methods. However, I would still like to ask which method is more appropriate, inputting the full matrix (from data or scaled.data) or the pseudobulk matrix into the gsva function, and if gsva or ssgsea is more appropriate.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612848/",
    "title": "Best Tool for Fast RNA-Seq Transcriptome Alignment to Extract Unmapped Reads",
    "question": "Hello all,\nI have RNA-Seq data from human cancer samples. I ultimately want to single out reads that do not align properly (i.e., don't align or are discordant/split-mapped/etc.) to the human reference for downstream analysis. I could do this by mapping to the genome with STAR which is what we typically do for alignment. However, I want to find the quickest/least expensive way. The idea is to first align to the transcriptome and keep only those unaligned/ambiguous reads for further analysis. I can then align these to the genome with STAR and my guess is this would be cheaper/faster than just aligning all of them to the genome to begin with. Please correct me if this is immediately wrong.\nNow my question is what is the best approach for the initial transcriptome mapping. Based on comparing several tools (STAR, Bowtie2, Hisat2, kallisto), I think using Bowtie2 to map reads to human transcripts. STAR is generally not used for mapping to the transcriptome and has a higher memory usage. Kallisto pseudo-alignment is fast but it wouldn't give me the results I want, which are the unaligned reads back. Hisat2 seems comparable to Bowtie2, but in this case I don't need the alignment to be splice-aware, and it seems like Hisat2 is not as well maintained. Please let me know if you have done something similar or have any thoughts on this. Thanks!",
    "answers": [
      "my guess is this would be cheaper/faster than just aligning all of them to the genome to begin with\nIf you are paying for all of your compute (e.g. CPU/storage ) then possibly, but it should not be a big difference. Since you don't want to do the analysis again and again, spending a bit more (in money and time) upfront would be advisable. Aligning to data to the genome will take care of all expressed reads, even those that may not be recognized by the transcriptome (previously unknown transcripts).\nYou can then filter the aligned file with samtools using the answer here : samtools extract unmapped reads\nIf you are using salmon then you could use the --writeUnmappedNames parameter to get the read names that don't map (ref: https://salmon.readthedocs.io/en/latest/salmon.html#writeunmappednames). You can then use filterbyname.sh from BBMap suite to get those reads out of the original data.\nIf you are open to using bbmap.sh the aligner from BBMap suite then you can capture reads that do not align at the time you do the mapping. You could do something like (increase the -Xmx10g to -Xmx30g, if you are using a human size genome). You will need to have samtools available in $PATH to get the BAM file.\n$ bbmap.sh threads=N -Xmx10g in1=R1.fq.gz in2=R2.fq.gz outu1=R1_unmapped.fq.gz outu2=R2_unmapped.fq.gz out=aligned.bam ref=reference_genome.fa ambig=random\nIf you do not want to get the aligned data then simply remove out=aligned.bam from the command line.",
      "Honestly, I'd just stick with STAR for this purpose. You can use the --outReadsUnmapped option to directly output the unmapped reads in FASTQ format or the --outSAMunmapped to directly output the unmapped reads in SAM format. STAR also has an option to create a sparse genome index if memory is a concern.\nYou can use lightweight tools like kallisto or salmon, but you have to actually go through the FASTQ file TWICE in order to get your unmapped reads (you go through it once for the pseudoalignment, and then you go through it a second time to extract the reads that were not pseudoaligned); this will therefore be much less efficient. I actively develop kallisto, so if you want, I'm happy to consider outputting the unmapped reads in FASTQ format as a feature in a future release of kallisto.\nbowtie2-to-transcriptome will probably be more efficient than STAR. However, I've tried using bowtie2 to output unaligned reads in the past, and I remember have issues in paired-end mode: going from a paired-end input FASTQ to a paired-end output unmapped FASTQ (I don't think it's possible to directly do this in bowtie2)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9609844/",
    "title": "Impact of the number of PCs on the clustering in scRNA seq",
    "question": "Hello all,\nI am working with a scRNA-seq dataset. I apply a PCA and select two different number of PCs (10 and 20). Then I apply a Louvain clustering on the reduced space with a fixed resolution (0.3) and I compare the two clusterings. I get more clusters when I selected 10 PCs than when I selected 20 PCs.\nI wonder why is that and I would therefore appreciate any hint!\nThank you very much :)\n(I join the scree plot showing the percentage of variance explained by each PC)\nAlong with the two UMAP showing the clustering.",
    "answers": [
      "As much as it might feel like an unsatisfactory answer, this totally depends on your downstream needs and what meaningful biology you can attach to the clustering. Some of these clusters will be based on technical variation, some will be QC-associated, some will associate with cell cycle while others will be driven by some effect based on a set of differential genes. Find out what markers are driving the clusters and you'll get a much better understanding of the clustering 'success' yourself.\nWhen gauging the success of graphical clustering the first question to ask is how meaningful are the clusters and you do that by looking at the marker genes for that cluster and see if you can make sense of it.\nFor your larger UMAP (bottom right), for example, I'd be quite interested in finding out the set of genes that would resolve cluster 2 (green) and cluster 8 (blue). I would equally be interested in understanding the stripe running down the middle of cluster 1 (orange), cluster 0 (dark blue) and cluster 5 (brown).\nSelecting the optimal number of PCs is not a simple question and typically the most practical solution is to iterate a few times and see if you get better resolution with a larger input set.\nSince you're already on python, you can look into tools like cNMF which allow you to optimise feature selection more readily than standard graphical clustering approaches. However, I would storngly recommend exploring your data a bit more before moving on with extra technical tools. Once you've convinced yourself you understand what is going on, you'll be in a much better position to judge these kinds of things yourself."
    ]
  },
  {
    "url": "https://www.biostars.org/p/482158/",
    "title": "ATAC-seq +4 -5 shift",
    "question": "Dear all,\nI saw having the mapped reads have +4 and -5 shift in ATAC-seq is a common practice.\nSome place says \"reads should be shifted + 4 bp and − 5 bp for positive and negative strand respectively, to account for the 9-bp duplication created by DNA repair of the nick by Tn5 transposase and achieve base-pair resolution of TF footprint and motif-related analyses\"\nSome place says:\" When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site.\"\nI'm a little bit confused. Are shifting mainly to center the peak or avoid the duplication?\nDoes anyone have a good illustration on this? What will happen to the peak calls if this step is skipped?\nThank you!",
    "answers": [
      "I illustrated the molecular biology here: http://guertinlab.org/wp-content/uploads/2021/01/Tn5_illumina_adapters_mjg_2.pdf\nwe skip this step for peak calling--it really only matters for looking at the data at single nucleotide resolution composite profiles.",
      "The shifting isn't for any real purpose unless you want to plot the exact cut location (e.g., when searching for motifs), it simply harkens back to one of the first ATAC-seq papers where they performed this adjustment to account for the 9-base single-stranded over-hang on each end of the fragment. Papers since have simply followed suite. A vastly more sensible strategy would be to use the 9 bases on each end of the fragment, since these are bases that are necessarily open.",
      "Turns out +4/-4 is the correct shift. You can see that when you make a bigwig separately from the + and - strands, then the reads align with each other for +4/-4 and it's off by one for +4/-5. The main idea is that transposition events should be mapped to the same base pair regardless of strand."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612994/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612991/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612988/",
    "title": "ATAC seq: from peaks to differential analysis ??",
    "question": "Hi,\nI’m fairly new to ATAC-seq and have successfully run MACS2 separately for each of my samples. I now have individual *.narrowPeak files as output.\nMy experimental design looks like this:\nSample_ID     Cell_type     Condition     Donor\n\nSample_1      T_cells       Tumor         Donor_1\nSample_2      T_cells       Normal        Donor_1\nSample_3      T_cells       Tumor         Donor_2\nSample_4      T_cells       Normal        Donor_2\n...\nSample_11     Dendritics    Tumor         Donor_10\nSample_12     Dendritics    Normal        Donor_10\nSample_13     Dendritics    Tumor         Donor_11\nSample_14     Dendritics    Normal        Donor_11\nAs you can see, I have two cell types (T_cells and Dendritics), and for each donor, I have paired Tumor and Normal samples.\nMy goal is to perform a differential accessibility analysis (Tumor vs Normal), accounting for both Donor and Cell_type. I’m also interested in comparing Tumor (T_cells) vs Tumor (Dendritics).\nI heard that it is possible to use DESeq2 for ATAC-seq data, so my design will look like this: https://bioconductor.org/packages/3.21/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#group-specific-condition-effects-individuals-nested-within-groups\nI have a few questions:\n1) Can I use the exact same code from the vignette, as typically done for RNA-seq data? Or are there any parameters or steps specific to ATAC-seq that I should consider?\n2) I’m struggling with how to convert my individual *.narrowPeak files into a count matrix. Do you have any recommendations or tools to help with this step?\n3) Are there alternative methods to DESeq2 that would be better suited for this kind of analysis? I guess limma should work the same no ?\nThank you in advance for your help !",
    "answers": [
      "2:\n# make saf file:\nawk 'OFS=\"\\t\" {print $1\":\"$2+1\"-\"$3, $1, $2+1, $3, \"+\"}' ${sample}_peaks.narrowPeak > featureCounts_peaks.saf\n\n$featureCounts -a featureCounts_peaks.saf \\\n    -F SAF \\\n    --read2pos 5 \\\n    -p \\\n    -o ${peak_all_dir}${sample}_countMatrix.txt ${rmBL_dir}*.bam\n${rmBL_dir}*.bam: all bam files for generating narrowPeaks\n3: DEseq2 is oke. design matrix and param depends on research question, not on the ATAC-seq or RNA-seq as they are all readcount type",
      "I also recommend converting the peak to SAF file as QX shows, although I just count fragments centered on cut site. There's little difference either way though.\nFor more guidance on creating a count matrix, you do need to create a common set of peaks first. Two main ways to do this, either take the intersection of peaks present in all samples or the union.\nI use bedtools, e.g.\n# N just represents number of total samples.\nbedtools mutliinter -i ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | awk '$4 == N' | bedtools sort -i - | bedtools merge -i - > All_Samples_Intersection.bed\nOr,\ncat  ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | bedtools sort -i - | bedtools merge -i - > All_Samples_Union.bed\nThen you can convert that bed into SAF and use featurecounts to count reads from each sample and treat similar to RNA-seq.\nOne thing I would suggest, I usually use a low count filter before moving forward with DE analysis. With ATAC-seq, there's usually more noise (I use relatively lax peak calling), so I tend to use a higher count threshold."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612987/",
    "title": "CUT&RUN analysis pipeline",
    "question": "Hello, My lab has generated data using the CUT&RUN protocol. Unfortunately, Its single-end data. I tried the CUT&RUN tools and the henipipe tool. Both are for paired end data.\nAny suggestion how I can analyse these data ? Any idea how to set the parameters in case I use the classical Trimmomatic and bowtie2 ? Any other pipelines to try ?\nThanks and Regards. (Loosing my mind)",
    "answers": [
      "You don't need to do anything too special. Trim adapters with your favorite software, align to genome using BWA-MEM or bowtie2, and call peaks using MACS2.\nMost of the fancy stuff with CUT&RUN is when you have paired end data. With paired end data you get information on insert/fragment size, which can be used to infer nucleosomal and subnucleosomal fragment, which in turn lets you guess whether afragment was from nucleosome protection or TF binding."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612985/",
    "title": "Cut&Run replicates handling",
    "question": "Is there a CUT&RUN peakcaller that supports replicates? If not, what is your recommended approach for handling replicates?\nIt appears that all available peak callers, such as SEACR, are designed to call peaks from single pull-down experiments.\nOur Experiment: We have two conditions, Untreated (UTR) and Treatment (TREAT), each with two biological replicates. Each condition also has respective INPUT data for normalization.\nWe have completed:\nQuality Control (QC)\nAlignment\nDuplicate marking/removal\nSpike-in Calibration\nNext, we would like to proceed to peak calling and differential analysis. We would appreciate your recommendations on suitable peak callers and the best practices for handling replicates.",
    "answers": [
      "You can use nf-core cutandrun pipeline, it supports replicates in the step of consensus peaks calculations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612984/",
    "title": "Unable to extract count matrices from multi-layered Seurat object",
    "question": "Hello, I have a merged R object with 18 samples\nall_combined <- merge(\n  sample_1MI1_so,\n  y = c(\n    sample_1MI3_so,\n    sample_2MI1_so,\n    sample_2MI3_so,\n    sample_3C_MI2_so,\n    sample_3MI1_so,\n    sample_3MI3_so,\n    sample_4D_MI2_so,\n    sample_4MI1_so,\n    sample_4MI3_so,\n    sample_5MI1_so,\n    sample_5MI3_so,\n    sample_6MI1_so,\n    sample_6MI3_so,\n    sample_7MI1_so,\n    sample_7MI3_so,\n    sample_8MI1_so,\n    sample_8MI3_so\n  ),\n  add.cell.ids = c(\n    \"sample_1MI1_so\",\n    \"sample_1MI3_so\",\n    \"sample_2MI1_so\",\n    \"sample_2MI3_so\",\n    \"sample_3C_MI2_so\",\n    \"sample_3MI1_so\",\n    \"sample_3MI3_so\",\n    \"sample_4D_MI2_so\",\n    \"sample_4MI1_so\",\n    \"sample_4MI3_so\",\n    \"sample_5MI1_so\",\n    \"sample_5MI3_so\",\n    \"sample_6MI1_so\",\n    \"sample_6MI3_so\",\n    \"sample_7MI1_so\",\n    \"sample_7MI3_so\",\n    \"sample_8MI1_so\",\n    \"sample_8MI3_so\"\n  ),\n  merge.data = TRUE\n)\nView object\nall_combined\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 55 layers present: counts.1, counts.2, counts.3, counts.4, counts.5, counts.6, counts.7, counts.8, counts.9, counts.10, counts.11, counts.12, counts.13, counts.14, counts.15, counts.16, counts.17, counts.18, data.1, scale.data.1, data.2, scale.data.2, data.3, scale.data.3, data.4, scale.data.4, data.5, scale.data.5, data.6, scale.data.6, data.7, scale.data.7, data.8, scale.data.8, data.9, scale.data.9, data.10, scale.data.10, data.11, scale.data.11, data.12, scale.data.12, data.13, scale.data.13, data.14, scale.data.14, data.15, scale.data.15, data.16, scale.data.16, data.17, scale.data.17, data.18, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nIn the all_combined object, I', trying to extract the counts matrices from it, but I'm not sure how to do this? Below is what I've tried\nall_combined_count_matrix <- LayerData(object = all_combined, assay = \"RNA\", layer = \"counts\")\nWarning: multiple layers are identified by counts.1 counts.2 counts.3 counts.4 counts.5 counts.6 counts.7 counts.8 counts.9 counts.10 counts.11 counts.12 counts.13 counts.14 counts.15 counts.16 counts.17 counts.18\n only the first layer is used\n\n# only 1st sample counts matrix is extracted....want to extract all 18 counts matrices...all_combined object contains 18 seurat objects merged into one\n\n`\nManually combine all 18 count matrices into one\n# List all layers that start with \"counts\"\nall_counts_layers <- Layers(all_combined[[\"RNA\"]])\nall_counts_layers <- counts_layers[grepl(\"^counts\", all_counts_layers)]\n\n# # view all_count_layers\n#  [1] \"counts.1\"  \"counts.2\"  \"counts.3\"  \"counts.4\"  \"counts.5\"  \"counts.6\"  \"counts.7\"  \"counts.8\"  \"counts.9\"  \"counts.10\" \"counts.11\" \"counts.12\" \"counts.13\" \"counts.14\"\n# [15] \"counts.15\" \"counts.16\" \"counts.17\" \"counts.18\"\n\n\n\n# Extract each layer and combine\nall_count_matrices <- lapply(all_counts_layers, function(layer) {\n  LayerData(all_combined, assay = \"RNA\", layer = layer)\n})\n\n\n# View all_count_matrices (prints out all 18 count matrices. NOTE: each matrices has different number of rows/genes and different number of columns. However, total rows adds up to 23,447 genes and total columns add to 135,704 cells)\n# all_count_matrices\n\n\n\n# Combine into one gene x cell matrix\nall_combined_counts <- do.call(cbind, all_count_matrices)\n\n# Error in cbind.Matrix(x, y, deparse.level = 0L) : \n#   number of rows of matrices must match\nAny advice on how to do this effectively would be greatly appreciated. I'm using Seurat v5.3.0",
    "answers": [
      "Alright, I fixed the issue by joining the counts layers in all_combined. I saved it to a new seurat object. Hopefully this helps someone else!\nall_combined_join_layers <- JoinLayers(all_combined)\nThis joins the 18 counts layers into one layer (and also the 18 normalized data layers)\nall_combined_join_layers\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 21 layers present: data, counts, scale.data.1, scale.data.2, scale.data.3, scale.data.4, scale.data.5, scale.data.6, scale.data.7, scale.data.8, scale.data.9, scale.data.10, scale.data.11, scale.data.12, scale.data.13, scale.data.14, scale.data.15, scale.data.16, scale.data.17, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nI then extracted the counts matrix from all_combined_join_layers\nall_combined_count_matrix_2 <- LayerData(object = all_combined_join_layers, assay = \"RNA\", layer = \"counts\")\n\nall_combined_count_matrix_2\n\n23477 x 135704 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n\nENSMMUG00000023296 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692             . . . . . . . . . 1 . . . . . 1 2 . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612983/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612982/",
    "title": "usage of ChromHMM and Segway",
    "question": "I want to use ChromHMM and Segway to identify chromatin states or genomic segmentation based on existing models. Where are these models stored and what form these models are, and how can I use them for direct calculations? ChromHMM(https://ernstlab.github.io/ChromHMM/); Segway(https://segway.hoffmanlab.org/)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612980/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612979/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612978/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612977/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612976/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612975/",
    "title": "phage sequence no alignment to genome",
    "question": "I am currently working with a bacterial strain (let’s call it Strain A) for which I have a complete genome assembly (StrainA_genome.fasta). From this strain, I obtained two sets of phage-related sequences using different experimental approaches:\n1. An assembled phage genome from lab experiments: StrainA_phage_gapfilled.fasta\n2. A pair of short-read sequencing files from another phage preparation: StrainA_phage_R1.fasta and StrainA_phage_R2.fasta\nSeparately, I predicted prophage regions on StrainA_genome.fasta using Cenote-Taker 3 and PhageBoost, and collected all the predicted prophage sequences into a file: StrainA_predicted_phages.fasta.\nMy goal is to verify whether the experimentally derived phage sequences correspond to any of the predicted prophage regions, i.e., to confirm that the experimentally recovered sequences truly originate from prophages within Strain A.\nWhat I tried\nI attempted various alignments, including:\nI used bwa mem to align both the raw phage reads and the gap-filled phage assembly against the predicted prophage regions, using commands like:\n # Align short reads to predicted phages \n  bwa mem -t 4 StrainA _predicted_phages.fasta StrainA_phage_R1.fasta StrainA_phage_R2.fasta > phage_reads_vs_predicted.sam\n\n # Align assembled phage genome to predicted phages\n  bwa mem -t 4 StrainA_predicted_phages.fasta StrainA_phage_gapfilled.fasta > phage_assembly_vs_predicted.sam\nnone of the reads or assemblies mapped to any of the predicted prophage sequences.\nI then aligned the same experimental phage sequences (both reads and the assembled genome) directly to the full bacterial genome (StrainA_genome.fasta), expecting at least partial matches in the prophage regions. However, there were still no alignments.\n  # Also tried aligning both to the host genome:\n  bwa mem -t 4 StrainA_genome.fasta StrainA_phage_gapfilled.fasta > phage_vs_host.sam\nThis was very unexpected — if the prophages are indeed part of the genome, I would expect at least some reads to map to those regions.\nI also tried the same comparisons using Minimap2/bowtie2/blast, even tuned parameters to allow more mismatches and gaps. Unfortunately, there were no alignments..\nTo verify the identity of the experimentally obtained phage sequences, I submitted StrainA_phage_gapfilled.fasta to NCBI ORF Finder, extracted several of the longer predicted protein sequences, and then ran BLASTp against the virus (taxid:10239) database. These searches returned high-confidence hits to known phage proteins, supporting the idea that the experimental sequences are indeed phage-derived.\nAdditionally, I extracted CRISPR spacer sequences from Strain A using CRISPRCasTyper, and tried to align them (using BLAST) to the gap-filled experimental phage genome. Only one spacer hit was found, with a relatively low score (e-value ~1e-2).\nI am now wondering:\n1. Could this complete lack of alignment be caused by my alignment strategy?\nAre there better tools or methods specifically suited to phage–prophage or phage–host genome comparisons?\n2. Could experimental artifacts explain this?\nI do not perform the experimental work myself. Could the phage DNA isolation,amplification, or assembly methods introduce chimerism or unrelated sequences that mislead mapping?\n3. Or is it genuinely common to see such divergence between experimentally isolated phages and prophage predictions from the same strain genome?\nAny thoughts or suggestions on improving the alignment strategy or interpreting these results would be greatly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612831/",
    "title": "Time scaled tree with IQTree giving off looking branch length",
    "question": "Hi everyone!\nI'm having some weird results using IQTree to build a time-scaled tree. I have used the tool before but this is my first time doing this type of analysis. I have a set of 80 E. coli which I have submitted to an SNP analysis. I have now a core SNP alignment that I have run through Gubbins to remove recombination sites. This alignment I fed it to IQTree and used the time reconstruction like this:\niqtree -s alignment.fasta --date date.tsv\nWhere date.tsv is a two-column file with sample name and isolation date (Between 2012 and 2020).\nI took the generated tree to IToL and plotted the branch lengths as time and this is (a fragment of) what came out:\nAs you can see the dates seem to be extremely large, plus shorter branches have a longer time distance. I wonder if I'm misinterpreting the whole thing or if doing an SNP alignment is simply too much since it will find too many differences.\nAny help would be greatly welcome.",
    "answers": [
      "Did you remove invariant (constant) sites from the SNP alignment? If so, you should probably also be specifying an ascertainment bias correction. See: http://www.iqtree.org/doc/iqtree-doc.pdf#page=158.20\nAn ascertainment bias correction (+ASC) model (Lewis, 2001) should be applied if the\nalignment does not contain constant sites (such as morphological or SNPs data). \n\nFor example:\n• MK+ASC: For morphological data.\n• GTR+ASC: For SNPs data.\n+ASC will correct the likelihood conditioned on variable sites. Without +ASC, the branch\nlengths might be overestimated."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612835/",
    "title": "Signature Matrix Modifications/Subclustering",
    "question": "Hi all, I am trying to tease out a rare cell type, ILC2s, from scRNA-seq data from human liver samples. Is it possible to modify an already existing signature matrix? Also having difficulty getting clean subclusters of NKs, ILC1, ILC2, ILC3s. Any advice would be greatly appreciated. I am using the most updated Seurat version for my workflow, with the end goal being a signature matrix for CIBERSORTx.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612841/",
    "title": "Picard CollectRawWgsMetrics runs much slower on non-Apple chip machine",
    "question": "After some testing, we found a ~10x speedup by applying --READ_LENGTH 1000000 and --USE_FAST_ALGORITHM true to Picard's CollectRawWgsMetrics and the metrics were identical to the 1^-05 decimal place compared to output without USE_FAST_ALGORITHM and default READ_LENGTH (150bp).\nSo we applied the same settings to an Intel-based CENTOS 7 machine, but saw no speedup at all.\nApple M-chip:\nConfiguration:\n[Fri May 30 14:57:43 EDT 2025] Executing as ___________ on Mac OS X 15.4.1 aarch64; OpenJDK 64-Bit Server VM 22.0.1+8; Deflater: Jdk; Inflater: Jdk; Provider GCS is available; Picard version: Version:3.3.0\nCollectRawWgsMetrics call:\nCollectRawWgsMetrics --INPUT 22-284-01352B.bam --OUTPUT CollectRawWgsMetrics_output_txt.txt --INCLUDE_BQ_HISTOGRAM true --USE_FAST_ALGORITHM true --READ_LENGTH 1000000 --REFERENCE_SEQUENCE ./picard/GRCh38_full_analysis_set_plus_decoy_hla.fa --MINIMUM_MAPPING_QUALITY 0 --MINIMUM_BASE_QUALITY 3 --COVERAGE_CAP 100000 --LOCUS_ACCUMULATION_CAP 200000 --STOP_AFTER -1 --COUNT_UNPAIRED false --SAMPLE_SIZE 10000 --ALLELE_FRACTION 0.001 --ALLELE_FRACTION 0.005 --ALLELE_FRACTION 0.01 --ALLELE_FRACTION 0.02 --ALLELE_FRACTION 0.05 --ALLELE_FRACTION 0.1 --ALLELE_FRACTION 0.2 --ALLELE_FRACTION 0.3 --ALLELE_FRACTION 0.5 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\nMemory utilization during processing:\n30-32GB\nCENTOS Intel-chip\nConfiguration:\n[Fri May 30 15:12:46 EDT 2025] Executing as ____________ on Linux 4.18.0-513.18.1.el8_9.x86_64 amd64; OpenJDK 64-Bit Server VM 17.0.8.1+1; Deflater: Jdk; Inflater: Jdk; Provider GCS is available; Picard version: 3.4.0\nCollectRawWgsMetrics call:\nCollectRawWgsMetrics --INPUT 22-284-01352B.bam --OUTPUT CollectRawWgsMetrics_output_txt.txt --INCLUDE_BQ_HISTOGRAM true --USE_FAST_ALGORITHM true --READ_LENGTH 1000000 --REFERENCE_SEQUENCE ./picard/GRCh38_full_analysis_set_plus_decoy_hla.fa --MINIMUM_MAPPING_QUALITY 0 --MINIMUM_BASE_QUALITY 3 --COVERAGE_CAP 100000 --LOCUS_ACCUMULATION_CAP 200000 --STOP_AFTER -1 --COUNT_UNPAIRED false --SAMPLE_SIZE 10000 --ALLELE_FRACTION 0.001 --ALLELE_FRACTION 0.005 --ALLELE_FRACTION 0.01 --ALLELE_FRACTION 0.02 --ALLELE_FRACTION 0.05 --ALLELE_FRACTION 0.1 --ALLELE_FRACTION 0.2 --ALLELE_FRACTION 0.3 --ALLELE_FRACTION 0.5 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false --TMP_DIR /scratch/moleculardiagnosticlab/tmp\n(NOTE: the CENTOS machine is a part of a HPC cluster so we use the SSD scratch space as the TMP_DIR).\nMemory utilization during processing:\n31.22 GB (max)\nThoughts\nThe obvious answer could be \"Wow those M-chips really are fast!\" Not sure that explains the difference though, since using default params and no USE_FAST_ALGORITHM reverts back to the long processing time on the M-chip as well.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612838/",
    "title": "Reporting Imputed SNPs in GWAS",
    "question": "Hello everyone!\nI'm curious about the practice of reporting SNPs from imputed data in GWAS studies. Are imputed SNPs typically reported, and if so, how are they validated?\nAdditionally, what's the rationale behind doing imputation after GSA (Global Screening Array) experiments and reporting Genome-wide significant SNP/ suggestive SNP coming from the imputed data when whole-genome sequencing (WGS) could potentially yield similar results?\nI'd appreciate any insights or clarifications. Thanks!\n-Aastha",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612834/",
    "title": "Does Beagle use the reference genome for both phasing AND imputing genotypes?",
    "question": "Hi, I am phasing/imputing my genotypes and I am using Beagle. I am providing a reference genome for the imputation and I am not 100% confident that Beagle uses the reference panel (ref) for both phasing the genotypes and imputing them. I know it uses the ref for imputation but I was wondering if it also uses the same reference for phasing.\nAnyone can confirm?\nThanks!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612833/",
    "title": "In GERPplusplus, odd gerpelem outputs",
    "question": "I use the GERPplusplus software to find constrained elements in multiple sequences alignements (msa). First I calculate the Rejected Substitution (RS) score for each positions using the command gerpcol. Then, the command gerpelem characterize constrained regions using the ouptup of the first command.\nAccording to the documentation, the outputs of gerpelem are:\nstart   end length       RS-score   p-value\nBut my outputs do not match:\nregion  1975    3125    2269.7  7.66825e-28 1150    0   0\nregion  3279    3684    841.284 9.91585e-17 1555    0   0\nregion  825 884 140.106 9.70729e-09 1614    0   0\nregion  494 698 398.796 7.29589e-06 1818    0   0\nregion  405 427 54.86   7.61356e-06 1840    0   0\nThe order and number of the columns do not match. And even though the sixth column seems to be the length of the region, assuming column 2 is start and column 3 end of the region, it is the case for the first line, not for the following lines.\nI am very tempted to believe that column 3 is the RS/GERP score, but with a limited understanding of the results, it seems difficult to trust these numbers.\nHas anyone used gerpelem before? Is it a normal behaviour for this software?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612828/",
    "title": "required annotations for VQSR",
    "question": "Hi, I have a sample that doesn’t have the required annotations for Variant Quality Score Recalibration (VQSR). Since I don’t have any other files to regenerate the VCF with these annotations, I’m wondering how I should proceed.\nIs it possible to continue without applying VQSR and its recalibration? If yes, how should I filter this sample?\nAlso, besides hard-filtering, are there any other filtering methods I could use in this case?\nThanks a lot for your help",
    "answers": [
      "Hi, you're in a common scenario when working with variants that lack the necessary annotations for Variant Quality Score Recalibration (VQSR). Here's a clear breakdown of your options:\nCan you proceed without VQSR?**\nYes, you can absolutely proceed without VQSR. VQSR is ideal, especially for large datasets and high-quality cohorts, but it requires:\nA large number of variants (typically >30 samples)\nKnown resources (e.g., HapMap, Omni, dbSNP)\nSpecific annotations (e.g., QD, MQ, FS, ReadPosRankSum, etc.)\nIf your sample doesn't have those annotations and you can't regenerate the VCF, then VQSR is off the table.\nHow should you filter your sample instead?\n1. Hard Filtering (GATK Recommended)\nFor single samples or datasets lacking the necessary annotations/resources, GATK recommends hard filtering — applying fixed thresholds to annotations to separate true variants from artifacts.\nHere is a typical hard-filtering thresholds for SNPs and indels:\nFor SNPs:\nFILTER: QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0\nFor Indels:\nFILTER: QD < 2.0 || FS > 200.0 || ReadPosRankSum < -20.0\nUse GATK VariantFiltration to apply these filters. Note: If your VCF lacks these annotations entirely (e.g., QD, FS, MQ), then you'll need to filter based on what's available (e.g., QUAL score and depth).\nAlternative Filtering Methods\nBesides hard filtering, here are some other options depending on your setup:\n2. bcftools filter\nYou can apply custom expressions on fields like QUAL, DP, and AF:\nbcftools filter -e 'QUAL < 30 || DP < 10' input.vcf -o filtered.vcf\n3. Machine learning / ensemble filtering\nIf you're working with tools like DeepVariant, Octopus, or Strelka, they often include internal filtering or confidence scores which can be used.\n4. Custom scoring\nSome pipelines develop sample-specific filtering heuristics (e.g., based on known variant concordance, sample depth distribution, or variant allele frequency). This can be done in Python, R, or with tools like vcftools.\n5. Use of population-level knowledge (if applicable)\nIf you're genotyping a sample from a well-characterized population (e.g., 1000 Genomes, gnomAD), you can cross-reference known variants to boost confidence.\nHope this helps friend"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612805/",
    "title": "Identify transgene insert loci/copy number in nanopore De-novo assembly",
    "question": "Hello,\nI am trying to identify the insert loci and copy number of a transgenic expression cassette in a yeast. The cassette contains my gene of interest with a proprietary promoter and a marker. I have a tried and true workflow for use with Illumina short reads i.e. align reads to reference+cassette, call variants, compare coverages for copy number and identify SVs for insert locus, but now I want to use ONT long reads to generate a de-novo assembly which actually contains the cassette inserts.\nHere's what I know from short read analysis: there are two suspected insert loci based on SV analysis, and I believe the cassette inserted itself several times back-to-back at each locus based on comparison of coverage at the marker gene vs. surrounding coverage along the rest of the chromosome when aligned to the chromosome-level reference genome. However, the exact copy number at each locus remains uncertain.\nI created a de-novo assembly with Flye, and although it does successfully assemble the cassette, it is only present within its own scaffold that spans its length, and does not integrate into any of the larger scaffolds. I tried generating several assemblies experimenting with various parameters within Flye and got the same results, with some assemblies omitting the cassette entirely.\nMy suspicion as to why this is happening is that the average read length in my ONT library is ~3Kbp, while the total length of the cassette is ~4.4Kbp. So, because there are multiple true inserts densely crowded around one another, and the average ONT read is shorter than the length of the cassette, I suspect the assembler is unable to resolve the reads that span across the breakends of the inserts as they appear to conflict with one another, thus making it impossible for the assembler to tell that there are multiple copies and/or in what order they exist, so it just assembles one copy into its own scaffold and calls it a day...\nDoes this seem reasonable? Any advice on further steps i.e. gap closing, etc.?\nThanks!",
    "answers": [
      "Interesting, your analysis might be correct.\nOne way forward\ngrep fastq reads which contain parts of your insert (left side, right side, center etc). You could use: https://github.com/fulcrumgenomics/fqgrep or custom scripts or grep itself\nalign these reads to the genome/contigs you have with minimap2\nfind where these reads hit/partially hit and check the alignments. Are there reads which cross from insert into the genome ?\nYou might need to annotate your contigs eg with the helixer web service, to get an impression of whats going on.\nAnother way would be attempting a reassembly, eg with Shasta or hifiasm --ont. Flye should be decent for fungi though."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612793/",
    "title": "Heatmap for visualizing the differentially expressed genes in RNA-seq data",
    "question": "Hello everyone,\nI have RNA-seq count data that includes 8 different sample groups with unequal replication. I want to visualize this count data for all 8 sample groups in a single heatmap. The heatmap should clearly differentiate between up-regulated and down-regulated genes among the different sample groups. However, the differentially expressed genes identified through pairwise comparisons using DESeq2 analysis are not showing clear differentiation in the heatmap plot. I have tried using log2 TPM normalized values for the heatmap. Is my analysis statistically correct, or should I approach this differently? Please help me.",
    "answers": [
      "You have to scale the counts first to emphasize differences. Clustering based on unscaled values emphasizes the magnitude of counts, not differences. See for a minimal example Scaling RNA-Seq data before clustering?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612818/",
    "title": "single-cell differential expression of unbalanced cell number",
    "question": "Hi Biostars community,\nI have a question regarding the appropriate approach for differential expression testing of scRNA-seq data between two cell groups with unbalanced cell numbers.\nWithin the total cells sequenced from a single patient, a rare novel cell subpopulation of ~200 cells was discovered. I wanted to identify genes that are differentially expressed between this small subpopulation and the rest of the cells, which comprise ~4000 cells. I tried Seurat embedded Wilcoxon, LR and MAST and got very similar results. But I'm unsure if these methods are the most suitable in my scenario.\nDoes anyone have any recommendations for the most suitable methods? Much appreciated!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9607362/",
    "title": "Differential Expression tests for cell-type clusters from a single-cell RNAseq dataset with no biological replicates",
    "question": "Dear All,\nI am presented a single-cell RNAseq dataset that has two treatment conditions. Each condition had three biological replicates but due to a mistake the three replicates in each condition were merged during sequencing - so the sample origin of the cells is not available.\nAfter identifying several cell-type clusters from this dataset, I am now looking for a method to determine the differentially expressed genes (DEGs) between the treatment conditions for each cluster. One of my goals is to identify cell-type clusters that were the most strongly affected by treatment.\nSince the sample IDs are lost, the traditional pseudobulking and mixed model strategies do not seem to be possible. I have also tried the following:\nVarious DE tests (including GLM, Wilcoxon, etc.) for each cluster independently. Using a fixed FDR threshold for all clusters, the number of DEGs is badly affected by the number of cells in a cluster (verified by a series of downsampling tests).\nThe Augur classification test. Unfortunately clusters with high AUCs in the Augur test did not coincide with those with higher numbers of DEGs in Part 1 (some of them even had no DEGs).\nCould anyone recommend either (a) tests or algorithms that robustly detect DEGs, independently of cluster size; (b) a pseudobulking, or pseudo-replication strategy for single-sample data; or (c) methods that do not rely on DEGs to show effect of a condition on the clusters?\nAny input would be appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612814/",
    "title": "Annotation Issue using miRBASE gff",
    "question": "Hello,\nSo as always I run into issue upon issue as I try to develop a pipeline for small RNA analsysis using Qiagen mirna Library kit. So I aligned against my genome and got some successful results, but when it came to annotating using the gff on miRBase no features popped up. The reference genome I used is a prebuilt bowtie index of the NCBI reference since it was available and I dont have the computing power to build an index on my computer. I am now stuck and really dont know how to continue forward, Im trying to use another build for alignment maybe its a coordinate issue or something Im not sure any help would be greatly appreciated Im so close to getting results, I know it but I grow disheartened as a significant amount of time was spent building this and frankly I dont just want to give up.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612810/",
    "title": "Evo 2 webinar - 1 day left to register",
    "question": "One day left! Join Dr. Brian Hie to discuss Evo 2 and the future of AI-driven life sciences. Date: Tuesday, June 10 Time: 9:00 AM PDT | 12:00 PM EDT | 6:00 PM CEST. Sign up here: https://bit.ly/Evo2webinar",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612807/",
    "title": "Error Scaffolding with RagTag",
    "question": "We performed high-fidelity (HiFi) whole genome sequencing of two wheat cultivars, Madsen and Pritchett, using the PacBio Revio Circular Consensus Sequencing (CCS) platform. The high-accuracy long reads were first assembled into contigs using Hifiasm. Post-assembly, we conducted quality control and completeness assessments using tools such as BUSCO and Gfastats. For downstream scaffolding, we employed RagTag using the high-quality genome of the wheat cultivar ‘Attraktion’ as the reference assembly.\nHowever, I’m facing challenges with my reference-guided scaffolding project using RagTag and could use your insights. Madsen and Pritchett has nearly identical BUSCO scores (C: 99.7% [S: 2.0%, D: 97.7%], F: 0.2%, M: 0.1%, n: 4896, E: 0.4%). Madsen has 4424 contigs, and Pritchett has 2754, both assembled with Hifiasm. The genomes are about 14Gb big.\nI successfully scaffolded Madsen using RagTag, but Pritchett consistently fails with the same SLURM script and pipeline. For Pritchett, the job runs for ~7 days, reports as “completed,” but produces no ragtag.scaffold.fasta. The ragtag.scaffold.asm.paf.log is not complete and gets terminated at same point everytime.\nError says:\nTraceback (most recent call last):\nFile “/home/…/bin/ragtag_scaffold.py”, line 577, in <module>\nmain()\nFile “/home/…/bin/ragtag_scaffold.py”, line 420, in main\nal.run_aligner()\nFile “/home/…/BPN/lib/python3.10/site-packages/ragtag_utilities/Aligner.py”, line 128, in run_aligner\nrun_oe(self.compile_command(), self.out_file, self.out_log)\nFile “/home/…/lib/python3.10/site-packages/ragtag_utilities/utilities.py”, line 73, in run_oe\nraise RuntimeError(“Failed : minimap2 -x asm5 -t 24 … > ragtag.scaffold.asm.paf 2> ragtag.scaffold.asm.paf.log”)\nThe Slurm Job I gave was:\n#SBATCH --partition=abc\n#SBATCH --cpus-per-task=24\n#SBATCH --mem=1500000\n#SBATCH --time=14-00:00:00\nragtag.py scaffold “$REF” “$QUERY” -o “$OUT” -t 24 -u\nTroubleshooting Steps: Ran minimap2 manually on Pritchett’s reference (attraktion.fasta) and query (pt2_busco.fa); it generated a 442 MB .paf file in ~21 hours. Came to know that RagTag does not use pregenerated paf file.\nTested RagTag on a Pritchett subset (~409 Mbp, 10 contigs); it succeeded in ~10 hours, placing 9/10 sequences (~402 Mbp).\nSomeone suggested that with large genomes, minimap2 might struggle due to multi-indexing issues that can slow things down or cause memory overload. They recommended indexing the reference with minimap2 using -I 20G (which should be suitable for wheat) and then passing the prebuilt .mmi index directly to RagTag as if it were a FASTA file. I followed this approach — created the .mmi file and used it in RagTag — but unfortunately, it still didn’t resolve the issue with Pritchett.\nUsed SLURM settings: bigmem, 24 CPUs, 1.5 TB memory, 14-day limit, BPN environment (RagTag v2.1.0)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612800/",
    "title": "biothings-mcp - MCP server to query genes, variants, chemicals, taxons",
    "question": "I'm a developer on this project and wanted to introduce biothings-mcp. It's an MCP server that wraps the BioThings.io APIs to provide foundational biological data, including gene, variant, chemical, and taxonomic information which can be called by AI chat-bots\nIt can be used to answer questions like:\n\"What are the functions and pathways associated with the human gene BRAF?\"\n\"Find all variants in the EGFR gene and provide their clinical significance from ClinVar.\"\n\"What is the molecular formula and structure of the compound 'aspirin'?\"\n\"Download the protein FASTA sequence for UniProt ID P04637.\"\n\"What is the taxonomic lineage for Mus musculus?\"\nIt's packaged as an MCP server, allowing you to plug its functionality directly into AI environments. For instance, you can add it to IDEs like Cursor or VS Code Copilot, or chatbots like Claude, to access BioThings data from your AI assistant.\nThe tool is available at: https://github.com/longevity-genie/biothings-mcp\nUsage example:",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612799/",
    "title": "synergy-age-mcp - a database of synergistic and antagonistic genetic interactions in longevity",
    "question": "As a core developer, I'm sharing synergy-age-mcp, an MCP server for the curated database of experimentally validated data on genetic interventions affecting lifespan. It focuses on synergistic, antagonistic, and epistatic interactions.\nIt can help answer questions like:\n\"Which genetic interventions show the strongest synergistic effects in C. elegans?\"\n\"What is the relationship between daf-2 and daf-16 mutations in C. elegans longevity?\"\n\"How do insulin signaling pathway interventions compare between worms, flies, and mice?\"\n\"Which genetic combinations show antagonistic interactions that reduce lifespan benefits?\"\n\"How do caloric restriction mimetics interact with other longevity interventions?\" This server uses the Model Context Protocol (MCP), so you can connect it to AI assistants like Claude Desktop or IDEs like Cursor to have direct access to its query capabilities from your chat.\nThe tool is available at: https://github.com/longevity-genie/synergy-age-mcp\nUsage example:",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612798/",
    "title": "opengenes-mcp - MCP server on the database for longevity genes",
    "question": "I'm one of the core developers and wanted to introduce opengenes-mcp, an MCP server for OpenGenes, the database on the genetics of aging and longevity\nIt's useful for answering research questions such as:\n\"Which genetic interventions extended the lifespan of mice the most?\"\n\"What polymorphisms in FOXO3 are associated with human longevity?\"\n\"Which hallmarks of aging are associated with the KL (Klotho) gene?\"\n\"Are there any liver-specific interventions that increase lifespan in mice?\"\n\"Find genes that are associated with both longevity in human studies and have lifespan-extending effects in model organisms.\"\nA key feature is that it's an MCP server, designed for easy integration with AI tools. You can connect it to AI chat clients like Claude or IDEs like Cursor and Windsurf to query the data conversationally.\nThe tool is available at: https://github.com/longevity-genie/opengenes-mcp\nUsage example:",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612797/",
    "title": "gget-mcp - a bioinformatics toolkit for genomics queries and analysis (based on gget lbrary)",
    "question": "As a core developer, I wanted to share gget-mcp with the community. It's a tool that wraps the popular get library and provides a rich set of functionalities for sequence analysis, functional genomics, and structural biology that AI chatbots can call.\nIt can be used to answer questions like:\n\"Find information about the human TP53 gene and get its protein sequence.\"\n\"Perform enrichment analysis for a set of cancer-related genes: TP53, BRCA1, BRCA2, ATM, CHEK2.\" \"Get the 3D structure information for the protein encoded by the EGFR gene from AlphaFold.\"\n\"Find mutations in the COSMIC database for the PIK3CA gene.\" \"Perform a BLAST search with the DNA sequence 'ATGGCGCCCGAACAGGGAC' to identify its origin.\"\n\"Align multiple protein sequences to find conserved regions.\"\nBecause it's an MCP server, you can easily add it to chatbots (like Claude Desktop) or IDEs (like Cursor, Windsurf, or VS Code Copilot) to access these features directly from your AI-powered chat.\nThe tool is available at: https://github.com/longevity-genie/gget-mcp\nExample of a chat:",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612792/",
    "title": "Best pairwise alignment for insertion and deletion-containing sequences",
    "question": "Hi folks,\nWhat is the most accurate Biopython function for performing pairwise alignment of a 200-bp sequence against its reference? Some of my sequences may contain insertions and deletions, so handling indels accurately is important. I’d appreciate any suggestions or recommendations.\nFor example for a mutant read with insertion and deletions, I am interested in getting \"Expected alignment\" instead of \"Alignment 1\":",
    "answers": [
      "Hello, I'd try tweaking the Affine Gap Scores of Align.PairwiseAligner(), and if those are not good enough, consider the possibilty to specify your own gap scoring function.\nHth."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612773/",
    "title": "How to deal with intermediate files",
    "question": "Sometimes during an analysis we create intermediate files that take a long time to generate (results of numerical analyses, bioinformatics pipelines, etc) and that can subsequently be used for graphs and reports. Where do you save those so that they can be used from a different computer or shared with a collaborator?\nFor example, I may do compute-intensive long analyses on our HPC, but then on my laptop I would do the report with graphs, etc. The code and report code I can share between HPC and my laptop and collaborators using git, but how about the analyses results? What do you do? I don't have a good systematic method (I rely on rsync). Furthermore, in collaborations (or just between my HPC and the laptop) it is easy to share the code using git, but how about sharing intermediate results files?\nAny suggestions appreciated.",
    "answers": [
      "There are a bunch of options, and I've seen a lot of different methods over the years. Here are a few of the common ones I've seen/used:\nMost universities I've worked at have a subscription to one of the major file sharing services (Google Drive, OneDrive, DropBox, etc...).\nSFTP/SCP.\nGlobus.\nSending physical hard drives (the least efficient, but somehow a sequencing company once thought this was appropriate).\nIt really depends on the number, size, and sensitivity of the files. Email will suffice for most small files and can be encrypted easily. Otherwise, compressing and encrypting are your best bets to make things more manageable. Sometimes, it's not feasible to share intermediate files, so sending the scripts used to generate output from raw data with appropriate seeds sets should generate the same data, but then you probably should to also send containers to do the analysis in to ensure consistency. Useful for things that generate silly amounts of data like linkage analyses (for example)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612778/",
    "title": "Quality control metrics in mmCIF",
    "question": "I have downloaded from PDB 291 mmCIF files that have information for P53 structures. 289 of them have information for experimental strutures: 248 files correspond to X-RAY diffraction structures, 30 for NMR and 11 for EM. I would like to perform a quality control of the files, but I'm new to this format and I don't know which metrics I should check. Considering that the structures come from three different types of experiments, which metrics do you recommend checking in each type of experiment and is there any recommended threshold for them?\nThanks in advance",
    "answers": [
      "There is a lot of useful information in the mmCIF file headers that can be relevant. Each method will have slightly different information available that will be relevant.\nHere are a few of the more important factors I would take into consideration:\nResolution (lower Å is better)\nR-work/free (quality and agreement of proposed models; x-ray only)\nConformer agreement (RMSD; NMR)\nClashscore (not in header; number of clashing atoms per 1000 atoms)\nThe list goes on, so it would be worth exploring the information in the headers and see what is relevant for your use case."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612783/",
    "title": "phage sequence no alignment to genome",
    "question": "I am currently working with a bacterial strain (let’s call it Strain A) for which I have a complete genome assembly (StrainA_genome.fasta). From this strain, I obtained two sets of phage-related sequences using different experimental approaches:\n1. An assembled phage genome from lab experiments: StrainA_phage_gapfilled.fasta\n2. A pair of short-read sequencing files from another phage preparation: StrainA_phage_R1.fasta and StrainA_phage_R2.fasta\nSeparately, I predicted prophage regions on StrainA_genome.fasta using Cenote-Taker 3 and PhageBoost, and collected all the predicted prophage sequences into a file: StrainA_predicted_phages.fasta.\nMy goal is to verify whether the experimentally derived phage sequences correspond to any of the predicted prophage regions, i.e., to confirm that the experimentally recovered sequences truly originate from prophages within Strain A.\nWhat I tried\nI attempted various alignments, including:\nI used bwa mem to align both the raw phage reads and the gap-filled phage assembly against the predicted prophage regions, using commands like:\n # Align short reads to predicted phages \n  bwa mem -t 4 StrainA _predicted_phages.fasta StrainA_phage_R1.fasta StrainA_phage_R2.fasta > phage_reads_vs_predicted.sam\n\n # Align assembled phage genome to predicted phages\n  bwa mem -t 4 StrainA_predicted_phages.fasta StrainA_phage_gapfilled.fasta > phage_assembly_vs_predicted.sam\nnone of the reads or assemblies mapped to any of the predicted prophage sequences.\nI then aligned the same experimental phage sequences (both reads and the assembled genome) directly to the full bacterial genome (StrainA_genome.fasta), expecting at least partial matches in the prophage regions. However, there were still no alignments.\n  # Also tried aligning both to the host genome:\n  bwa mem -t 4 StrainA_genome.fasta StrainA_phage_gapfilled.fasta > phage_vs_host.sam\nThis was very unexpected — if the prophages are indeed part of the genome, I would expect at least some reads to map to those regions.\nI also tried the same comparisons using Minimap2/bowtie2/blast, even tuned parameters to allow more mismatches and gaps. Unfortunately, there were no alignments..\nTo verify the identity of the experimentally obtained phage sequences, I submitted StrainA_phage_gapfilled.fasta to NCBI ORF Finder, extracted several of the longer predicted protein sequences, and then ran BLASTp against the virus (taxid:10239) database. These searches returned high-confidence hits to known phage proteins, supporting the idea that the experimental sequences are indeed phage-derived.\nAdditionally, I extracted CRISPR spacer sequences from Strain A using CRISPRCasTyper, and tried to align them (using BLAST) to the gap-filled experimental phage genome. Only one spacer hit was found, with a relatively low score (e-value ~1e-2).\nI am now wondering:\n1. Could this complete lack of alignment be caused by my alignment strategy?\nAre there better tools or methods specifically suited to phage–prophage or phage–host genome comparisons?\n2. Could experimental artifacts explain this?\nI do not perform the experimental work myself. Could the phage DNA isolation,amplification, or assembly methods introduce chimerism or unrelated sequences that mislead mapping?\n3. Or is it genuinely common to see such divergence between experimentally isolated phages and prophage predictions from the same strain genome?\nAny thoughts or suggestions on improving the alignment strategy or interpreting these results would be greatly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612780/",
    "title": "Mitochondrial Annotation Tool",
    "question": "Hi,\nI am working on de novo assembly of bovine mitochondrial genome. I have done assembly and I need to annotate the genome and generate circular genome plot. The Mitos2 webserver is not working currently and I am unable to run the galaxy version.\nPlease suggest any other online tool or command line tool that can be used for this analysis.\nLooking forward for suggestions.\nThanks",
    "answers": [
      "Hello, for the annotation you could try Prokka using the option --kingdom Mitochondria (--gcode 5).\nHth."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612771/",
    "title": "Assessing the difference between a de novo assembly and a reference genome",
    "question": "I have recently generated an assembly using Hifiasm. I did it by running the following command:\n hifiasm -o HG002.asm --ont -t32 HG002-ont.fq.gz\nI extracted chromosome 1 from my assembly (Query) and proceeded to extract chromosome 1 from the (Reference) The goal was to compare there two and see what are the differences and what are the similarities. This is the script that I used:\n#!/bin/bash \nmodule add mummer/4.0.0rc1 \ndnadiff Reference_chr1.fa chr1_asm.fa \nThis is part of the report, and I need assistance in interpreting some of the results. Specifically, I’d like to understand:\nBreakpoints: What exactly are they, and why is the count identical in both cases?\nInsertions: In my query and the reference, are these insertions novel?\nSNPs :Why is the number of SNPs the same in both my query and the reference? In the attached image, the second column represents the Reference, and the third column is the Query.",
    "answers": [
      "Hello, you will find informations on dnadiff's output in its documentation's README; also notice that\nFor more information on the formats and meanings of all the files\nproduced, please see the documentation for the corresponding utility.\nHth."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612776/",
    "title": "HISAT2 or STAR",
    "question": "HISAT2 and STAR both are used for aligning our reads to reference genome. But I do not know what the actual difference between them is. When and why should I use either of them?",
    "answers": [
      "Generally, they are equivalent. They both are designed to handle spliced reads (e.g. RNA-seq) well.\nHISAT2 may be better if you are on a memory-limited machine. STAR is designed to use large amounts of RAM for quick and efficient alignments. STAR has a lot of functionality and maybe is more easily customizable (in my opinion the STAR manual is more comprehensive, but I haven't looked at HISAT2 in a while.).\nIf you are working on a genome other than Mouse or Human, I find HISAT2 to be easier to use \"out of the box.\" For example, with STAR the indexing process usually needs parameter adjustment for smaller genomes.\nDepending on your use case, I wouldn't worry too much about the differences if you're just doing general analysis, like alignment and quantification of counts over protein coding genes.\nIf you are trying to perform more customized, specific, complex, or sensitive analysis, then maybe the differences and tunable parameters might matter more."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612775/",
    "title": "What to do after fastp?",
    "question": "Hello everyone, I am a newbie in bioinformatics and doesn't know much about it. I am assigned to do RNA-Seq analysis of a knockdown sample. Being a totally beginner, I am self-learning RNA-Seq, but I am still facing a lot of difficulties in understanding all the steps. I have understood and done till fastp of my data, but how to carry out a good analysis after this? To all the bioinformatics gurus, tell me the best resources and courses so that I can master this skill too. Along with this tell me steps after fastp.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612720/",
    "title": "SNP calling from multiple genomes: No valid SNPs",
    "question": "Hey,\nI have multiple whole genome sequences, but want SNP data to perform some GWAS, and to calculate a GRM. I used minimap2 to align all genomes against the reference, then used samtools to binarize, sort, and index the resulting .sam files. Then I used bcftools mpileup and bcftools call to get .vcf files, one for each of the genomes (except the reference). Then I use bcftools merge to get a single .vcf, and plink --recode --vcf merged.vcf --out merged and plink --file merged --make-bed --out merged to get the corresponding PLINK files. However, when I want to e.g. filter for minor allele frequency with PLINK, it says Error: All variants removed due to minor allele threshold(s). When I use GCTA directly to build a GRM, it says 1356568 SNPs have been processed. Used 0 valid SNPs..\nWhen converting the .ped file to a csv with some cat command from the internet, the table contains 0, G, C, T, A, and there are SNPs with 2, 1, but also no 0 entries.\nI am very new to this field. Where in this pipeline could be the error? How could I check what is wrong with my data?\nAny help is much appreciated.",
    "answers": [
      "The typical SNP calling route is to run say 3 short read datasets (fastq input) against one reference genome. Then your approach would likely work.\nOnly now are we starting to see SNP calling from genome to genome (both fasta, i.e not fastq) comparisons, which is not as well catered for in terms of toolsets. I don't know of any toolchains which can do what you want starting from genome to genome alignments.\nMaybe you can fake a higher depth in your vcf/bcf (since depth 1 which is max in a genome to genome comparison is likely to be seen as a false positive if tools are set to expect 30X coverage WGS)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612770/",
    "title": "ATAC-seq analysis",
    "question": "I am running macs3 hmmratac on the ATAC-seq bam files with the command below:\n macs3 hmmratac --cutoff-analysis-only -i sample.bam -f BAMPE \nand getting the following the error:\nRandom seed selected as: 10151\nUse --hmm-type to select a Gaussian ('gaussian') or Poisson ('poisson') model for the hidden markov model in HMMRATAC. Default: 'gaussian'.\n\nINFO  @ 08 Jun 2025 21:31:10: [107 MB] #1 Read fragments from BAMPE file...\nINFO  @ 08 Jun 2025 21:31:15: [144 MB]  1000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:18: [153 MB]  2000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:23: [154 MB]  3000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:27: [167 MB]  4000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:31: [170 MB]  5000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:36: [178 MB]  6000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:40: [186 MB]  7000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:45: [193 MB]  8000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:49: [199 MB]  9000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:53: [215 MB]  10000000 fragments parsed\nINFO  @ 08 Jun 2025 21:31:57: [225 MB]  11000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:00: [235 MB]  12000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:04: [244 MB]  13000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:08: [246 MB]  14000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:12: [256 MB]  15000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:15: [273 MB]  16000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:19: [282 MB]  17000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:23: [293 MB]  18000000 fragments parsed\nINFO  @ 08 Jun 2025 21:32:26: [294 MB] 18913216 fragments have been read.\nINFO  @ 08 Jun 2025 21:32:34: [294 MB] #  Read 18913216 fragments.\nINFO  @ 08 Jun 2025 21:32:34: [294 MB] #2 Use EM algorithm to estimate means and stddevs of fragment lengths\nINFO  @ 08 Jun 2025 21:32:34: [294 MB] #  for mono-, di-, and tri-nucleosomal signals...\nINFO  @ 08 Jun 2025 21:32:34: [294 MB] # A random seed 10151 has been used in the sampling function\nINFO  @ 08 Jun 2025 21:32:35: [328 MB] # Downsampled 1594057 fragments will be used for EM training...\n ValueError:  Adjust --means and --stddev options and re-run command\nAfter alignment, I have only considered the reads mapped to Chr1-22, and X and Y, followed by marking duplicates using Picard, using samtools with -f 2 -F 1548 -q 30 for filtering and finally usingbedtools intersect -nonamecheck -v -abam to remove the blacklisted regions. What could be the reasons for the error at hmmratac step? I tried running macs3 hmmratac on the bam file obtained just after the alignment without involving any downstream filtering steps, and it runs absolutely fine without any error.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612767/",
    "title": "Bioconductor scRNAseq tutorial constantly breaks down due to dependencies",
    "question": "I'm currently trying to follow the scRNAseq tutorial listed here...using ubuntu/mint linux and the latest R.\nhttps://www.bioconductor.org/help/course-materials/2022/CSAMA/lab/2-tuesday/lab-04-singlecell/singlecell_CSAMA2022.html\nand its sort of annoying since its been erroring out constantly unless I install a bunch of dependencies that the tutorial doesn't mention. Each time it does so I have to go to the internet and research whatever went wrong and then install it only to get smacked with another cryptic error indicating another missing dependency.\nFor example on this line\nBiocManager::install(\"scater\")\nI am currently running down the dependency conga line I've installed\nlibcurl4-openssl-dev libfontconfig1-dev libcairo2-dev\nand now its demanding that I install I think some library for rendering Arabic script for some reason?\nI was wondering...am I going about this the wrong way? Is there some method I can use to avoid all this hassle?\nAlso related question is the linked tutorial...the 'standard recommended' way to process and analysis scRNAseq? I am not very experienced with scRNAseq so I don't know whats the consensus for the most popular/best pipeline to use since everybody seems to have their own thing. I just clicked on the tutorial that popped up. I don't know if it is good or not.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612764/",
    "title": "Trouble building combined pangenome graph with circularized genomes + embedded ORF1 paths",
    "question": "Hi VG team,\nI’m working on constructing a pangenome graph for Anelloviridae. My goal is the following:\n1.  Circularize each individual genome (using vg circularize).\n2.  Embed an ORF1 path in each graph via vg map + vg augment. \n3.  Combine all those augmented graphs into a single unified reference (vg combine), and then: \n4.  Map metagenomic reads against this global reference.\nHowever, I’m running into several issues during the process. Here’s a simplified summary of the commands I’m using:\nGraph creation per sample (in loop):\nvg construct -r sample.fasta > sample.vg\nvg paths -Lv sample.vg > sample.paths\nvg circularize sample.vg -P sample.paths > sample.circ.vg\nvg index -x sample.circ.xg sample.circ.vg\nvg gbwt -x sample.circ.xg -o sample.circ.gbwt -P --pass-paths\nvg index -g sample.circ.gcsa sample.circ.xg\nvg convert -p sample.circ.xg > sample.circ.pg\n\nvg map -F ORF1.fasta -x sample.circ.xg -g sample.circ.gcsa -m long > sample.orf1.gam\nvg augment -i sample.circ.pg sample.orf1.gam > sample_wORF1.vg\nThen I sanitize and prefix all paths and run:\nvg combine *_wORF1_prefixed.gfa > combined.vg\nvg convert -f combined.vg > combined.gfa\ngfaffix combined.gfa -o combined.fix.gfa\n\nvg convert -f combined.fix.gfa -p > combined.pg\nvg index -x combined.xg combined.pg\nvg gbwt -x combined.xg -o combined.gbwt -P --pass-paths\nvg prune -u -g combined.gbwt -k 31 -m combined.node_mapping combined.pg > combined.pruned.vg\nvg index -g combined.gcsa -f combined.node_mapping combined.pruned.vg\nIn some cases, I get errors like:\nInputGraph::InputGraph(): Cannot open node mapping file All_anelloviridae_2.node_mapping\nI think cannot find paths in graph\nIn other cases, I get an error related with duplicate path name found in graph even after prefixing each path with the sample name during vg combine.\nI’ve confirmed that: • Path names are unique (I overwrite with sample_ prefix using awk). • Each *_wORF1.vg has a proper embedded path. • The ORF1 is always mapped and augmented correctly.\nQuestions: • Is this the correct sequence for building a circular multi-sample graph with embedded ORF1 paths? • Should I embed ORF1 before or after circularization? • Does vg combine preserve embedded paths in a way that vg gbwt can consume safely? • Is there a recommended way to ensure vg gbwt doesn’t complain about duplicate or missing paths after vg combine?\nAny suggestions or clarifications would be appreciated. I’m happy to provide example files if useful.\nThanks,\nFlor @florenmartino",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9609350/",
    "title": "HOMER findMotifsGenome.pl failed compilation",
    "question": "I have been using the findMotifsGenome.pl to find enriched motifs in bed files containing CUT&RUN peaks called using SEACR, but I keep getting the error below for the known and de novo motif discovery meaning they are missing from the output. I have never had a problem running and installing HOMER on previous computers.\nRunning the command:\nfindMotifsGenome.pl relaxed.bed hg38 motif_results/ -size 200\nFirst error:\nKnown motif enrichment Missing comma after first argument to return at /Users/imb41/HOMER/.//bin/HomerSVGLogo.pm line 163, near \");\" Compilation failed in require at /Users/imb41/HOMER/bin/findKnownMotifs.pl line 10. BEGIN failed--compilation aborted at /Users/imb41/HOMER/bin/findKnownMotifs.pl line 10.\nSecond error:\nMissing comma after first argument to return at /Users/imb41/HOMER/.//bin/HomerSVGLogo.pm line 163, near \");\" Compilation failed in require at /Users/imb41/HOMER/bin/compareMotifs.pl line 11. BEGIN failed--compilation aborted at /Users/imb41/HOMER/bin/compareMotifs.pl line 11. Job finished - if results look good, please send beer to ..\nI tried reconfiguring using perl configureHomer.pl and then removing the HOMER installation, and reinstalling, but still got the same error.\nI was wondering if it could be a problem with the latest version of Perl (v5.40.1)? I can't see anything obvious wrong with line 163 of HomerSVGLogo.pm (below), but I am not familiar with working with Perl.\nsed -n '163p' ~/HOMER/bin/HomerSVGLogo.pm return $plotXY(\\@data,$xlabel,$ylabel,\\@seriesLabels,$xlow,$xhigh,$ylow,$yhigh); perl -c ~/HOMER/bin/HomerSVGLogo.pm Missing comma after first argument to return at /Users/imb41/HOMER/bin/HomerSVGLogo.pm line 163, near \");\" /Users/imb41/HOMER/bin/HomerSVGLogo.pm had compilation errors.\nThank you in advance for any help!!",
    "answers": [
      "I use the commamd conda install perl==5.32.1 solve this problem"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612762/",
    "title": "Introduction to Genome-Wide Association Studies (GWAS) | 23-27 June",
    "question": "Dear all,\nWe are excited to offer an Introduction to GWAS course designed for students, researchers, and professionals who want to learn how to design and implement a GWAS study.\nMore info & registration: https://www.physalia-courses.org/courses-workshops/course49/\nOver five days, the course will combine introductory lectures and hands-on sessions, covering:\n - Key GWAS concepts and methodologies\n\n- Building and automating a GWAS bioinformatics pipeline\n\n- Interpreting and visualizing GWAS results\nSuitable for both beginners and more advanced users with a background in genetics and some familiarity with R and the Unix command line.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/229757/",
    "title": "normalize between 2 different platform",
    "question": "I have some Non-coding RNAseq data with 2 different platform,Illumina Genome Analyzer IIx and Illumina HiSeq 2000,I want to do meta analysis and before that I should normalize them,how should I normalize between 2 different platform?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612760/",
    "title": "How to normalize bulk TCR-seq data generated by different methods and platforms",
    "question": "Dear experts,\nIt's very lucky for me to find this website as recently I encounter similar problem like Normalizing TCR data.\nThere are several TCR-seq datesets originated from different technologies and platforms like Adaptive Biotechnologies/Illumina/Ion Torrent PGM. Some of the data, there exists raw data format but as for immunoseq data, I can only obtain processed data as far as I'm concerned. The sequence depth is different varying from each otherso the abundace analysis seems very comparable. But I am afraid that such difference is caused by differnt sequence methods and platforms.\nWhat can I do to normalize these datesets or how can I intergrate these datesets? Could you please give me some suggestions and instructions on how to normalize TCR-seq data? I really need these datesets!!!\nLooking forward to your reply.\nThanks and best regards,\nLinqy",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612754/",
    "title": "Building database with snpEff -- -gtf22 option not recognized/file not read",
    "question": "Hello!\nI'm trying to create a custom database for snpEff. The documentation here (https://pcingola.github.io/SnpEff/snpeff/build_db/) calls for the option \"-gtf22\" before the name of the directory containing the GTF (and Fasta, etc). However, this option seems no longer to be recognized by snpEff, and I can't find the updated version in the documentation or in other queries. Without a \"-gtf\" option, the command produces an error stating that it can't read the input file (ie, the name of the directory containing the filies in the following command):\njava -jar snpEff.jar build -v mm37.61\nI get the same errors even doing their example build with the mouse database (with the latest mouse genome version used): mm39.114\njava -jar snpEff.jar -c snpEff_dp-db-build-4-26-25.config build -gtf22 -v mm39.114\nOR: without -gtf option: java -jar snpEff.jar -c snpEff_dp-db-build-4-26-25.config build -v mm39.114",
    "answers": [
      "Based on the ordering of the commands here: https://github.com/pcingola/SnpEff/issues/583\nI believe I just needed to place the \"build\" command before the config file specification."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612744/",
    "title": "Discussions about QC of sc-multiomic-ATAC_RNA-seq from 10xgenomics platform in terms of using repetitive elements to filter peaks",
    "question": "Recently, I am doing analysis using sc-multiomic-ATAC&RNA-seq data from 10xGenomic platform.\nStandard practice seems to suggest filtering out peaks overlapping repetitive elements to reduce technical noise and mapping artifacts. However, I'm concerned this might be throwing out the baby with the bathwater, as emerging evidence suggests many repetitive elements contain functional regulatory sequences. But in the tutorial of Signac/Seurat, there seems to be no filtering criteria related to it.\nQuestions for the Community\nWhat's the current best practice? Are you filtering all repeat-overlapping peaks, or using a more nuanced approach?\nCell-type specificity concerns: Has anyone observed cell-type-specific accessibility patterns in repetitive elements that would be lost with aggressive filtering?\nDownstream analysis impact: How much does repeat filtering affect:\nCell type identification and clustering?\nDifferential accessibility analysis?\nIntegration with scRNA-seq data?\nTrajectory analysis?\nMy Proposed Tiered Approach Based on literature review, I'm considering a tiered filtering strategy:\nTier 1: Always Remove (High Confidence); Rationale: These are likely technical artifacts with minimal regulatory potential.\nSimple repeats (microsatellites, tandem repeats) Low complexity sequences Satellite DNA RNA genes (tRNA, rRNA, snRNA, etc.)\nTier 2: Context-Dependent (Moderate Confidence); Rationale: These can contain regulatory elements but are also sources of noise.\nLINEs (Long Interspersed Nuclear Elements)\nSINEs (including Alu elements)\nLTR retrotransposons\nTier 3: Usually Keep (Low Confidence for Removal); Rationale: Often contain regulatory sequences and show tissue-specific patterns.\nDNA transposons\nRolling circle elements\nUnknown/unclassified repeats\nSpecific Technical Questions\nOverlap threshold: What percentage overlap should trigger filtering? 50%? 80%? Any overlap?\nPeak strength: Should we consider the accessibility signal strength when deciding whether to filter repeat-overlapping peaks?\nRepetitive categories: Should we need to consider the category of repetitive families when we do filtering?\nWhat I'm Looking For\nExperiences from the community with different filtering strategies\nReferences to papers that have systematically evaluated this question\nPractical advice on balancing noise reduction with biological signal retention\nTool recommendations for sophisticated repeat filtering",
    "answers": [
      "I am going to guess you've seen/thoroughly read this paper about the ENCODE blacklist regions and how they were generated.\nIn short, they were generated using the input samples from pretty much every ChIP-seq input control in ENCODE:\nThis defines a comprehensive and cell-type agnostic signal across the genome that is unaffected by high signal from a particular cell-line (eg. CNVs) or low signal due to differential processing of input data. This defines a comprehensive and cell-type agnostic signal across the genome that is unaffected by high signal from a particular cell-line (eg. CNVs) or low signal due to differential processing of input data.\nChIP-seq, ATAC, CUTandRUN, etc, are all limited in some ways just by nature of the assay. You're correct that regulatory elements are often repetitive, but if you've got a read mapping to 20 different regions equally well, you've got no way to resolve that with short read sequencing (and long read just isn't helpful or sensible given the fragment sizes in these assays). That said, the whole element is often not repetitive and thus some reads end up mapping uniquely to it, which can still enable analysis.\nIn general, the regions with crazy high artificial signal are often skipped by peak callers anyway if input control are provided (though I realize such controls are not in play with the multiome kit). As for your questions, well, it depends. I haven't looked at the impacts on cell type identification and clustering, but I imagine doing no filtering would result in poorer separation of populations due to increased noise. Perhaps not, if you're only using the top X most variable peak regions to perform dimensionality reduction and cluster anyhow. But I don't see leaving them in as having any benefit. Not removing these regions can have large impacts on normalization, as differences in the proportions of reads mapping to these areas between samples will skew counts for \"real\" signal.\nDifferential analyses of such regions will largely be pointless - they'll be high in all samples, but potentially quite variable (as data/assay quality tends to trend with the proportion of reads mapping to such regions). And given those regions will also be high in input controls, it's an uphill battle to convince anyone that such differences are anything beyond noise (again, I recognize there are no input controls here).\nFor bulk ATAC, if you don't want to blanket remove regions based on a blacklist, you can make your own greylist from your input samples, but that's not a possibility here.\nTo get solid answers to your questions, I think you'd have to investigate your data carefully (or set up experiments to actually investigate these questions more directly, whatever those may look like).",
      "It should be noted that concerns about mapping artifacts in ChIP-seq really come from an era where reads were 36bp (or even 25bp!) in length, and single-ended. The 10X data will be 2x50 at a minimum, often times 2x76 or even 2x150; in these cases mapping issues are much less of a concern. In my own analysis, if I stratify basic peak quality control plots by \"peak overlaps ENCODE blacklist region\" I see very minor differences. Personally, I stick to IDR (https://github.com/nboley/idr) for filtering called peaks."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612733/",
    "title": "Using VEP custom input",
    "question": "I have a DataFrame with the following columns. Is there any way to use this file in VEP to include columns 6 to 12 in my variant annotation outputs?\n    chr   start     end       transcript    gene   exp_snv  obs_snv      pLI      o/e  lof.oe_ci_lower  lof.oe_ci.upper         biotype  canonical  mane_select\n0  chr1   65419   71585  ENST00000641515   OR4F5   0.53873      0.0  0.33668  0.00000        0.000            1.828  protein_coding       True         True\n1  chr1  923923  944574  ENST00000616016  SAMD11  54.47500     75.0  0.00000  1.37680        1.204            1.764  protein_coding       True         True\n2  chr1  923923  944574  ENST00000618323  SAMD11  54.13100     76.0  0.00000  1.40400        1.231            1.795  protein_coding      False        False\n3  chr1  925150  935793  ENST00000437963  SAMD11  15.47900     15.0  0.00001  0.96906        0.483            1.272  protein_coding      False        False\n4  chr1  925731  944574  ENST00000342066  SAMD11  55.24600     75.0  0.00000  1.35760        1.141            1.693  protein_coding      False        False",
    "answers": [
      "VEP can integrate custom annotation from standard format files into your results by using the --custom flag.\nsee https://www.ensembl.org/info/docs/tools/vep/script/vep_custom.html\n( but I would just index the file with tabix and use bcftools annotate )"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612746/",
    "title": "Bacterial transcriptome analysis- any need for --dta in HISAT2?",
    "question": "When working with a bacterial sample, is it still necessary to pass --dta in HISAT2? The StringTie manual mentions to use it in general but since it pertains to splice sites I wasn't sure if it's relevant here. Thanks in advance.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612736/",
    "title": "nanopore unaligned bam files",
    "question": "Hi,\nI have received nanopore data with primary analysis from a sequencing center. It includes pod5 file, fastq files and bam files (both pass and fail folders for each). I checked the bam files and looks like all reads are currently unmapped, despite the methylation information (MM/ML tags) are avaialable. So, looks like the basecalling was performed using a modified-base-aware model but the alignment itself has been skipped or wrong reference genome or something like this. So, I have 2 questions: 1) is this normal to receive the bam files in this shape and 2) is there a way to do the alignment using the fastq files or extracted sequences from bam files (using example minimap2) and merge that with the methyl info available at bam files? So, I hoping not to require to go back to pod5 files and both base-calling and alignment (example with dorado) as the pod5 files are extremely large. Thanks in advance!",
    "answers": [
      "is this normal to receive the bam files in this shape and\nYes. If you asked for methylation calls then this is a way to capture that information. Programs like modkit use this information: https://github.com/nanoporetech/modkit\nis there a way to do the alignment using the fastq files or extracted sequences from bam files (using example minimap2) and merge that with the methyl info available at bam files?\nYes you can directly use the fastq files for alignments with minimap2, if you have no interest in methylation."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612742/",
    "title": "I woudl ike to understnad how to set the aucMaxRank value for my data set",
    "question": "Thanks for a great package. In the tutorial you explain, the AUCell_run command takes by default only 5% of the genes to calculate the scores per gene set for faster execution, but recommend to check the distribution of the genes before using it on \"real\" data.\nIn the tutorial this is done with the histogram and results of the AUCell_buildRankings function.\nThis is how the quantiles table looks like for the tutorial data:\n## Quantiles for the number of genes detected by cell: \n## (Non-detected genes are shuffled at the end of the ranking. Keep it in mind when choosing the threshold for calculating the AUC).\n##     min      1%      5%     10%     50%    100% \n##  193.00  271.08  364.20  447.40  921.00 2056.00\nDo I understand it correctly, that according to this table, 5% of the cells can detect 364.2 genes per cell out of total of 2056 genes (which would be 100%)?\nHow do I decide on this value?\nthanks for the help",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612741/",
    "title": "Issue with Package installation in R.",
    "question": "using C compiler: ‘gcc (Ubuntu 12.3.0-17ubuntu1) 12.3.0’\nusing C++ compiler: ‘g++ (Ubuntu 12.3.0-17ubuntu1) 12.3.0’\ng++ -std=gnu++17 -I\"/usr/share/R/include\" -DNDEBUG  -I'/home/omicslogic/R/x86_64-pc-linux-gnu-library/4.5/Rcpp/include'     -fpic  -g -O2 -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -ffile-prefix-map=/build/r-base-qkOIYD/r-base-4.5.0=. -fstack-protector-strong -fstack-clash-protection -Wformat -Werror=format-security -fcf-protection -fdebug-prefix-map=/build/r-base-qkOIYD/r-base-4.5.0=/usr/src/r-base-4.5.0-3.2404.0 -Wdate-time -D_FORTIFY_SOURCE=3   -c RcppExports.cpp -o RcppExports.o\ngcc -std=gnu2x -I\"/usr/share/R/include\" -DNDEBUG  -I'/home/omicslogic/R/x86_64-pc-linux-gnu-library/4.5/Rcpp/include'     -fpic  -g -O2 -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -ffile-prefix-map=/build/r-base-qkOIYD/r-base-4.5.0=. -fstack-protector-strong -fstack-clash-protection -Wformat -Werror=format-security -fcf-protection -fdebug-prefix-map=/build/r-base-qkOIYD/r-base-4.5.0=/usr/src/r-base-4.5.0-3.2404.0 -Wdate-time -D_FORTIFY_SOURCE=3  -c loop_apply.c -o loop_apply.o\nIn file included from /usr/share/R/include/R.h:70,\n                 from loop_apply.c:1:\n/usr/share/R/include/R_ext/Boolean.h:65:16: error: expected ‘{’ before ‘:’ token\n   65 |   typedef enum :int { FALSE = 0, TRUE } Rboolean;  // so NOT NA\n      |                ^\n/usr/share/R/include/R_ext/Boolean.h:65:41: warning: data definition has no type or storage class\n   65 |   typedef enum :int { FALSE = 0, TRUE } Rboolean;  // so NOT NA\n      |                                         ^~~~~~~~                          ^~~~~~~~\n\nmake: *** [/usr/lib/R/etc/Makeconf:202: loop_apply.o] Error 1\nERROR: compilation failed for package ‘plyr’\n* removing ‘/home/omicslogic/R/x86_64-pc-linux-gnu-library/4.5/plyr’\nWarning in install.packages :\n  installation of package ‘plyr’ had non-zero exit status",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612739/",
    "title": "MAPQ algorithms of Giraffe",
    "question": "Hello! I am comparing my giraffe mapping result with bwa-mem result (aligned to linear reference). The MAPQ algorithms in two different mapping tools is different or not? Can I use same threshold(such as MAPQ>30) to filter the reads? Or they have different meanings in different mapping tools?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612535/",
    "title": "Phased haplotype using WGS data from Nebula Genomics and GATK",
    "question": "I am a technically-competent non-bioinformatician with a set of three WGS sequences for a grandmother, son, and daughter that were provided by Nebula Genomics (for each, I have two XXX.fq.gz files, along with XXX.cram, XXX.vcf, and XXX.tbi). While I have been able to inspect the genomes individually using tools like gene.iobio.io and IGV, and I have been able to get the GATK Docker container up and running, I have run into some errors while trying use the three genomes to generate phased haplotypes. In particular, when I run the following:\n./gatk HaplotypeCaller -R /mnt/Backup/Medical/Genetic/hg38/resources-broad-hg38-v0-Homo_sapiens_assembly38.fasta -I /mnt/Backup/Medical/Genetic/person1.cram -I /mnt/Backup/Medical/Genetic/person2.cram -I /mnt/Backup/Medical/Genetic/person3.cram -O /mnt/Backup/Medical/Genetic/gatk_output.vcf.gz\nI get the following output:\nUsing GATK jar /gatk/gatk-package-4.6.2.0-local.jar\nRunning:\n    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.6.2.0-local.jar HaplotypeCaller -R /mnt/Backup/Medical/Genetic/hg38/resources-broad-hg38-v0-Homo_sapiens_assembly38.fasta -I /mnt/Backup/Medical/Genetic/.person1.cram -I /mnt/Backup/Medical/Genetic/person2.cram -I /mnt/Backup/Medical/Genetic/person3.cram -O /mnt/Backup/Medical/Genetic/gatk_output.vcf.gz\n18:45:26.985 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.6.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n18:45:27.109 INFO  HaplotypeCaller - ------------------------------------------------------------\n18:45:27.112 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.6.2.0\n18:45:27.112 INFO  HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/\n18:45:27.112 INFO  HaplotypeCaller - Executing as root@739efec904b5 on Linux v6.10.14-linuxkit amd64\n18:45:27.112 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.12+7-Ubuntu-1ubuntu222.04\n18:45:27.112 INFO  HaplotypeCaller - Start Date/Time: May 28, 2025 at 6:45:26 PM GMT\n18:45:27.113 INFO  HaplotypeCaller - ------------------------------------------------------------\n18:45:27.113 INFO  HaplotypeCaller - ------------------------------------------------------------\n18:45:27.114 INFO  HaplotypeCaller - HTSJDK Version: 4.2.0\n18:45:27.114 INFO  HaplotypeCaller - Picard Version: 3.4.0\n18:45:27.114 INFO  HaplotypeCaller - Built for Spark Version: 3.5.0\n18:45:27.117 INFO  HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n18:45:27.117 INFO  HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n18:45:27.118 INFO  HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n18:45:27.118 INFO  HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n18:45:27.118 INFO  HaplotypeCaller - Deflater: IntelDeflater\n18:45:27.118 INFO  HaplotypeCaller - Inflater: IntelInflater\n18:45:27.119 INFO  HaplotypeCaller - GCS max retries/reopens: 20\n18:45:27.119 INFO  HaplotypeCaller - Requester pays: disabled\n18:45:27.120 INFO  HaplotypeCaller - Initializing engine\nWARNING 2025-05-28 18:45:27 CRAMFileReader  CRAM index file /mnt/Backup/Medical/Genetic/person1.cram.crai is older than CRAM /mnt/Backup/Medical/Genetic/person1.cram\nWARNING 2025-05-28 18:45:28 CRAMFileReader  CRAM index file /mnt/Backup/Medical/Genetic/person2.cram.crai is older than CRAM /mnt/Backup/Medical/Genetic/person2.cram\nWARNING 2025-05-28 18:45:28 CRAMFileReader  CRAM index file /mnt/Backup/Medical/Genetic/person3.cram.crai is older than CRAM /mnt/Backup/Medical/Genetic/person3.cram\n18:45:31.908 INFO  HaplotypeCaller - Done initializing engine\n18:45:32.023 INFO  NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.6.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so\n18:45:32.029 INFO  NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.6.2.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so\n18:45:32.030 INFO  SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation\n18:45:32.034 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output\n18:45:32.044 INFO  NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.6.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so\n18:45:32.059 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM\n18:45:32.060 INFO  IntelPairHmm - Available threads: 4\n18:45:32.061 INFO  IntelPairHmm - Requested threads: 4\n18:45:32.062 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation\n18:45:32.230 INFO  ProgressMeter - Starting traversal\n18:45:32.231 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute\n18:45:39.171 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0\n18:45:39.171 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0\n18:45:39.172 INFO  SmithWatermanAligner - Total compute time in native Smith-Waterman : 0.00 sec\n18:45:39.174 INFO  HaplotypeCaller - Shutting down engine\n[May 28, 2025 at 6:45:39 PM GMT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.20 minutes.\nRuntime.totalMemory()=700448768\nhtsjdk.samtools.SAMException: No sequence dictionary mapping available for header: SAMFileHeader{VN=1.4, SO=coordinate}\n    at htsjdk.samtools.SamFileHeaderMerger.getMergedSequenceIndex(SamFileHeaderMerger.java:741)\n    at htsjdk.samtools.MergingSamRecordIterator$MergedSequenceDictionaryCoordinateOrderComparator.getReferenceIndex(MergingSamRecordIterator.java:234)\n    at htsjdk.samtools.MergingSamRecordIterator$MergedSequenceDictionaryCoordinateOrderComparator.fileOrderCompare(MergingSamRecordIterator.java:214)\n    at htsjdk.samtools.SAMRecordCoordinateComparator.compare(SAMRecordCoordinateComparator.java:48)\n    at htsjdk.samtools.SAMRecordCoordinateComparator.compare(SAMRecordCoordinateComparator.java:43)\n    at htsjdk.samtools.ComparableSamRecordIterator.compareTo(ComparableSamRecordIterator.java:75)\n    at htsjdk.samtools.ComparableSamRecordIterator.compareTo(ComparableSamRecordIterator.java:36)\n    at java.base/java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:647)\n    at java.base/java.util.PriorityQueue.siftUp(PriorityQueue.java:639)\n    at java.base/java.util.PriorityQueue.offer(PriorityQueue.java:330)\n    at htsjdk.samtools.MergingSamRecordIterator.addIfNotEmpty(MergingSamRecordIterator.java:161)\n    at htsjdk.samtools.MergingSamRecordIterator.<init>(MergingSamRecordIterator.java:94)\n    at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:429)\n    at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336)\n    at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134)\n    at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:88)\n    at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188)\n    at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173)\n    at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1119)\n    at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:150)\n    at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:203)\n    at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:222)\n    at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166)\n    at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209)\n    at org.broadinstitute.hellbender.Main.main(Main.java:306)\nSo it seems that (maybe) I need to regenerate new .crai files? Using the suggestion in this post : How to produce a single joint-called VCF file using as input three WGS samples (VCF,CRAM,FASTQ) that the reference files used to generate the .cram files are not the same, I found that the file used for person1 was different from the other two:\nmore person1.vcf | grep \"##ref\"   \n##reference=file:///mnt/ssd/MegaBOLT_scheduler/reference/hg38.fa\n\nmore person2.vcf | grep \"##ref\"\n##reference=file:///mnt/ssd/MegaBOLT_scheduler/reference/G42_refdata/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna\n\nmore person3.vcf | grep \"##ref\"\n##reference=file:///mnt/ssd/MegaBOLT_scheduler/reference/G42_refdata/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna\nI'm looking for suggestions on what the most reasonable straight line path would be. Should I try to generate new .cram/.crai/.vcf files from the source FASTA data or is there an easier way to rebase these downstream analyses on a new reference genome? If so, what is the most widely adopted/robust human reference genome? Any pointers that might help me accomplish this?\nThanks!\nMatthias",
    "answers": [
      "I don't have too much experience running tools like this but my 2c\nregenerating the CRAM index (.crai) files is easy and relatively fast (minutes), you can run \"samtools index -@8 file.cram\" (to use 8 threads) for each of your cram files. unless that error was related to something really weird, I wouldn't suspect this to be the issue necessarily though.\nit is not generally good to mix reference genomes, but both \"hg38.fa\" and \"GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna\" are both \"versions\" of GRCh38 but subtle differences can really confuse things, particularly with cram (because CRAM files are reference compressed, so you need the exact reference genome to properly decompress/interpret them). you may want to ensure all your data is on the same reference, by potentially performing realignment of e.g. the one that is aligned to \"hg38.fa\" to \"GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna\" (aligning can take hours to maybe a day). this may be a good thing to do because the error from GATK is during \"SAM header merging\", while I don't know the exact thing going on in their code, it triggers my spidey sense because info about the reference sequences is in the SAM header (BAM and CRAM files, while being compressed versions of SAM, have \"SAM headers\") so if the info in these headers are different, potentially it could create that error\nGRCh38 is 'good enough' for most purposes. The latest genome is the \"T2T\" genome aka hs1 aka CHM13-T2T-v2.0, but it is not yet commonly adopted for most analysis pipelines, and the lessons that were learned while sequencing it have already helped the hg38 annotations. It's possible that they are similar enough that it is not a worry. if it does worry you, then you can redo alignment from the fastq to produce your own cram files.\nas long as you are doing stuff with trio's I might as well plug this little tutorial I made. I don't walk through the creation of phased variant calls, but once you have them, you can visualize them in jbrowse https://jbrowse.org/jb2/docs/user_guides/analyze_trio/\nyou could also try bcftools mpileup+whatshap as an alternative pipeline, though again, i don't have much practical experience running these tools so I could be off base",
      "I would ask Nebula to reanalyze person1.vcf for you, since you show it was aligned against a different reference so is not compatible.\nIf you have a beefy local workstation/cluster you might be able to do the reanalysis yourself but gatk is known to be slow and resource hungry.\nI'm not sure if your error relates to a lacking sequence dict. eg Building Dict File for GATK\nAlso - check to see if the SAMPLE eg person1 2 3 is present in the last header line of the VCF file (header starting with #, not ##. This is important for some later eg trio operations.",
      "I believe I found the correct reference genome for the odd man out. It is hg38 which seems like one of the original older reference genomes. when I have some time, I am going to try redoing the alignment using GATK. I do have a reasonably fast workstation and a functioning Docker image. Any suggestions on effectively accomplishing? This would be most welcome thanks."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612731/",
    "title": "IGV view for Chromatin accessibility ATAC-seq",
    "question": "Hi All,\nThis is my first time to analyze ATAC-seq data. So after done with the typical pipeline, the final step is view Chromatin accessibility, for this, I using IGV. This is what I see (see attach pic), could you give some advice? is this normal? or something wrong with my analysis or my ATAC-seq data? Because usually I see sort of wavelet peak, but for me, I only see one \"regtangle peak\". Thanks",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9495923/",
    "title": "How to interpret Nucleosome banding pattern in scATAC-Seq?",
    "question": "Hello,\nI am following Signac vignette to perform QC on my single cell ATAC-Seq data. As per my understanding the vignette considers all cells with nucleosome signal > 4 to have a high nucleosomal signal strength and nucleosome signal is calculated by taking a ratio of mononucleosomal to nucleosome-free fragments.\nFor my data, I see a median nucleosome signal strength for my samples ranging between 0.3 to 0.5. Does that mean my samples have a higher number of nucleosome-free fragments (fragments enriched at TSS) as compared to mononucleosomal region?\nI am not sure if I understand how this point towards the quality of the data.\nAny help is greatly appreciated!\nKhushbu",
    "answers": [
      "Hey,\nseems like his account is not existing anymore. If anyone else is still interested, I developed a tool evaluating the nucleosomal signal by analysing the strength of the periodical pattern.\nFeel free to check it out: Github: https://github.com/loosolab/PEAKQC and Preprint: https://www.biorxiv.org/content/10.1101/2025.02.20.639146v1.full.pdf\nBests, Jan"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612722/",
    "title": "WGCNA blockwiseModules function for small datasets",
    "question": "Hi,\nI'm doing WGCNA on a relatively small dataset (~1500 genes and 50 samples). All of the tutorials that I can find are using the blockwiseModules function in the WGCNA package for R, which seems great for large datasets. Since I'm working with a smaller dataset, though, I'm not super concerned about RAM.\nIs there anything lost by using the blockwiseModules function on a small dataset? Would the alternative be to use several separate functions instead (to calculate adjacency, TOM, merge, etc), and if so is there a benefit to doing the analysis that way?\nThanks for your help! This community has been a lifesaver!",
    "answers": [
      "You lose some granular control over parameters. If you set the block to be larger than the number of genes, you just get a \"default\" run of WGCNA, modified by the parameters you put as input. Also the saved TOM is nested in a list, so it can be somewhat unwieldy at first. But it's a great \"fast path\" to a WGCNA network if you set the block size to inf."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612723/",
    "title": "How to add GFP in STAR reference?",
    "question": "Hello,\nI wanted to ask how to add GFP to a STAR reference. I have added the GFP fasta sequence to the human fasta file. Then, I want to add the GFP to the gtf.\nI was going to add the below to the end of the .gtf file. Do I need to know anything about the strand? Is there a chance I would need to use the \"-\" for reverse strand?\nGFP     unknown exon    1       580     .       +       .       gene_id \"GFP\"; transcript_id \"GFP\"; gene_name \"GFP\"; gene_biotype \"protein_coding\";\nThank you",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612718/",
    "title": "Population Genomics Using Ancient DNA Data course",
    "question": "Dear all,\nWe are pleased to announce the upcoming Physalia online course: Population Genomics Using Ancient DNA Data\nDates: 22nd–26th September -Daily sessions: 2–8 PM (Berlin time)\nCourse website: https://www.physalia-courses.org/courses-workshops/adna-popgen/\nThis course introduces participants to the unique challenges and opportunities of working with ancient DNA (aDNA) in population genomics. You’ll learn how aDNA data is generated, processed, authenticated, and analyzed to answer key evolutionary and demographic questions.\nThrough a mix of lectures and hands-on exercises, attendees wiill gain practical experience with:\nAuthenticating and processing ancient genomic data\nExploring population structure and relatedness\nApplying D and F statistics to assess population affinity and gene flow\nPerforming demographic inference and simulations\nDetecting signatures of natural selection\nFor the full list of our courses and workshops, please visit: https://www.physalia-courses.org/courses-workshops/",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612716/",
    "title": "Can you compare GETx and TCGA miRNA non normalized counts a",
    "question": "Hi,\nIs it possible to compare TCGA miRNA non-normalized counts read with GETx miRNA counts read? How do you batch correct in this case, and is that enough to remove technical bias?",
    "answers": [
      "This was asked dozens of times before, please searh for previous threads. In brief, batch is nested with condition, so actually it's not possible to correct for anything. Then however this paper (https://www.nature.com/articles/sdata201861) is always cited when people need a justification to still attempt some correction. Read it, see whether you find it convincing, and if so, consider doing the same."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612697/",
    "title": "ATAC-seq analysis",
    "question": "After aligning ATAC-seq paired-end reads, the bam files are to be sorted. Should it be name sorted or position sorted?",
    "answers": [
      "Please read the manual. https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard\nCoordinate-sorted is preferred, here, and in most situations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9609844/",
    "title": "Impact of the number of PCs on the clustering in scRNA seq",
    "question": "Hello all,\nI am working with a scRNA-seq dataset. I apply a PCA and select two different number of PCs (10 and 20). Then I apply a Louvain clustering on the reduced space with a fixed resolution (0.3) and I compare the two clusterings. I get more clusters when I selected 10 PCs than when I selected 20 PCs.\nI wonder why is that and I would therefore appreciate any hint!\nThank you very much :)\n(I join the scree plot showing the percentage of variance explained by each PC)\nAlong with the two UMAP showing the clustering.",
    "answers": [
      "As much as it might feel like an unsatisfactory answer, this totally depends on your downstream needs and what meaningful biology you can attach to the clustering. Some of these clusters will be based on technical variation, some will be QC-associated, some will associate with cell cycle while others will be driven by some effect based on a set of differential genes. Find out what markers are driving the clusters and you'll get a much better understanding of the clustering 'success' yourself.\nWhen gauging the success of graphical clustering the first question to ask is how meaningful are the clusters and you do that by looking at the marker genes for that cluster and see if you can make sense of it.\nFor your larger UMAP (bottom right), for example, I'd be quite interested in finding out the set of genes that would resolve cluster 2 (green) and cluster 8 (blue). I would equally be interested in understanding the stripe running down the middle of cluster 1 (orange), cluster 0 (dark blue) and cluster 5 (brown).\nSelecting the optimal number of PCs is not a simple question and typically the most practical solution is to iterate a few times and see if you get better resolution with a larger input set.\nSince you're already on python, you can look into tools like cNMF which allow you to optimise feature selection more readily than standard graphical clustering approaches. However, I would storngly recommend exploring your data a bit more before moving on with extra technical tools. Once you've convinced yourself you understand what is going on, you'll be in a much better position to judge these kinds of things yourself."
    ]
  },
  {
    "url": "https://www.biostars.org/p/482158/",
    "title": "ATAC-seq +4 -5 shift",
    "question": "Dear all,\nI saw having the mapped reads have +4 and -5 shift in ATAC-seq is a common practice.\nSome place says \"reads should be shifted + 4 bp and − 5 bp for positive and negative strand respectively, to account for the 9-bp duplication created by DNA repair of the nick by Tn5 transposase and achieve base-pair resolution of TF footprint and motif-related analyses\"\nSome place says:\" When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site.\"\nI'm a little bit confused. Are shifting mainly to center the peak or avoid the duplication?\nDoes anyone have a good illustration on this? What will happen to the peak calls if this step is skipped?\nThank you!",
    "answers": [
      "I illustrated the molecular biology here: http://guertinlab.org/wp-content/uploads/2021/01/Tn5_illumina_adapters_mjg_2.pdf\nwe skip this step for peak calling--it really only matters for looking at the data at single nucleotide resolution composite profiles.",
      "The shifting isn't for any real purpose unless you want to plot the exact cut location (e.g., when searching for motifs), it simply harkens back to one of the first ATAC-seq papers where they performed this adjustment to account for the 9-base single-stranded over-hang on each end of the fragment. Papers since have simply followed suite. A vastly more sensible strategy would be to use the 9 bases on each end of the fragment, since these are bases that are necessarily open.",
      "Turns out +4/-4 is the correct shift. You can see that when you make a bigwig separately from the + and - strands, then the reads align with each other for +4/-4 and it's off by one for +4/-5. The main idea is that transposition events should be mapped to the same base pair regardless of strand."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612994/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612991/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612988/",
    "title": "ATAC seq: from peaks to differential analysis ??",
    "question": "Hi,\nI’m fairly new to ATAC-seq and have successfully run MACS2 separately for each of my samples. I now have individual *.narrowPeak files as output.\nMy experimental design looks like this:\nSample_ID     Cell_type     Condition     Donor\n\nSample_1      T_cells       Tumor         Donor_1\nSample_2      T_cells       Normal        Donor_1\nSample_3      T_cells       Tumor         Donor_2\nSample_4      T_cells       Normal        Donor_2\n...\nSample_11     Dendritics    Tumor         Donor_10\nSample_12     Dendritics    Normal        Donor_10\nSample_13     Dendritics    Tumor         Donor_11\nSample_14     Dendritics    Normal        Donor_11\nAs you can see, I have two cell types (T_cells and Dendritics), and for each donor, I have paired Tumor and Normal samples.\nMy goal is to perform a differential accessibility analysis (Tumor vs Normal), accounting for both Donor and Cell_type. I’m also interested in comparing Tumor (T_cells) vs Tumor (Dendritics).\nI heard that it is possible to use DESeq2 for ATAC-seq data, so my design will look like this: https://bioconductor.org/packages/3.21/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#group-specific-condition-effects-individuals-nested-within-groups\nI have a few questions:\n1) Can I use the exact same code from the vignette, as typically done for RNA-seq data? Or are there any parameters or steps specific to ATAC-seq that I should consider?\n2) I’m struggling with how to convert my individual *.narrowPeak files into a count matrix. Do you have any recommendations or tools to help with this step?\n3) Are there alternative methods to DESeq2 that would be better suited for this kind of analysis? I guess limma should work the same no ?\nThank you in advance for your help !",
    "answers": [
      "2:\n# make saf file:\nawk 'OFS=\"\\t\" {print $1\":\"$2+1\"-\"$3, $1, $2+1, $3, \"+\"}' ${sample}_peaks.narrowPeak > featureCounts_peaks.saf\n\n$featureCounts -a featureCounts_peaks.saf \\\n    -F SAF \\\n    --read2pos 5 \\\n    -p \\\n    -o ${peak_all_dir}${sample}_countMatrix.txt ${rmBL_dir}*.bam\n${rmBL_dir}*.bam: all bam files for generating narrowPeaks\n3: DEseq2 is oke. design matrix and param depends on research question, not on the ATAC-seq or RNA-seq as they are all readcount type",
      "I also recommend converting the peak to SAF file as QX shows, although I just count fragments centered on cut site. There's little difference either way though.\nFor more guidance on creating a count matrix, you do need to create a common set of peaks first. Two main ways to do this, either take the intersection of peaks present in all samples or the union.\nI use bedtools, e.g.\n# N just represents number of total samples.\nbedtools mutliinter -i ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | awk '$4 == N' | bedtools sort -i - | bedtools merge -i - > All_Samples_Intersection.bed\nOr,\ncat  ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | bedtools sort -i - | bedtools merge -i - > All_Samples_Union.bed\nThen you can convert that bed into SAF and use featurecounts to count reads from each sample and treat similar to RNA-seq.\nOne thing I would suggest, I usually use a low count filter before moving forward with DE analysis. With ATAC-seq, there's usually more noise (I use relatively lax peak calling), so I tend to use a higher count threshold."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612987/",
    "title": "CUT&RUN analysis pipeline",
    "question": "Hello, My lab has generated data using the CUT&RUN protocol. Unfortunately, Its single-end data. I tried the CUT&RUN tools and the henipipe tool. Both are for paired end data.\nAny suggestion how I can analyse these data ? Any idea how to set the parameters in case I use the classical Trimmomatic and bowtie2 ? Any other pipelines to try ?\nThanks and Regards. (Loosing my mind)",
    "answers": [
      "You don't need to do anything too special. Trim adapters with your favorite software, align to genome using BWA-MEM or bowtie2, and call peaks using MACS2.\nMost of the fancy stuff with CUT&RUN is when you have paired end data. With paired end data you get information on insert/fragment size, which can be used to infer nucleosomal and subnucleosomal fragment, which in turn lets you guess whether afragment was from nucleosome protection or TF binding."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612985/",
    "title": "Cut&Run replicates handling",
    "question": "Is there a CUT&RUN peakcaller that supports replicates? If not, what is your recommended approach for handling replicates?\nIt appears that all available peak callers, such as SEACR, are designed to call peaks from single pull-down experiments.\nOur Experiment: We have two conditions, Untreated (UTR) and Treatment (TREAT), each with two biological replicates. Each condition also has respective INPUT data for normalization.\nWe have completed:\nQuality Control (QC)\nAlignment\nDuplicate marking/removal\nSpike-in Calibration\nNext, we would like to proceed to peak calling and differential analysis. We would appreciate your recommendations on suitable peak callers and the best practices for handling replicates.",
    "answers": [
      "You can use nf-core cutandrun pipeline, it supports replicates in the step of consensus peaks calculations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612984/",
    "title": "Unable to extract count matrices from multi-layered Seurat object",
    "question": "Hello, I have a merged R object with 18 samples\nall_combined <- merge(\n  sample_1MI1_so,\n  y = c(\n    sample_1MI3_so,\n    sample_2MI1_so,\n    sample_2MI3_so,\n    sample_3C_MI2_so,\n    sample_3MI1_so,\n    sample_3MI3_so,\n    sample_4D_MI2_so,\n    sample_4MI1_so,\n    sample_4MI3_so,\n    sample_5MI1_so,\n    sample_5MI3_so,\n    sample_6MI1_so,\n    sample_6MI3_so,\n    sample_7MI1_so,\n    sample_7MI3_so,\n    sample_8MI1_so,\n    sample_8MI3_so\n  ),\n  add.cell.ids = c(\n    \"sample_1MI1_so\",\n    \"sample_1MI3_so\",\n    \"sample_2MI1_so\",\n    \"sample_2MI3_so\",\n    \"sample_3C_MI2_so\",\n    \"sample_3MI1_so\",\n    \"sample_3MI3_so\",\n    \"sample_4D_MI2_so\",\n    \"sample_4MI1_so\",\n    \"sample_4MI3_so\",\n    \"sample_5MI1_so\",\n    \"sample_5MI3_so\",\n    \"sample_6MI1_so\",\n    \"sample_6MI3_so\",\n    \"sample_7MI1_so\",\n    \"sample_7MI3_so\",\n    \"sample_8MI1_so\",\n    \"sample_8MI3_so\"\n  ),\n  merge.data = TRUE\n)\nView object\nall_combined\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 55 layers present: counts.1, counts.2, counts.3, counts.4, counts.5, counts.6, counts.7, counts.8, counts.9, counts.10, counts.11, counts.12, counts.13, counts.14, counts.15, counts.16, counts.17, counts.18, data.1, scale.data.1, data.2, scale.data.2, data.3, scale.data.3, data.4, scale.data.4, data.5, scale.data.5, data.6, scale.data.6, data.7, scale.data.7, data.8, scale.data.8, data.9, scale.data.9, data.10, scale.data.10, data.11, scale.data.11, data.12, scale.data.12, data.13, scale.data.13, data.14, scale.data.14, data.15, scale.data.15, data.16, scale.data.16, data.17, scale.data.17, data.18, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nIn the all_combined object, I', trying to extract the counts matrices from it, but I'm not sure how to do this? Below is what I've tried\nall_combined_count_matrix <- LayerData(object = all_combined, assay = \"RNA\", layer = \"counts\")\nWarning: multiple layers are identified by counts.1 counts.2 counts.3 counts.4 counts.5 counts.6 counts.7 counts.8 counts.9 counts.10 counts.11 counts.12 counts.13 counts.14 counts.15 counts.16 counts.17 counts.18\n only the first layer is used\n\n# only 1st sample counts matrix is extracted....want to extract all 18 counts matrices...all_combined object contains 18 seurat objects merged into one\n\n`\nManually combine all 18 count matrices into one\n# List all layers that start with \"counts\"\nall_counts_layers <- Layers(all_combined[[\"RNA\"]])\nall_counts_layers <- counts_layers[grepl(\"^counts\", all_counts_layers)]\n\n# # view all_count_layers\n#  [1] \"counts.1\"  \"counts.2\"  \"counts.3\"  \"counts.4\"  \"counts.5\"  \"counts.6\"  \"counts.7\"  \"counts.8\"  \"counts.9\"  \"counts.10\" \"counts.11\" \"counts.12\" \"counts.13\" \"counts.14\"\n# [15] \"counts.15\" \"counts.16\" \"counts.17\" \"counts.18\"\n\n\n\n# Extract each layer and combine\nall_count_matrices <- lapply(all_counts_layers, function(layer) {\n  LayerData(all_combined, assay = \"RNA\", layer = layer)\n})\n\n\n# View all_count_matrices (prints out all 18 count matrices. NOTE: each matrices has different number of rows/genes and different number of columns. However, total rows adds up to 23,447 genes and total columns add to 135,704 cells)\n# all_count_matrices\n\n\n\n# Combine into one gene x cell matrix\nall_combined_counts <- do.call(cbind, all_count_matrices)\n\n# Error in cbind.Matrix(x, y, deparse.level = 0L) : \n#   number of rows of matrices must match\nAny advice on how to do this effectively would be greatly appreciated. I'm using Seurat v5.3.0",
    "answers": [
      "Alright, I fixed the issue by joining the counts layers in all_combined. I saved it to a new seurat object. Hopefully this helps someone else!\nall_combined_join_layers <- JoinLayers(all_combined)\nThis joins the 18 counts layers into one layer (and also the 18 normalized data layers)\nall_combined_join_layers\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 21 layers present: data, counts, scale.data.1, scale.data.2, scale.data.3, scale.data.4, scale.data.5, scale.data.6, scale.data.7, scale.data.8, scale.data.9, scale.data.10, scale.data.11, scale.data.12, scale.data.13, scale.data.14, scale.data.15, scale.data.16, scale.data.17, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nI then extracted the counts matrix from all_combined_join_layers\nall_combined_count_matrix_2 <- LayerData(object = all_combined_join_layers, assay = \"RNA\", layer = \"counts\")\n\nall_combined_count_matrix_2\n\n23477 x 135704 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n\nENSMMUG00000023296 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692             . . . . . . . . . 1 . . . . . 1 2 . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612983/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612982/",
    "title": "usage of ChromHMM and Segway",
    "question": "I want to use ChromHMM and Segway to identify chromatin states or genomic segmentation based on existing models. Where are these models stored and what form these models are, and how can I use them for direct calculations? ChromHMM(https://ernstlab.github.io/ChromHMM/); Segway(https://segway.hoffmanlab.org/)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612980/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612979/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612978/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612977/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612976/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612975/",
    "title": "phage sequence no alignment to genome",
    "question": "I am currently working with a bacterial strain (let’s call it Strain A) for which I have a complete genome assembly (StrainA_genome.fasta). From this strain, I obtained two sets of phage-related sequences using different experimental approaches:\n1. An assembled phage genome from lab experiments: StrainA_phage_gapfilled.fasta\n2. A pair of short-read sequencing files from another phage preparation: StrainA_phage_R1.fasta and StrainA_phage_R2.fasta\nSeparately, I predicted prophage regions on StrainA_genome.fasta using Cenote-Taker 3 and PhageBoost, and collected all the predicted prophage sequences into a file: StrainA_predicted_phages.fasta.\nMy goal is to verify whether the experimentally derived phage sequences correspond to any of the predicted prophage regions, i.e., to confirm that the experimentally recovered sequences truly originate from prophages within Strain A.\nWhat I tried\nI attempted various alignments, including:\nI used bwa mem to align both the raw phage reads and the gap-filled phage assembly against the predicted prophage regions, using commands like:\n # Align short reads to predicted phages \n  bwa mem -t 4 StrainA _predicted_phages.fasta StrainA_phage_R1.fasta StrainA_phage_R2.fasta > phage_reads_vs_predicted.sam\n\n # Align assembled phage genome to predicted phages\n  bwa mem -t 4 StrainA_predicted_phages.fasta StrainA_phage_gapfilled.fasta > phage_assembly_vs_predicted.sam\nnone of the reads or assemblies mapped to any of the predicted prophage sequences.\nI then aligned the same experimental phage sequences (both reads and the assembled genome) directly to the full bacterial genome (StrainA_genome.fasta), expecting at least partial matches in the prophage regions. However, there were still no alignments.\n  # Also tried aligning both to the host genome:\n  bwa mem -t 4 StrainA_genome.fasta StrainA_phage_gapfilled.fasta > phage_vs_host.sam\nThis was very unexpected — if the prophages are indeed part of the genome, I would expect at least some reads to map to those regions.\nI also tried the same comparisons using Minimap2/bowtie2/blast, even tuned parameters to allow more mismatches and gaps. Unfortunately, there were no alignments..\nTo verify the identity of the experimentally obtained phage sequences, I submitted StrainA_phage_gapfilled.fasta to NCBI ORF Finder, extracted several of the longer predicted protein sequences, and then ran BLASTp against the virus (taxid:10239) database. These searches returned high-confidence hits to known phage proteins, supporting the idea that the experimental sequences are indeed phage-derived.\nAdditionally, I extracted CRISPR spacer sequences from Strain A using CRISPRCasTyper, and tried to align them (using BLAST) to the gap-filled experimental phage genome. Only one spacer hit was found, with a relatively low score (e-value ~1e-2).\nI am now wondering:\n1. Could this complete lack of alignment be caused by my alignment strategy?\nAre there better tools or methods specifically suited to phage–prophage or phage–host genome comparisons?\n2. Could experimental artifacts explain this?\nI do not perform the experimental work myself. Could the phage DNA isolation,amplification, or assembly methods introduce chimerism or unrelated sequences that mislead mapping?\n3. Or is it genuinely common to see such divergence between experimentally isolated phages and prophage predictions from the same strain genome?\nAny thoughts or suggestions on improving the alignment strategy or interpreting these results would be greatly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612711/",
    "title": "Learn tNGS",
    "question": "HI,I'm learning about tNGS(Targeted Next-Generation Sequencing) which is aimed to detect pathogenic microbes these days. I think that learning from a project is a good way to fully understand it.But I didn't find any complete project for me to study. Is there any experienced senior who can provide me with some reference process links or learning ideas? The focus can be on the analysis and processing of data in bioinformatics software.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612671/",
    "title": "Server Requirements for long read data analysis",
    "question": "Hello everyone,\nI would like to ask what are the minimum to very good server specifications for setting up a server suitable for long read data analysis. I have only analyzed short read sequencing data before, so requirements such as good GPU are not much of a problem. If your lab also uses long read data, I appreciate your inputs or suggestions. I appreciate all your help!",
    "answers": [
      "Like always, it depends on what you want to do and at what scale. The most resource-intensive step determines the minimum requirements of the system. Do you want to do the basecalling of ONT data? Then, the system requirements for dorado will shape GPU support and vRAM requirements of your system, including the necessity of CUDA (nvidia) support. (Indeed from the description it looks like I could do the basecalling on a MacBook Air) https://dorado-docs.readthedocs.io/en/latest/ Otherwise, it may be assembly with Flye or whatever you want to use. If you already have a system for short read analysis (with a minimum of 128-256GB RAM) and enough storage, it may be sufficient to add CUDA/GPU support for the bassecalling step to it. Also, if you have a machine already, it is worthwhile just trying out some standard NF-core pipelines (https://nf-co.re/genomeassembler/1.0.1/) with a public dataset of the size and organims you will be dealing with.",
      "Basecalling - https://github.com/Kirk3gaard/2025-Crowdsource-GPU-basecalling-stats Note that apple silicon does not seem to work well these days. Go for an nvidia card\nserver or workstation ? Depends on your budget, consumer cards like the 4090 etc are a lot cheaper than the data center A100 H100 etc. 24GB GPU Ram is enough for basecalling, but insufficient for dorado-correct (32GB+)\nCPU - go for AMD for the number of hyperthreaded cores, they work very well\nRAM - as much as possible, min 128 GB, more is better\nAnd yes, please say what applications you have in mind, and try some public datasets."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612704/",
    "title": "The molecular signatures database (MSigDB) in R",
    "question": "I'm trying to use the msigdb package, but I can't seem to access the EH9632 resource. When I call msigdb::getMsigdb(), it only fetches up to EH8300. From what I understand in the official vignette, the most recent version of MSigDB should be v 1.9.x, but my setup seems to be defaulting to older versions (e.g., 1.14.0 instead of 1.19.x). I've tried updating the package and Bioconductor version, but I haven't been able to force access to the EH9632 resource. Code I'm using:\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n      install.packages(\"BiocManager\")\n\nBiocManager::install(\"msigdb\", force = TRUE)\n\npackageVersion(\"msigdb\")\n\nlibrary(msigdb)   \nlibrary(ExperimentHub) \nlibrary(GSEABase)     \n\neh <- ExperimentHub()\n\nmsigdb_records <- query(eh, \"msigdb\")\n\nprint(msigdb_records)\nAny help on how to access the correct version or force download of EH9632 would be greatly appreciated!",
    "answers": [
      "Hi, with the current version of Bioconductor and msigdb I was able to access the EH9632 dataset you requested. I'm sharing with you the versions I have and the commands I used. Try with these versions as well. I’m also attaching the dataset in case you’re unable to access it if necessary.\nlibrary(msigdb)   \nlibrary(ExperimentHub) \nlibrary(GSEABase)\n\nversion\nplatform       x86_64-w64-mingw32               \narch           x86_64                           \nos             mingw32                          \ncrt            ucrt                             \nsystem         x86_64, mingw32                  \nstatus                                          \nmajor          4                                \nminor          5.0                              \nyear           2025                             \nmonth          04                               \nday            11                               \nsvn rev        88135                            \nlanguage       R                                \nversion.string R version 4.5.0 (2025-04-11 ucrt)\nnickname       How About a Twenty-Six\n\npackage.version(\"BiocManager\")\n[1] \"1.30.25\"\n\npackage.version(\"msigdb\")\n[1] \"1.16.0\"\n\neh <- ExperimentHub()\nmsigdb_records <- query(eh, \"msigdb\")\nprint(msigdb_records)\n\nExperimentHub with 51 records\n# snapshotDate(): 2025-04-12\n# $dataprovider: Broad Institute, Emory University, EBI, NA\n# $species: Homo sapiens, Mus musculus\n# $rdataclass: GSEABase::GeneSetCollection, data.frame, list\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based, maintainer, rdatadateadded, preparerclass,\n#   tags, rdatapath, sourceurl, sourcetype \n# retrieve records with, e.g., 'object[[\"EH5421\"]]' \n\n           title                   \n  EH5421 | msigdb.v7.2.hs.SYM      \n  EH5422 | msigdb.v7.2.hs.EZID     \n  EH5423 | msigdb.v7.2.mm.SYM      \n  EH5424 | msigdb.v7.2.mm.EZID     \n  EH6727 | MSigDB C8 MANNO MIDBRAIN\n  ...      ...                     \n  EH8298 | msigdb.v7.5.1.mm.idf    \n  EH8299 | msigdb.v7.5.1.mm.SYM    \n  EH8300 | imex_hsmm_0722          \n  EH9632 | MSigDB v2024.1.Hs       \n  EH9633 | MSigDB v2024.1.Mm\n\nEH9632 <- eh[[\"EH9632\"]]\nmsigdbeh not installed.\n  Full functionality, documentation, and loading of data might not be possible without installing\nInstall msigdbeh [yes/no] no\nloading from cache\nI hope this helps,\nBest regards\nMarco"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9608925/",
    "title": "plotProfile Errors of DiffBind",
    "question": "I am using the published data (doi.org/10.1111/nph.19791) , working on comparing the differential binding sites of H3K27me3 between the endo WT and crwn samples, three biological replicates for each condition. H3 signal as control in my analysis.\nThus far, the computational steps have proceeded smoothly (excluding the use of blacklist and greylist). However, I have encountered a problem at the final stage of generating the profile plot. Despite attempting to disable parallel processing and thoroughly checking the consistency of my data, I have been unable to resolve the issue.\nprofiles <- dba.plotProfile(dbObj5)\nGenerating report-based DBA object...\nGenerating profiles...\n**Error in value[[3L]](cond): \n  profileplyr error: Error: BiocParallel errors\n  1 remote errors, element index: 1\n  5 unevaluated and other errors\n  first remote error:\nError: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments**\nHere is the dataframe\ndata completement check\nI> nrow(dbObj5$binding)\n[1] 4568\n> complete_cases <- complete.cases(dbObj5$binding)\nsum(complete_cases)\nsum(!complete_cases)\n[1] 4568\n[1] 0\n\n\nsessionInfo()\nR version 4.4.1 Patched (2024-09-28 r87201 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22631)\nMatrix products: default\nlocale:\n[1] LC_COLLATE=Chinese (Simplified)_China.utf8 LC_CTYPE=Chinese\n(Simplified)_China.utf8\n[3] LC_MONETARY=Chinese (Simplified)_China.utf8 LC_NUMERIC=C\n[5] LC_TIME=Chinese (Simplified)_China.utf8\ntime zone: Etc/GMT-8\ntzcode source: internal\nattached base packages:\n[1] stats4 stats graphics grDevices utils datasets methods base\nother attached packages:\n[1] profileplyr_1.22.0 DiffBind_3.16.0\nSummarizedExperiment_1.36.0\n[4] Biobase_2.66.0 MatrixGenerics_1.18.1 matrixStats_1.5.0\n[7] GenomicRanges_1.58.0 GenomeInfoDb_1.42.3 IRanges_2.40.1\n[10] S4Vectors_0.44.0 BiocGenerics_0.52.0 BiocParallel_1.40.0",
    "answers": [
      "No, I gave up for the published data. It looks like there's missing data somewhere. While the program works for my own data."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612702/",
    "title": "Lab Head positions at the WEHI, Melbourne Australia",
    "question": "The WEHI is a leading Medical Research Institute in Melbourne, Australia, with a large and active research Division of Bioinformatics & Computational Biology. The WEHI has a continuing recruitment campaign for new Laboratory Heads across all areas including in bioinformatics, computational biology and AI:\nhttps://www.wehi.edu.au/careers/make-your-future-melbourne/",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612701/",
    "title": "Merging single ATAC seq data sets from multiple sample",
    "question": "How to combine multiple single cell data(ATAC) into one file. As an example the dataset I have is this to start with\nzcat GSE268807_G129_D_barcodes.tsv.gz | head\nAAACAGCCAAAGCTCC-1\nAAACAGCCAAGGTCGA-1\nAAACAGCCACAGGAAT-1\nAAACAGCCACATTAAC-1\nAAACAGCCACCTGGTG-1\nAAACAGCCACGCAACT-1\nAAACAGCCAGCTAACC-1\nAAACAGCCAGCTAATT-1\nAAACATGCAAATTCGT-1\nAAACATGCAACACCTA-1\n\n\n\n\nzcat GSE268807_G150_D_barcodes.tsv.gz | head\nAAACATGCAGACAAAC-1\nAAACATGCATAAAGCA-1\nAAACATGCATGAATCT-1\nAAACCAACACCTACGG-1\nAAACCGAAGCAGCTCA-1\nAAACCGAAGCTTGCTC-1\nAAACCGAAGGCGGATG-1\nAAACCGCGTTTGACCT-1\nAAACGCGCAAAGCCTC-1\nAAACGCGCAATGCCTA-1 \n\n\nzcat GSE268807_G129_D_matrix.mtx.gz| head   %%MatrixMarket matrix coordinate integer general\n%metadata_json: {\"software_version\": \"cellranger-arc-2.0.2\", \"format_version\": 2}\n131903 17176 60994557\n25 1 1\n33 1 1\n54 1 1\n60 1 1\n61 1 1\n63 1 1\n85 1 1   zcat GSE268807_G150_D_matrix.mtx.gz| head\n%%MatrixMarket matrix coordinate integer general\n%metadata_json: {\"software_version\": \"cellranger-arc-2.0.2\", \"format_version\": 2}\n114970 3068 23594006\n69 1 2\n137 1 1\n158 1 1\n248 1 1\n465 1 1\n469 1 1\n476 1 1 \n\n\nzcat GSE268807_G129_D_features.tsv.gz| head\nENSG00000243485 MIR1302-2HG     Gene Expression chr1    29553   30267\nENSG00000237613 FAM138A Gene Expression chr1    36080   36081\nENSG00000186092 OR4F5   Gene Expression chr1    65418   69055\nENSG00000238009 AL627309.1      Gene Expression chr1    120931  133723\nENSG00000239945 AL627309.3      Gene Expression chr1    91104   91105\nENSG00000239906 AL627309.2      Gene Expression chr1    140338  140339\nENSG00000241860 AL627309.5      Gene Expression chr1    149706  173862\nENSG00000241599 AL627309.4      Gene Expression chr1    160445  160446\nENSG00000286448 AP006222.2      Gene Expression chr1    266854  266855\nENSG00000236601 AL732372.1      Gene Expression chr1    360056  360057  \n\n\nzcat GSE268807_G150_D_features.tsv.gz| head\nENSG00000243485 MIR1302-2HG     Gene Expression chr1    29553   30267\nENSG00000237613 FAM138A Gene Expression chr1    36080   36081\nENSG00000186092 OR4F5   Gene Expression chr1    65418   69055\nENSG00000238009 AL627309.1      Gene Expression chr1    120931  133723\nENSG00000239945 AL627309.3      Gene Expression chr1    91104   91105\nENSG00000239906 AL627309.2      Gene Expression chr1    140338  140339\nENSG00000241860 AL627309.5      Gene Expression chr1    149706  173862\nENSG00000241599 AL627309.4      Gene Expression chr1    160445  160446\nENSG00000286448 AP006222.2      Gene Expression chr1    266854  266855\nENSG00000236601 AL732372.1      Gene Expression chr1    360056  360057\nHere I have feature file, barcode file and mtx file., this is the data source .\nMy final objective is the make one file for each such as one barcode, one feature and one mtx file.\nFor barcode and feature I can think of merging where I can filter the duplicates, but I'm not able to figure out how to merge the mtx file.\nAny suggestion or help would be really appreciated\n`",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612699/",
    "title": "GSEA analysis in R",
    "question": "I have used DESeq2 to perform differential gene expression study and saved the result. For using GSEA of clusterProfiler package, it needs a rank list, now the result of DESeq2 should be ranked on the basis of log2foldChange or test statistic? ANd if log2FC then should i shrunken the l2FC?",
    "answers": [
      "It doesn't matter really. Both shrunken logFCs and test statistics are fine and there is no real justification of one over the other. This was asked many times before."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612694/",
    "title": "Assembly issues",
    "question": "I’m trying to assemble reads using spades on galaxy.org. spades but I get a mistake. from the fastqc of the reads I noticed that they have residues of the universal illuminates adapters used for sequencing. I tried to clean the sequences by inserting on trimmomatic also the adapters provided by the company, but I still have spades error. Do any of you know how I could solve the problem and assemble the reads?\nThanks, Anna",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612651/",
    "title": "Use taxdb tp get scientific names in local BLAST output",
    "question": "Hello,\nI have searched this issue around several websites and despite some similar posts I cannot make this work...any help would be greatly appreciated.\nFor the identification of insect barcodes I have made a local database in which to identify each sequence.\nFirst, I have downloaded the gene sequences for Diptera insect order as .fasta\nThen I have made a db using makeblastdb -in diptera_coi.fasta -dbtype 'nucl' -out db/diptera_coi\nThe taxdb.bti/btd files were downloaded in the /bin where the barcode queries are stored.\nI have exported the path with export BLASTDB=/home/alex/ncbi-blast-2.16.0+/bin\nexecuted blastn -query trap_barcodes.fasta -task blastn -db db/diptera_coi -out trap_comparison_blastn.csv -outfmt \"6 qseqid qlen sseqid slen sscinames pident evalue bitscore qcovs qcovhsp qstart qend sstart send\" -evalue 1e-20 -max_target_seqs 10 -num_threads 20\nAll I miss from the output is the sscinames which keeps turning out as NA. I would be forever grateful for a clear explanation of what I am missing and doing wrong.",
    "answers": [
      "With blast+ v.2.16.\nGet the sequences for diptera from \"nt\" blast db.\n$ blastdbcmd -db nt -taxids 7147 -outfmt %f > test.fa\nWhile the default taxID database files should work let us not take any chances. We can extract the accession numbers for the diptera sequences along with their taxID's in a messy way (this is not the only way but we will go with this)\n$ blastdbcmd -db nt -taxids 7147 -outfmt %asep%T > intermed_file\nThen we can convert this file to create a taxid_map file that is tab delimited\n$ sed s/\"sep\"/\"\\t\"/g intermed_file > test_map.txt\nNow ready to create the local blast db for diptera with taxonomy info\n$ makeblastdb -in test.fa -dbtype nucl -out dipt_db -parse_seqids -taxid_map test_map.txt\nTry out a test search\n$ more query.fa \n>test\nTCTGGTGCCAGCAGCCGCGGTAATTCCAGCTCCACTAGCGTATATTAAAATTGTTGCGGTTAAAACGTTCGAAGTTTATT\nCTTGTCCAACACGGGTGCTACTCCTTTATGATGGCAGTAGGTCACTGGATTGTTGCGACTATAAGACTGGGTGCGCCCGT\nCGGCCTCGCGGTCGGCGCGGTCGTAGTGTGGCGCTGATGCCTTTCATCGGGTGCAGTGTTTCCGCAAGCCCAGCTGCTAT\nTACCTTGAACAAATTAGAGTGCTCTAAGCAGGCTATCCTACGGCCGAGAATAACTTGCATGGAATAATGGAATATGACCT\nCGGTCTTAATATTCATTGGTTTGTAATCAGATCAAGAGGTAATGATTAACAGAAGTAGTTGGGGGCATTAGTATTACGGC\nGCGAGAGGTGAAATTCGTAGACCGTCGTAAGACTAACTAAAGCGAAACGATTTGCCATGGATGCTTTCATTAATCAAGAA\nCGAAAGTTAGAGGATCGAGGCGATTAGATACCGCCCTAGTTCTAACCGTAAACTATGCCAATTAGCAATTGGGAGACGCT\nActual search\n$ blastn -query query.fa -task blastn -db dipt_db -out blastn2.csv -outfmt \"6 qseqid qlen sseqid slen scomname pident evalue bitscore qcovs qcovhsp qstart qend sstart send\"\nThe resulting file (only a portion shown)\n$ more blastn2.csv \ntest    560     emb|X57172.1|   1950    Asian tiger mosquito    100.000 0.0     1011    100     100     1       560     561     1120\ntest    560     gb|U65375.1|    1735    yellow fever mosquito   98.404  0.0     959     100     100     1       560     563     1125\ntest    560     gb|L78065.1|    8312    Anopheles albimanus     83.080  5.52e-164       576     100     100     1       560     2109    2679\ntest    560     gb|U07981.1|    2385    Eucorethra underwoodi   81.720  2.68e-155       547     99      99      1       554     526     1052\ntest    560     gb|AF033949.1|  612     Bactrocera xanthodes    85.893  1.14e-96        352     61      56      239     553     218     528\ntest    560     gb|AF033949.1|  612     Bactrocera xanthodes    96.552  6.35e-05        49.1    61      5       42      70      4       32\ntest    560     gb|AF033948.1|  563     Bactrocera umbrosa      85.893  1.14e-96        352     56      56      239     553     198     508\ntest    560     gb|AF033945.1|  614     Bactrocera xanthodes    85.893  1.14e-96        352     62      56      239     553     219     529\ntest    560     gb|AF033945.1|  614     Bactrocera xanthodes    96.667  1.82e-05        50.9    62      5       42      71      5       34\ntest    560     gb|AF033943.1|  622     Bactrocera xanthodes    85.893  1.14e-96        352     61      56      239     553     228     538\ntest    560     gb|AF033943.1|  622     Bactrocera xanthodes    91.667  0.40    35.6    61      4       48      71      20      43\ntest    560     gb|AF033941.1|  611     oriental fruit fly      85.893  1.14e-96        352     62      56      239     553     218     528"
    ]
  },
  {
    "url": "https://www.biostars.org/p/493686/",
    "title": "Search all SRA data for sequence? Raw reads only",
    "question": "Is their a way to search raw reads in the SRA? I don't mean an individual entry but rather all raw illumina reads submitted to NCBI. I could limit it by year, but I'm looking to scan all of the raw unassembled reads in public repositories. Yes, I know this is a significant task.\nI'm doing metagenomic / viral analysis and looking for evidence that might exist in the raw sequences but get thrown out in the consensus / assembly data.",
    "answers": [
      "Have you tried BLASTn from the NCBI web site? Simply select SRA as a target database from the drop-down menu.",
      "I don't mean an individual entry but rather all raw illumina reads submitted to NCBI.\nThere is no realistic way to do that. You can blast against select SRA accessions via BLAST web page as shown by @Mensur.\nCurrent size of SRA as of Feb 2021",
      "There's now Logan Search. (presently it covers through the end of 2023)\n\"Given a DNA sequence, the service replies in a few minutes in which SRA accession(s) it is likely to occur. ... In more technical depth, the search engine uses kmindex, a k-mer based sequence search tool that uses Bloom filters. It was applied to construct an index over all genome assemblies of all of SRA, more specifically over the unitigs of Logan.\"\nThere's a talk about it available here.\nThe associated GitHub repo: Logan.\nI put together a set of Jupyter Notebooks demonstrating using Python & Jupyter to assist in analyzing the results you get returned. You can run these notebooks in a MyBinder-provided remote session without installing a thing on your machine or signing up for anything by going here and clicking on a 'launch binder' badge. Save anything useful back to your local machine promptly as the remote session is temporary."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612681/",
    "title": "Is this Rosalind inspired project actually useful?",
    "question": "I created this project to help me with Rosalind because I found that I needed to create python friendly versions of things like codon tables and BLOSUM matrices repeatedly. I'm fairly new to the world of bioinformatics but I was wondering if this GitHub project has utility outside of me using it for Rosalind.\nAnd another question I had was, is there anything useful that could be added to this project to make it more user-friendly/useable for my future bioinformatics projects?",
    "answers": [
      "That functionality can be found in biopython, which is the most general bioinformatics toolkit in python.\nIf it was helpful for you, it wasn't useless.\nThat said, learning how to find tools that do what you're looking for is a skill in itself. In general, if you find yourself thinking \"this has got to be a common issue, there has to be some solution\", there probably is one (or twelve). Take a few minutes to hunt for them and see if they'll work. Save your brain power and time for the interpretation."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612687/",
    "title": "Subset a GAM (graph alignment) file",
    "question": "I would like to subset a GAM file to contain less reads so I can visualize it with Sequence Tube Map.\nI've tried\nvg view -a myreads.gam | head -n1000 | vg view -G -J\nbut it doesn't recognize the standard input.\nI was hoping that something like samtools view -s 0.1 bamfile.bam\nThanks",
    "answers": [
      "For the first 1000 reads:\nvg view -a myreads.gam | head -n1000 | vg view -a -G -J - > myreads_1000.gam\nFor downsampling like samtools\nvg filter myreads.gam -d 0.1 > myreads_0.1.gam"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612685/",
    "title": "Reproducibility in Bioinformatics course",
    "question": "Dear all, We are excited to announce our upcoming online course: Reproducibility in Bioinformatics\nDates: 7th–9th July -Daily sessions: 2 PM – 8 PM (Berlin time)\nThis course is designed to increase awareness and provide practical skills to improve reproducibility in bioinformatic analyses. Through a combination of theoretical sessions and hands-on exercises, participants will learn how to work with containers, version control systems, virtual environments, and workflow management tools such as Docker, Singularity, Git, Snakemake, and Nextflow.\nFor more information, please visit: https://www.physalia-courses.org/courses-workshops/bioinformatics-reproducibility/",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612684/",
    "title": "Any tips for optimizing an all vs all alignment with minimap2",
    "question": "The objective of my study is to try and correct a pool of nanopore reads and then filter out any low quality reads that didn't get corrected (below for an example of what I mean. There are 2 highly erroneous reads in this snippet that stand out that I want filtered out of this dataset)\nI'm thinking of using minimap2, but I don't know what parameters to set or how to even go about picking good parameters for this? I'm worried that the -x ava-ont preset won't work well, because most of the reads are not standard nanopore reads, but rather high-quality corrected reads. If anyone has any ideas or recommendations for how to best go about doing the alignment or an even better way to filter out these low quality reads without the use of a reference genome please let me know. I can't just filter them out based on thread scores because the corrections result in .fasta files, not .fastq.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612682/",
    "title": "Mismatch in orthology results from local alignment and phylogenetic analysis.",
    "question": "Hello! I used Diamond to do a local alignment between Isatis tinctoria and Arabidopsis thaliana proteomes. I sorted my hits based on bit scores and chose the top hit as the best hit or ortholog in Arabidopsis for each Isatis gene. I am interested in analyzing a few of these sequences in detail and I took a list of those sequences in Isatis and the corresponding best hits or orthologs from Arabidopsis and made a phylogenetic tree. My issue is as follows: In some cases, my top hits from local alignments are also correctly represented in the tree. But, there are sequences that did not show up as top Arabidopsis orthologs, in my analysis, but still they have corresponding orthologs in Isatis, and form distinct clades in the tree. My newick file is as follows:\n(((AT5G63590:0.08693,(Isati.0715s0020.v1.1:0.00055,Isati.6782s0008.v1.1:0.02950)0.967:0.07033)0.963:0.06333,(Isati.6782s0006.v1.1:0.10572,(Isati.3013s0018.v1.1:0.00797,(Isati.0715s0021.v1.1:0.01039,Isati.6782s0007.v1.1:0.01396)0.244:0.01325)1.000:0.13392)1.000:0.13123)1.000:0.15223,((AT5G63580:0.45733,(**(AT5G08640:0.04952,Isati.4752s0005.v1.1:0.03349)**0.881:0.06364,(AT4G22880:0.76381,AT3G51240:1.20999)0.991:0.46263)0.857:0.05249)0.796:0.02081,((Isati.1778s0010.v1.1:0.04721,(Isati.0382s0006.v1.1:0.16573,(Isati.1317s0015.v1.1:0.00939,Isati.1778s0011.v1.1:0.03979)0.655:0.01919)0.965:0.09734)1.000:0.29822,**(AT5G63595:0.40421,(Isati.6782s0005.v1.1:0.43539,(Isati.5352s0003.v1.1:0.08806,(Isati.6569s0008.v1.1:0.09523,(Isati.0644s0012.v1.1:0.01209,Isati.7517s0001.v1.1:0.03264)**0.837:0.01128)0.064:0.00054)1.000:0.22698)0.406:0.00411)0.734:0.02980)0.858:0.04745)0.853:0.02420,(AT5G63600:0.46052,(AT5G43935:0.18660,(Isati.6782s0004.v1.1:0.04278,(Isati.3712s0003.v1.1:0.00053,(Isati.0715s0022.v1.1:0.00055,Isati.3013s0017.v1.1:0.01651)0.797:0.00347)0.970:0.04268)1.000:0.17695)0.985:0.12234)0.895:0.05119);\nLet me explain with an example. Consider the clade of AT5G63595 (represented in bold). This Arabidopsis gene did not show up as a top hit for any of the Isatis genes in my forward local alignment analysis. While the gene Isati.6782s0005.v1.1:0 showed AT5G08640 to be the best hit in Arabidopsis in my analysis.\nI also tried to globally align this Isatis gene and the two Arabidopsis sequences to gauge the global alignment identity score using Emboss needle. In this analysis as well, the Isatis gene shows greater identity based on global alignment to AT5G08640 and not AT5G63595 as shown in the tree.\nNext, I also calculated the pairwise evolutionary distances between the sequences using Poisson correction model. In this analysis as well, the Isatis gene shows lower evolutionary distance to AT5G08640 than AT5G63595.\nCan someone help me understand what is happening here and why are the tree results different despite higher global alignment score and lower evolutionary distance with AT5G08640 and not AT5G63595?\nIs there a way, I can confidently identify best hits or orthologs using my local alignment results?\nThank you!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612674/",
    "title": "How to automate blasting contigs?",
    "question": "I have sequenced a bacterial genome and obtained contigs using de novo assembly. I obtained over 100 contigs, 4 longer than 50 000 bases, 460 above 1000 bp.\nIs there a way to automate the blasting of these contings?\nIs there a threshold for the length of the valid contigs?\nThank you",
    "answers": [
      "There is nothing to automate if all the contigs are in a single file.\nblastn -query contigs.fna -db nr -task blastn -out contigs_vs_nr.txt -evalue 1e-10 -num_threads 10\nIf you have the contigs in individual files that end in .fna:\nfor i in *.fna\ndo\nblastn -query $i -db nr -task blastn -out ${i%.fna}_vs_nr.out -evalue 1e-10 -num_threads 10\ndone\nFor metagenomes I do not use contigs smaller than 2,000 bp. As you seem to have a pure isolate, a 1,000 cutoff could be used as well."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612675/",
    "title": "Are there tools to find differences in biochemical pathways between organisms?",
    "question": "With the current availability of bioinformatics and genetic data, would it be possible to find out what specific biochemical pathway is associated with a given species?\nFor instance, would it be possible to determine what biochemical pathways are specific for, say, E. coli, in comparison to Enterococcus? Of course, there is a lot of data; let's focus on the quorum-sensing pathway. Using the KEGG database, for instance, it is possible to get the visualization of the pathways for E. coli:\nas well as for Enterococcus:\nIs there a way to determine what of these pathways are different between these two species? Maybe a specific bioinformatic tool or a different database than KEGG?\nThank you.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612673/",
    "title": "DiffBind plot.profile error",
    "question": "Hello, do you know how to resolve the following error?\nError: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\nwhile executing the code:\n> results <- dba.analyze(contrast)\n> mutants <- dba.report(results, contrast=c(1:2, 4), bDB=TRUE)\nGenerating report-based DBA object...\n> mutant_profiles <- dba.plotProfile(results, sites=mutants)\nthe error is the same without the specified contrast:\nprofile <- dba.plotProfile(results)\nThe results look like this:\n> results\n8 Samples, 9041 sites in matrix:\n          ID Tissue   Factor Condition Treatment Replicate    Reads FRiP\n1     X3h1_1     na     X3h1    mutant        na         1 16622186 0.20\n2     X3h1_2     na     X3h1    mutant        na         2 16434472 0.19\n3     lhp1_1     na     lhp1    mutant        na         1 16125186 0.16\n4     lhp1_3     na     lhp1    mutant        na         2 16393211 0.14\n5 lhp1_3h1_1     na lhp1_3h1    mutant        na         1 16203922 0.20\n6 lhp1_3h1_2     na lhp1_3h1    mutant        na         2 14497532 0.20\n7       WT_1     na       WT      wild        na         1 15590707 0.13\n8       WT_3     na       WT      wild        na         2 20354129 0.18\n\nDesign: [~Factor] | 6 Contrasts:\n  Factor    Group Samples Group2 Samples2 DB.DESeq2\n1 Factor     lhp1       2    3h1        2      4886\n2 Factor lhp1_3h1       2    3h1        2      2435\n3 Factor     X3h1       2     WT        2      4563\n4 Factor lhp1_3h1       2   lhp1        2      4667\n5 Factor     lhp1       2     WT        2       939\n6 Factor lhp1_3h1       2     WT        2      5420\nI'd be very grateful for your help!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9611735/",
    "title": "9th Berlin Summer School in NGS Data Analysis 2025 (June 30 - July 4, 2025) -- FINAL CALL --",
    "question": "9th Berlin Summer School in NGS Data Analysis 2025\nWhen? June 30 - July 4, 2025\nWhere? Berlin, Germany\nLink: www.ecseq.com/summer-school\nLearn the essential computing skills for NGS bioinformatics\nUnderstand NGS analysis algorithms (e.g. read alignment) and data formats\nUse bioinformatics tools for handling NGS data\nPerform first downstream analyses for studying genetic variation\nCompare different approaches for differential expression analysis\nThe purpose of this intense one week summer course is to get a deep understanding in Next-Generation Sequencing (NGS) with a special focus on bioinformatics issues. Advantages and disadvantages of current sequencing technologies and their implications on data analysis will be discovered. You will be trained on understanding NGS data formats and handling potential problems/errors therein. In the summer school we will use a real-life RNA-seq dataset from the current market leader illumina.\nAll students will be enabled to perform important first tasks of NGS data analysis themselves. The layout of the summer school has been adapted to the needs of beginners in the field of NGS bioinformatics and allows scientists with no or little background in computer science to get a first hands-on experience in this new and fast evolving research topic.\nIn the evenings there will be social events, like a conference dinner, or a guided city tour through Berlin. These are always great networking possibilities.\nApply Now",
    "answers": [
      "Almost full! Apply now if you still want to take part!\nFirst come, first served.\nwww.ecseq.com/summer-school",
      "I would like to introduce you to our invited speakers:\nProf. Dr. Helene Kretzmer\nHasso-Plattner-Institut\nDr. Helene Kretzmer is a Professor and Chair of Computational Genomics at the Hasso Plattner Institute in Potsdam, Germany. Her research focuses on integrating multi-modal sequencing data, machine learning, and software development to bridge basic and translational research. She is particularly interested in understanding how epigenetic factors regulate early mammalian embryonic development and the epigenetic footprint of cancer.\nDr. Jan Ewald\nScaDS.AI Leipzig\nDr. Jan Ewald is a Data Scientist and Postdoctoral Researcher at Leipzig University, affiliated with the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI). His work centers on developing machine learning models for multi-omics data integration, with a focus on biologically informed neural networks to enhance both prediction accuracy and interpretability in personalized medicine.",
      "Good news! Due to the high demand, we have enlarged the room and extended the registration deadline to 20 June!"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612569/",
    "title": "Random seed in scanpy",
    "question": "Hello,\nI am working with scanpy to analyze some single cell RNA seq data. I was wondering if I should set random.seed(0) at the beginning of my jupyter notebook. Would that keep the results reproducible? Would it cause any issues?\nThank you",
    "answers": [
      "Generally, you need fixed seeds if you want to make analysis reproducible that has a random element. I cannot speak for ScanPy and Python, but in R (towards single-cell, and generally) this could be UMAP/PCA (and most dimensionality reductions), Kmeans and some other clustering approaches, subsampling procedures and more. If there is an option to set a fixed seed then I would always do that (in fact I do). In R you set a seed before calling the function.\nset.seed(1)\ndoSomething()\n...and then the seed is wasted. Setting this once on top of your script is not enough, it will be vanished once the first function that has a random element uses it. Needs to be set before every function. Python might be different. I would check if running analysis several times give precisely same results. If not, could be a seed problem."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612588/",
    "title": "How to obtain the nodes in the graph corresponding to the variant in VCF",
    "question": "I constructed a variation graph with a vcf file and a reference genome using vg construct. I hope to obtain the nodes in the graph corresponding to the variant in VCF. But I don't find the easy way to get it, although I believe that information can be obtained by traversing the reference path and connecting edges. Could you please help me? Thanks.",
    "answers": [
      "Straightforward way is using vg deconstruct -a {vg_graph.gbz} > {variants_in_graph.vcf}. The variants can be refered back to the input vcf based on the position and REF/ALT alleles. Note that the variant ID is modified in the graph."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612659/",
    "title": "miRNA Quantifiaction Tool",
    "question": "So im struggling to find a tool to quantify the reads obtained from the Qiagen mirna library prep kit. Most places online tell me to use an already established pipleline to quanitfy mirna, but they have their own pipelines and settings. Im working on developing my own pipeline and ive read the featurecounts may have an issue when aligned to mirna, the settings could be adjusted to be more cautious of this. But imm left wondering if featurecounts is sufficient enough to quantify known mirna. Other piplines dedup but dont do it based on umi, which is already part of my pipeline.",
    "answers": [
      "Im working on developing my own pipeline\nIs there a specific reason you want to do that? Even if you are learning the process, there are only so many ways the data can be analyzed. A pre-existing pipeline can save time and effort.\nBut imm left wondering if featurecounts is sufficient enough to quantify known mirna.\nfeatureCounts should be fully sufficient to do the counting as long as you have a properly aligned BAM along with the right annotation. Post your command lines if you are running into specific issues.\nThere is a prior post for additional reference: miRNA low mapping(Qiagen miRNA Library Prep) Were you not satisfied with the suggestions in that thread."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612646/",
    "title": "WGCNA pickSoftThreshold problem",
    "question": "Hello everyone!\nI'm new to WGCNA and currently experiencing some problems...\nI want to get a scale-free topography for my RNA-seq data, but the R2 coef varies from -1 to 0.6. I don't understand how to interpret negative R2 values and how to pick softThreshold in this case.\nI normalized my data with vst, then picked genes with low CV and high variance, and got ~2.5K genes at the end. I transposed the data so my columns are genes, and rows are the samples.\nI did quality check and eliminated one outlier identified on the dendrogram.\nNevertheless my R2 has the following profile and . The first plot is for signed and the second is for unsigned.\nCould anybody give me some tips on how to make this thing work? Thank you a lot in advance",
    "answers": [
      "2.5K genes sounds to me like over-filtering; and what was your rationale for removing variable (high CV) genes? My suggestion would be to only filter on detectability, typically a hard threshold on expression value or rank (and possibly remove outlier samples/genes)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612645/",
    "title": "Fungal Annotation Comparison",
    "question": "Hello,\nI kind of stuck in a situation. Where i need to compare two annotations of a single Fungal genome.\nGenome 1\nDownloaded .fa and annotation from NCBI (Initially published in 2014)\nGenome sequenced using Illumina and assembled using ALLPATHS-LG keeping min contig size > 1000bp resulting in total 168 supercontigs\nfor the annotation authors used RNASeq data which contains\n15 strand-specific and paired read datasets\n18 non-stranded and paired RNA-Seq datasets\n4 EST datasets\nIn short authors used RNASeq data sets of multiple fungal genus.\nfor annotation authors Implied following process\nTranscriptome assembly using Trinity -> Transcriptome alignment to genome using PASA -> gene models using predictions from GeneMarkES, GeneId, Augustus, GlimmerHMM, and SNAP\nfinal annotation on NCBI contains ~17642 genes of which ~17280 are protein coding\nNow I got the exact same strain which we resquenced\nGenome-2\nSequenced using Nanopore R10 and Illumina ShortRead\nAssembled using Flye and polished with Shortread data. keeping min contig size > 5000bp resulting in 68 scaffolds (11 chromosomes and remaing unplaced scaffolds)\nRepeats annotated using RepeatModeler/Masker\nFor annotation we used RNASeq data specifically from this strain with inhouse experiments\nStrain was used to do infection assays and RNA was extracted/sequenced (4-replicates)\nSimilarly, Non infection strain control (4-replicates)\nFLask grown strain RNA (4-replicates)\nI used all this data and followed the Funannotate pipeline fo annotate the genome. Which used similar tools in the annotation process.\nMy final annotation has ~14170 genes and ~14968 proteins predicted\nAs you can see there is a difference of around 3000 genes/proteins in my annotation and NCBI annotation.\nMy Question\nWhat can be sourse of this difference ? My guess is \"its because in NCBI genome, authors used a wide verity of RNAseq data but i only used a small strain specific data\"\nHow can I compare the two annotations, to see what is similar and what is defferent, Any tool/pipiline or process that can help in this regrad.\nCan this be because of the Contigs cutoff size ? as far as i know the average gene size is around ~1838.69 bp in my annotation and my supervisor also mentioed that there wont be any genes under 5000 bp.\nI am stuck on this process and cant seem to find any answer. Your input will be a great help.\nRegards",
    "answers": [
      "I can think of three possible explanations:\na) The first annotation has some false positive predictions (which means some non-coding regions were incorrectly predicted as genes).\nb) The first annotation has haplotypic duplications (which means some alleles were assembled separately instead of being collapsed into a single sequence).\nc) The second annotation has some false negative gene predictions (which means there were unnoticed genes)\n\nThe main thing I would do is analyzing proteins from both annotations with BUSCO. The annotation that has higher BUSCO completeness (\"C\") and less orthogroups with duplicate genes (\"D\") is better.\n\nI don't think that the contig length cutoff in the second genome assembly affected the annotation significantly, because based on the low number of scaffolds I suppose that the assembly is quite good, and, thus, all or almost all genes are in long contigs.",
      "This question is difficult to answer satisfactorily. But I will take a stab.\nWhile it may be the same strain, technology keeps moving ahead and it is likely that the assembly you ended up with is better (mainly because you had long reads in mix). (We will keep the \"strain\" discussion aside since it is possible that the strain used for NCBI assembly may be significantly different than what you have in your lab. Whether that accounts for a difference for 3000 genes ... that seems a little excessive so more than likely that is due to gene prediction/annotation).\nUser submitted assemblies undergo the NCBI annotation process at some point and after manual curation they will end up getting a RefSeq version of the genome. You don't say if the annotation you used from NCBI for comparison was RefSeq version. If not, you should definitely download RefSeq version for comparison of annotation.\nWhile NCBI's Eukaryotic annotation pipeline is available for download, it currently says that fungi are not supported. You could instead use the Request Annotation tab at the top of https://www.ncbi.nlm.nih.gov/refseq/annotation_euk/process/ page and see if NCBI would be willing to run your assembly through their pipeline. That may be one way of getting an annotation that can be more directly compared. Other would be for you to annotate the NCBI assembly using funannotate.",
      "There are many reasons for this discrepancy as others have said\nconsider that different technologies are behind the assemblies. Long vs short reads. Long reads lead to less fragmented assemblies and annotations and are more accepted these days (standard).\ntry reannotating both assemblies with helixer (they have a web-service too) to enable apples vs apples comparisons\nBUSCO is a good idea as @shelkmike said\nUse a tool like proteinortho to do the blasts (mentioned by lieven.sterck and get a comparative summary table out to compare the common or singleton genes"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612640/",
    "title": "Is comparing seeds sufficient, or should alignments be compared instead?",
    "question": "In seed-and-extend aligners, the initial seeding phase has a major influence on alignment quality and performance. I'm currently comparing two aligners (or two modes of the same aligner) that differ primarily in their seed generation strategy.\nMy question is about evaluation:\nIs it meaningful to compare just the seeds — e.g., their counts, lengths, or positions — or is it better to compare the final alignments they produce?\nI’m leaning toward comparing .sam outputs (e.g., MAPQ, AS, NM, primary/secondary flags, unmapped reads), since not all seeds contribute equally to final alignments. But I’d love to hear from the community:\nWhat are best practices for evaluating seeding strategies?\nIs seed-level analysis ever sufficient or meaningful on its own?\nWhat alignment-level metrics are most helpful when comparing the downstream impact of different seeds?\nI’m interested in both empirical and theoretical perspectives. Thanks in advance!",
    "answers": [
      "Based on my experience in developing an alignment tool, I believe both are important.\nIn the seeding phase, the priority is ensuring high sensitivity. So you need to do an initial sensitivity assessment, by checking if seeding or chaining results covered the true sequence and positions with both mutated and mutation-free queries.\nIdeally, seeds/chains with high scores (more or longer seeds) would also improve the specificity, especially helpful to return only top matches. But the chain with the highest score does not always point to the best alignment, so final alignment is also necessary."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612499/",
    "title": "How to download protozoa reference genomes from NCBI",
    "question": "I want to download all available protozoa genomes from the NCBI database. Using ncbi-datasets download is unfortunately not an option, as it doesn't recognize \"protozoa\" as a valid taxon.\nHowever, I found that the genomes are shared via FTP at the following locations: GenBank - protozoa RefSeq - protozoa\nI tried downloading the contents using wget, but it only retrieves the directory listings — the actual genome files inside the subfolders (e.g., .fna.gz, .gbff.gz, etc.) are not being downloaded recursively.\nThis is the command I used: wget -r --continue --progress=bar:force:noscroll ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/\nHow can I modify this or use another method to properly download all genome files, including those in the subdirectories?\nAny help or suggestions would be greatly appreciated!\nP.S I'm also interested if there is any way to download only microfungi genomes.",
    "answers": [
      "I found a solution of my problem. In case it will be helpful to anyone I leave a workflow here:\nDownload assembly_summary.txt file\nwget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/protozoa/assembly_summary.txt\nExtract FTP paths matching required parameters\ngrep -v \"^#\" assembly_summary.txt | \\ awk -F '\\t' '\n$25 == \"protozoa\" && $11 == \"latest\" && $5 == \"reference genome\" {\nprint $20 > \"protozoa_ftp_paths.txt\";\nprint $0 > \"protozoa_assembly_summary_filtered.txt\"\n   }\nAppend the filename based on the directory name.\n mkdir -p protozoa_fna\n\n    cd protozoa_fna\n\n    while read dir; do\n        base=$(basename \"$dir\")\n        file=\"${base}_genomic.fna.gz\"\n        full_url=\"${dir}/${file}\"\n        echo \"Downloading $full_url\"\n        wget -c \"$full_url\" --progress=bar:force:noscroll\n    done < ../protozoa_ftp_paths.txt",
      "I know they're not the same, but you might be interested in https://protists.ensembl.org/index.html as well. I don't know how up to date and consistent it is compared with NCBI."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612609/",
    "title": "Clustering on Big data (100 million samples)",
    "question": "I am working with the huge data, I have 100 files of same kind of data in homogeneous format. Each file containes 10,00,000 smaple. each sample is fingerprint (1024) of compound. I need to cluster compounds, I have to reduce their dimensions from 1024 to 100. I can't reduce Dimensions in batches (using UMAP) and also can't cluster in batches. I can't even concatenate all files in a single files because of RAM constraints. Looking for some better time and memory efficient approach.\nWhat I planned is I will UMAP.fit() on random 10 files from 100 files (as all data is of same kind) which will work on mutiple cores and further using fitted UMAP I will UMAP.transform() on all 100 files to reduce it to n_components = 100 and will keep concatenating it to the single file (which my system can hold in RAM) and further I will Cluster that single file using fast_HDBSCAN (as it works on multi cores). Are there any disadvantages of this approach? is it even a right way to do?\nhoping for constructive responses...",
    "answers": [
      "The right approach would be to find a server with plenty of memory and do this all at once. That said, you might be able to get away with it.\nWhat are disadvantages? When you pick your initial random collection and do the embedding, you might have a non-representative group of files. If that's the case, some groups will pop up later during UMAP transformation, and their relationship with others may not be mapped properly. If you are lucky and at least one representative of each group is included in the initial group, this could work."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9583651/",
    "title": "Where can I find a vcf file with individuals in it, that have a specific disease?",
    "question": "I built a tool for PRS - Calculation and interpretation for my thesis, and in order to evaluate it, I need a .vcf file from one (or many) individuals with a certain disease (ideally: Hypertension, Breast cancer, Lung cancer or Type 2 Diabetes).\nDoes anybody have any ideas where I can find data like that?",
    "answers": [
      "human data often comes with restrictions on access due to privacy. the uk biobank and nih all of us require applying for access for example\nyou can access certain types of data freely on certain types of patients however in other circumstances\nfor example, here are somatic mutations from the TCGA (the cancer genome atlas) in vcf format. this is just the somatic mutations called with various tools. cnv calls, gene expression, and other things are also available with uncontrolled access for bam/cram/germline vcf require controlled access https://portal.gdc.cancer.gov/repository?filters=%7B%22op%22%3A%22and%22%2C%22content%22%3A%5B%7B%22content%22%3A%7B%22field%22%3A%22cases.case_id%22%2C%22value%22%3A%5B%22set_id%3AkNa6uowBB1xFKettIGZf%22%5D%7D%2C%22op%22%3A%22IN%22%7D%2C%7B%22op%22%3A%22in%22%2C%22content%22%3A%7B%22field%22%3A%22files.data_format%22%2C%22value%22%3A%5B%22vcf%22%5D%7D%7D%5D%7D\nthere is also the 1000 genomes dataset, which is a fully open repository of human sequencing data with no controlled access (which is quite unique)! the clinical data available with the 1000 genomes is limited but it has been analyzed in some papers, for example https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0135193 and it has full unrestricted access to bam/cram/vcf (including germline/ancestry info) at https://www.internationalgenome.org/data that said, you may be going out on a limb trying to clinically characterize the 1000 genomes project",
      "I would recommend trying the NIAID Data Ecosystem because you can search simultaneously across numerous existing repositories. I just briefly tried using the advanced search tool to find datasets with 'SNP' as the Topic Category and 'lung cancer' anywhere else in the record and got 72 results: https://data.niaid.nih.gov/search?q=topicCategory.name%3A%22snp%22+AND+%22lung+cancer%22\nThese look like mostly xlsx files but you should be able to convert them. You could also try other combinations of search/filters to get something else or narrow it down to your specific parameters.",
      "UK biobank : https://www.ukbiobank.ac.uk/enable-your-research/apply-for-access"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612635/",
    "title": "ERROR::INVALID_TAG_NM:Record A, Read name B, NM tag (nucleotide differences) in file [0] does not match reality [1]",
    "question": "I seem to be getting a malformed bam from bwa mem\nAfter bwa mem & using GRCh38DH, 1000 Genomes Project version as ref I run ValidateSamFile and get many errors as follows:\nERROR::INVALID_TAG_NM:Record 349909, Read name LH00400:21:22N52NLT4:7:1101:27171:1154, NM tag (nucleotide differences) in file [0] does not match reality [1]\nI inspect one of the offending reads and see:\nCIGAR: 101M48S\n\nRNAME: chr2\n\nPOS: 16145020\n\nNM tag: NM:i:0\nI pulled the corresponding ref sequence to compare to the read sequence:\nsamtools faidx my_ref.fa chr2:16145020-16145168\nThe read and ref are identical for the first 100 bases, eg:\nref[:100] == read[:100]\n\nTrue\nbut not after that:\nref[:101] == read[:101]\n\nFalse\nstarting with base 101 ref is all 'N''s\nref[100:]\n\n'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\nwhat is going on here?\nMy main two questions are:\nWhy does the CIGAR indicate 101M when it seems only the first 100 bases match between ref and read?\nwhere exactly is this INVALID_TAG_NM error coming from and how to fix (or can this be ignored)? I tried running samtools calmd -br {input.bam} {input.ref} and I can get ValidateSamFile to pass, but am concerned about why I would have to do this at all.",
    "answers": [
      "or can this be ignored\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360036854731-ValidateSamFile-Picard#--IGNORE\nList of validation error types to ignore.\n--IGNORE  INVALID_TAG_NM\nwhere exactly is this INVALID_TAG_NM error coming from\nhttps://github.com/utgenome/picard/blob/8ef0bb735d06ffd1d4a399e15632713adbad7bb6/src/java/net/sf/picard/sam/SamFileValidator.java#L418C34-L418C48"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612496/",
    "title": "Cut&Run TF Analysis - Very Low Peak Counts Despite Good Library Quality",
    "question": "I’m analyzing Cut&Run data for a transcription factor with the following setup:\nMultiple time points with replicates\nIgG control processed identically as a BAM file as control for macs2\nLibraries have excellent quality (0.7% duplication rate before peak calling)\nUsing MACS2 callpeak with BAMPE format and hg38 reference and also using a IgG control which i ran through the same steps until peak calling and then used it as a BAM file control against the treatment\nCurrent filtering: Using samtools filter with less than 120bp size cutoff for TF analysis as per https://doi.org/10.1186/s13059-019-1802-4\nAlso filtering for “filters”: [ { “id”: “1”, “isProperPair”: “true”, “mapQuality”: “>=30”, “reference”: “!chrM” } ]\nMain Issues:\nVery low peak counts: Only 11-16 peaks per sample (expecting more)\nNo motifs found: MEME-ChIP returns no significant motifs\nAlso seeing low alignment percentage of about 50 - 70 across samples after bowtie2\nHigh duplicate reads in both sample and control - 40 to 50 per cent before and after alignment. I performed dedup but many CUTRUN analysis methods mention to retain duplicates.\nQuestions:\nShould I remove the <120bp size filter for Cut&Run peak calling?\nShould i not remove duplicates? In some cut and run analysis I have seen that they recommend not to remove. https://doi.org/10.1186/s13059-019-1802-4\nAre there better motif discovery tools than MEME-ChIP for novel TF targets with few peaks?\nCould MACS2 parameters be too stringent?",
    "answers": [
      "Show an IGV screenshot. Probably your IP simply did not work and data are not usable.\nLibraries have excellent quality (0.7% duplication rate before peak calling)\nI assume you mean duplicates marked in the BAM file? If so, then this is more a sign of a bad library because enriched libraries like CUT&RUN typically have not that low of a duplication percentage. That low is expected when sequencing genomic DNA at the typical C&R depth, indicating that you have indeed a failed IP and your library is mainly gDNA background.\nHigh duplicate reads in both sample and control - 40 to 50 per cent before and after alignment. I performed dedup but many CUTRUN analysis methods mention to retain duplicates.\nHow is this in line with the quote above?\nShould i not remove duplicates? In some cut and run analysis I have seen that they recommend not to remove. https://doi.org/10.1186/s13059-019-1802-4\nFor peak calling I would always remove them, despite if the difference between number of peaks with and without duplicates is \"day and night\" it indicates that something is very wrong.\nAre there better motif discovery tools than MEME-ChIP for novel TF targets with few peaks?\nThe problem is almost certainly data quality and not software, and this cannot be rescued in silico."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612630/",
    "title": "Ensembl Genebuild Project Leader, EMBL-EBI, Hinxton near Cambridge, UK",
    "question": "We have one job open at the moment:\nJob: Ensembl Genebuild Project Leader Please follow the instructions in the ad to apply (https://www.ensembl.info/2025/06/02/job-ensembl-genebuild-project-leader/) Closing date: 30 June 2025\nAll the best, Ensembl",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612621/",
    "title": "tools to increase hi-c resolution",
    "question": "Hi, all,\nWhile numerous tools have been developed to predict high-resolution data from low-input samples, none of them can directly output files in .hic or .mcool formats.\nIt would be greatly appreciated if you could share these tools/resources.\nBest, Junfeng",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612618/",
    "title": "How to fix this this tangle in my bacterial genome assembly",
    "question": "Hi,\nI am new to the genome assembly space. I have recently tried to assemble a bacterial genome and the resulting assembly graph looks like this: https://imgur.com/a/BHxtjFG\n4.2Mb of it seems correctly assembled, but there is a 6kb or 12kb region, probably repetitive, that is tangled and unresolved.\nI'm wondering what I should do at this stage?\nDoes anyone have any tips or resources for best practices for manual curation of genomes.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612613/",
    "title": "Help configuring RepeatMasker",
    "question": "Hi,\nI’m trying to use RepeatMasker with the GRCh38.p14 genome from NCBI. I've downloaded the Dfam-7.hmm.gz.md5 file, but I'm unsure about how to properly reconfigure RepeatMasker to use this file. Any advice would be gratly appreciated.\nTaxon \"homo sapiens\" is in partition 7 of the current FamDB however,\nthis partition is absent.  Please download this file from the original\nsource and rerun configure to proceed.\nThanks.",
    "answers": [
      "There's a configure script in the RepeatMasker distribution. From the documentation: https://www.repeatmasker.org/RepeatMasker/\nRun Configure Script: The program requires some initial configuration. This should also be re-run after updates to the library files.\ncd /usr/local/RepeatMasker # if installed as by the manual\ncd $CONDA_PREFIX/share/RepeatMasker # if installed with conda...\nperl ./configure\nThe only difficulty here is to locate the script and to place the library in the right directory. If RM is installed with conda, the configure location is likely: $CONDA_PREFIX/share/RepeatMasker/ and it still has to be run after conda install."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612608/",
    "title": "How to process Iso-Seq files in SRA public fastqs",
    "question": "I am currently practicing Iso-seq data. I downloaded SRR30229262 from NCBI and try to process as paper link claimed. Program: lima v2.13.0 / pbmm2 v1.17.0 I used the genome in our lab here, but I think it can be reference here.\npbmm2 align --unmapped --sort -j 128 00_start_annotation/JPeel5_2.fasta SRR30229262.fastq SRR30229262.bam\nnohup lima --isoseq -j 128 05_raw_RNA_PacBio/SRR30229262.bam 05_raw_RNA_PacBio/primers.fasta05_flnc_RNA_PacBio/SRR30229262_flnc.bam  > 05_flnc_RNA_PacBio/SRR30229262_flnc.log 2>&1 &\nBut lima seems to not take any computation resources to process, and only show that it is running when I use \"jobs\". I am not sure how to solve this tough question. The primer I used:\n>primer_5p\nAAGCAGTGGTATCAACGCAGAGTACTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n>primer_3p\nAAGCAGTGGTATCAACGCAGAGTAC",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/442882/",
    "title": "ACMG assignment tools",
    "question": "Hi, I've been asked to semi automate (as far as full automation of the process seems to be impossible) the ACMG annotations of WGS data for our clinicians.\nWhat are the options here? I've checked several:\nvarsome (paid full access, closed code)\nInterVar (seems nice, but works only with annovar, and I need to annotate ACMG terms on VEP output).\nTAPES ( (recent paper, supposedly support VEP), but github page is dead and no response to the issues, crashes on exapmle files, so not an option.\nCharger (investigating it right now - but the documentation is sparse :( ).\nAm I missing something else here? Did anyone, by chance, had a luck to fit the VEP output to InterVar?\nBest, Eugene",
    "answers": [
      "Hey, if you are still searching for ideas on how to implement the semi-automation, that you have described, you may also check autoPVS1 tool.\nautoPVS1 paper\nautoPVS1 github\nEdit: got to know some more ACMG tools! (also on 25.02.2021)\nGeneral Mendelian disease:\nSherloc\nClinGen Pathogenicity Calculator\n(25.02) Ella\n(25.02) EVIDENCE\nCancer disease:\nCPSR\nCharGer\nPathoMAN\nClinVAP\nHope it helps :)",
      "Hi, free ACMG implementation without restrictions can be found at https://genebe.net . AFAIK there is also a free API coming.",
      "Hi, you can try varsome.com, there is an implementation of ACMG rules and you can query variants for free.",
      "Great points, there!\nI'd like to employ KAVIAR and Sherloc guidelines , APIs if available.\nUnfortunately, KAVIAR guidelines are not updated last few years"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612606/",
    "title": "EggNOG-mapper problem",
    "question": "Hi all,\nI'm running EggNOG-mapper on a metagenomics dataset for functional annotation using the DIAMOND mapping mode on a high-performance virtual machine in Google Cloud Platform (GCP). The dataset is approximately 200 GB in size, consisting of .faa protein sequence files generated by FragGeneScan. I’m using the latest EggNOG-mapper version with the DIAMOND and annotation databases downloaded locally to the instance.\nThe GCP virtual machine I'm using is an n2-standard-96 with 96 vCPUs and 384 GB RAM, running Ubuntu 22.04 (minimal image). My data disk is a 2 TB SSD persistent disk, with SCSI interface and x86/64 architecture. There's no swap configured, though the available memory is more than sufficient. The EggNOG-mapper database (eggnog_db) and temporary files are stored on the boot disk (/tmp).\nI run my annotation script in a loop, processing each .faa file one at a time, using all 64 cores. Here's the command I'm using:\nmkdir -p ~/workdir/eggnog/eggnog_db && cd ~/workdir/eggnog/eggnog_db \n\n#Download the EggNOG database (eggnog.db.gz) and unzip it \nwget -c http://eggnog5.embl.de/download/emapperdb-5.0.2/eggnog.db.gz \nwget -c http://eggnog5.embl.de/download/emapperdb-5.0.2/eggnog_proteins.dmnd.gz \nwget -c http://eggnog5.embl.de/download/emapperdb-5.0.2/eggnog.taxa.tar.gz \n\ngunzip eggnog.db.gz \ngunzip eggnog_proteins.dmnd.gz \ntar -xvzf eggnog.taxa.tar.gz \n\n#emapper functional annotation \n\nINPUT_DIR=\"$HOME/workdir/fraggenescan_out\"    # Current directory or change as needed \nOUTPUT_DIR=\"$HOME/workdir/eggnog\"    # Output folder for all results \nDB_DIR=\"$HOME/workdir/eggnog/eggnog_db\"  # Path to your EggNOG database \nCPU=64 \n\n\nfor faa_file in \"$INPUT_DIR\"/*_FGS.faa; do \n    base_name=$(basename \"$faa_file\" _FGS.faa) \n    output_prefix=\"${base_name}_annotation\" \n\n    emapper.py \\ \n      -i \"$faa_file\" \\ \n      --itype proteins \\ \n      -o \"$output_prefix\" \\ \n      --output_dir \"$OUTPUT_DIR\" \\ \n      --cpu \"$CPU\" \\ \n      --data_dir \"$DB_DIR\" \n\ndone \nThe DIAMOND step performs very well — CPU usage is fully utilized with over 60% in user space. However, once the workflow transitions into the Python-based annotation step, performance drops sharply. CPU usage becomes dominated by the kernel, with about 97% system (sy) usage and only 2–3% user (us) usage, as seen in top. RAM usage remains very low (under 5 GB), and no swap is used, despite the system having ample memory.\nTo try and improve speed, I also tested a parallel processing approach using GNU parallel (10 jobs* 8 cores) to annotate multiple .faa files at once, assigning fewer cores to each. However, this did not improve performance and actually made things slower overall — most likely due to I/O contention or internal bottlenecks within EggNOG-mapper’s post-DIAMOND steps.\nI’m looking for guidance on how to reduce the system CPU load and speed up the annotation step. Is this behavior typical for large datasets with EggNOG-mapper? Are there additional flags or optimizations that could help with the annotation performance?\nThanks in advance for any suggestions or insights",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612605/",
    "title": "Colsep for dendrogram and ColSideColors in heatmap.2",
    "question": "Hi dear community,\nI created a heat map of DE genes from my RNA-Seq experiment with the heatmap.2 function. I would like to seperate the two main clusters in my heat map visually.\nI managed to seperate the columns using the colsep and sepwidth arguments of the function but cant manage to seperate the dendrogram or the colours of the groups.\ncol.pan <- colorpanel(100, \"blue\", \"white\", \"red\")\ncolrow.colors <- case_when(\n  colnames(logCPM) == \"No\" ~ \"yellow\",\n  colnames(logCPM) == \"Yes\" ~ \"red\"\n)\n\nheatmap_plot <- heatmap.2(logCPM, \n          col = col.pan,\n          Rowv = TRUE,\n          scale = \"none\",\n          trace = \"none\", \n          dendrogram = \"column\", #both \n          cexRow = 5, \n          cexCol = 5, \n          density.info = \"none\",\n          margin = c(10, 30),\n          ColSideColors = colrow.colors,\n          labCol = \"\",\n          lhei = c(0.85, 6.8),\n          lwid = c(2.5, 20),\n          keysize = c(0.6),\n          key.par = list(cex = 3),\n          labRow = NA,\n          colsep = c(40),\n          sepwidth = c(2, 5),\n          distfun = function(x) dist(x, method=\"euclidean\"),\n          hclustfun = function(x) hclust(x, method=\"ward.D2\")\n          )",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612602/",
    "title": "Decision making on choosing the based on tblastn result",
    "question": "https://blast.ncbi.nlm.nih.gov/Blast.cgi![\nenter image description here\n]1\nI have performed tblastn using protein accession number against a refrence genome , as shown in the image attached top hit has E-low and % identity high also low where as the latter .how to decide",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/352256/",
    "title": "How to Identify the pathogenicity of a set of variants (by using ACMG guidelines)?",
    "question": "Dear Friends,\nAm trying to find the pathogenicity of a set of variants (by using ACMG guidelines: https://www.ncbi.nlm.nih.gov/pubmed/25741868 ). The vcf file of the variants looks like this:\nCHROM   POS     ID      REF     ALT     GT\n22      11005678        .       G       A       0|1\n22      16052167        .       A       AAAAC   0|1\nI went through the paper but did not find a clear idea - could you please let me know the steps to predict the pathogenicity of such variants?\nThanks, DK",
    "answers": [
      "Please check out our implementation of ACMG guideline: https://github.com/ding-lab/CharGer\nBest, Kuan Huang Lab | Computational Omics @ MSSM",
      "Dear DK,\nDon't know if you are still interested in bioinformatics tools to implement ACMG but if you do let me share this biostars post with some pointers (some written by me..).\nHope it helps, Damianos"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612591/",
    "title": "Why am I getting the exact same MarkDuplicates results when passing a query or coordinate sorted bam?",
    "question": "I am following a standard protocol for alignment, which mentions:\nIf a primary alignment is marked as duplicate, then all supplementary alignments for that same read should also be marked as duplicates ... For Picard, you must use >= version 2.4.1 and run on a queryname sorted input file.\nAlso from the picard MarkDuplicates docs:\nWhen the input is coordinate-sorted... supplementary/secondary alignments are not marked as duplicates. However, when the input is query-sorted (actually query-grouped), then ...secondary/supplementary reads are not excluded from the duplication test and can be marked as duplicate reads.\nThus I would expect that there would be a difference in output from MarkDuplicates when either coordinate or queryname sorted crams are used as input, but my pipeline is giving the exact same results regardless. What could be causing this?\nhere are my commands:\nalignment\n    bwa mem \\\n    -K 100000000 \\\n    -Y \\\n    -R \"@RG\\\\tID:{FC}.{LN}\\\\tSM:{SM}\\\\tPL:ILLUMINA\\\\tPU:{FC}.{LN}.{SAMPLE_BARCODE}\\\\tLB:{LB}\\\\tCN:{CN}\" \\\n    {ref} \\\n    {read1} \\\n    {read2} | \\\n    samtools view  -C --reference {ref} -o {cram} -\nsort by coordinate\nsamtools sort  -O CRAM --reference {ref} -o {coord_sort_cram} {cram}\nsort by queryname\nsamtools sort -n -O CRAM --reference {ref} -o {query_sort_cram}  {cram}\nmark dups of coordinate sorted\n    MarkDuplicates \\\n    I={coord_sort_cram} \\\n    O={coord_sort_cram.mark_dup_cram} \\\n    M={coord_sort_cram.dup_metrics} \\\n    R={ref} \nmake dups of query sorted\n   MarkDuplicates \\\n    I={query_sort_cram} \\\n    O={query_sort_cram.mark_dup_cram} \\\n    M={query_sort_cram.dup_metrics} \\\n    ASSUME_SORT_ORDER=queryname \\\n    R={ref}\nWhy are these the same?\n{coord_sort_cram.dup_metrics}\n{query_sort_cram.dup_metrics}",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612589/",
    "title": "use AverageExpression to stratify samples",
    "question": "I have a group of cancer patient samples for scRNA datasets. I need to stratify my samples based on the expression level of one gene (low vs high) in the tumor cells and examine their T cell components.\nIs it reasonable to extract the tumor cell cluster and rank the gene expression level using AverageExpression function? I will choose the 1st and 4th quartile or the top and bottom half for following comparisons.\nNot sure it is a bias method and it seems hard to find literatures with similar question.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612585/",
    "title": "Multiple taxonomy hits per OTU ID in VSEARCH despite --top_hits_only (90% ID after QIIME2 clustering at 97%)",
    "question": "Hi all,\nI clustered my ASVs into OTUs at 97% similarity using QIIME2's cluster-features-de-novo:\nqiime vsearch cluster-features-de-novo \\\n  --i-table asv_table.qza \\\n  --i-sequences asv_sequences.qza \\\n  --p-perc-identity 0.97 \\\n  --o-clustered-table otu_table_97.qza \\\n  --o-clustered-sequences otu_sequences_97.qza\nFor taxonomy assignment of these OTUs, I used standalone VSEARCH (not QIIME2) with the following parameters:\nvsearch --usearch_global dna-sequences.fasta \\\n  --db SILVA_138.1_SSURef_NR99_tax_silva_fixed.fasta \\\n  --id 0.90 \\\n  --maxaccepts 20 \\\n  --maxrejects 0 \\\n  --output_no_hits \\\n  --blast6out amf_taxonomy_vsearch_max20.csv \\\n  --top_hits_only\nHowever, even with --top_hits_only, I’m getting multiple hits per OTU ID in the blast6out result. I suspect this happens when multiple reference sequences match equally well (i.e., identical percent identity and bit score).\nQuestions: Is it expected that --top_hits_only still yields multiple hits when --maxaccepts is set to 20? I thought it would limit to one.\nWhat is the best practice to retain only one best taxonomy hit per OTU in this case? Should I manually select?\nAny suggestions for handling the long SILVA taxonomy strings with many levels? How can I clean or truncate them to a 7-level (kingdom to species) hierarchy in R?\nWould doing taxonomy assignment using QIIME2’s classify-consensus-vsearch help avoid this issue, or does it behave similarly?\nAny insight from your experience or pipeline practices would be highly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612583/",
    "title": "RepeatMasker output file question",
    "question": "I would like to ask, when the confident_TE.cons.fa file of the Transposon Annotation Tool is used as the -lib input of RepeatMasker, does the annotation file of RepeatMasker contain all the sequence information in confident_TE.cons.fa, or does it only contain the sequences that can be aligned with RepeatMasker?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612514/",
    "title": "Which GWAS tool to use that can work on Czech cases and British controls, on a binary trait?",
    "question": "Hi,\nI am running a GWAS on 482 Czech cases and 1928 British controls for a binary trait (1:4 case - control ratio) (Attached is their PCA plot, PC1 against PC2 showing clear separation).\nI ran \"plink2 --glm\" with various included covariates checking how the lambda value and number of significant snps change. ( Attached as \"gwas_summary.txt\"). For one example, I am also attaching the log file when PC1 + PC2 was used, and the resulting QQ-plot.\nIf PC2 is included than basically I receive \"UNFINISHED\" error code for all 5.8 million included snps.\nThe first few PC-s seem important given the difference in EUR subpopulations between the cases and controls , but then I do not receive a result if they are included in the analysis.\nI see that the Czech is a more homogeneous cohort compared to the diverse British - where SNP frequency difference could arise from population structure beside being affected by the disease.\nWhat could be a solution that is computationally efficient? ( using \"logistf\" R package with increased iteration number seems a good method for a subset of snps but maybe not genome-wide?)\nWould you recommend another GWAS tool to use?\nBest wishes, Anita\nPLINK v2.0.0-a.6.12LM AVX2 AMD (20 Apr 2025)\nOptions in effect:\n  --ci 0.95\n  --covar /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/Czech/czech_age_sex_10_PC.txt\n  --covar-name PC1 PC2\n  --extract-if-info R2 >= 0.8\n  --glm hide-covar firth-fallback\n  --keep /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/Czech/matched_controls_czech_cases.txt\n  --out /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/output/czech_pca_czech_cohort_matched_controls/pc1.2/output/gwas_results\n  --pfile /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/Czech/main_cohort\n  --pheno /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/phenotypes.txt\n  --pheno-name PHENO\n  --remove /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/0.samples_info/remove_samples.txt\n  --seed 1\n  --threads 1\n\nHostname: overdrive\nWorking directory: /media/pontikos_nas2/AnitaSzabo/phd_projects/overdrive_scripts/temp_phd/gwas_2024/genotype_to_gwas_scripts\nStart time: Sun May 25 20:51:36 2025\n\n257622 MiB RAM detected, ~218229 available; reserving 128811 MiB for main\nworkspace.\nUsing 1 compute thread.\n16416 samples (0 females, 0 males, 16416 ambiguous; 16416 founders) loaded from\n/media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/Czech/main_cohort.psam.\n5800849 out of 5818824 variants loaded from\n/media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/Czech/main_cohort.pvar.\n1 binary phenotype loaded (482 cases, 15934 controls).\n--keep: 2410 samples remaining.\n--remove: 2410 samples remaining.\n2 covariates loaded from /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/QC/Czech/czech_age_sex_10_PC.txt.\n2410 samples (0 females, 0 males, 2410 ambiguous; 2410 founders) remaining\nafter main filters.\n482 cases and 1928 controls remaining after main filters.\nCalculating allele frequencies... done.\n5800849 variants remaining after main filters.\n--glm logistic-Firth hybrid regression on phenotype 'PHENO': done.\nResults written to /media/pontikos_nas2/AnitaSzabo/phd_projects/gwas_2024/analysis/5.gwas/pipeline_output/without_high_LD_regionin_PCA/output/czech_pca_czech_cohort_matched_controls/pc1.2/output/gwas_results.PHENO.glm.logistic.hybrid .\n\nEnd time: Sun May 25 21:52:23 2025",
    "answers": [
      "There is really no way for you to do this analysis - having cases and controls perfectly confounded by populations means that every variant with an allele frequency difference between populations will appear to be associated with the trait. You need to add British cases and Czech controls, full stop. No software package will fix this."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612580/",
    "title": "Unsure whether or not I overcorrected for batch effect in dataset",
    "question": "Hi,\nI am doing a timecourse study looking at the differences between sexs and time. I had noticed from my PCA plot that there was a lot of large group variation within my biological replicates.\nIt looks to me that some of the points are clustering together. I performed a batch effect to account for unknown sources of variation and created a new PCA plot to examine the clustering within my samples. However, I am not sure whether I overcorrected for any possible unknown variation within my data.\nMy code is attached below.\n```{r setup, include=FALSE}\n\n\n\nBrown_Fat <- gene_fpkm %>%\n  select(gene_id,\n    MBF2_2, MBF2_3, MBF6_1, MBF6_2, MBF6_3, MBF10_1, MBF10_2, MBF10_3, MBF14_1, MBF14_2,\n    MBF14_3, MBF18_1, MBF18_2, MBF18_3, MBF22_1, MBF22_2, MBF22_3, FBF2_1, FBF2_2, FBF2_3, FBF6_1,\n    FBF6_2, FBF6_3, FBF10_1, FBF10_2, FBF10_3, FBF14_1, FBF14_2, FBF14_3, FBF18_1, FBF18_2,\n    FBF18_3, FBF22_1, FBF22_2, FBF22_3) %>%\n    column_to_rownames(var = 'gene_id') \n\nkeep <- rowMeans(Brown_Fat) > 0.5 # e.g., 1 or 0.5\nBrownFat_filtered <- Brown_Fat[keep, ]\nBrownFat_filtered_log2RPKM <- log2(BrownFat_filtered + 1)\n\nsamples <- c(\n  \"MBF2_2\", \"MBF2_3\", \"MBF6_1\", \"MBF6_2\", \"MBF6_3\", \"MBF10_1\", \"MBF10_2\", \"MBF10_3\",\n  \"MBF14_1\", \"MBF14_2\", \"MBF14_3\", \"MBF18_1\", \"MBF18_2\", \"MBF18_3\", \"MBF22_1\", \"MBF22_2\", \"MBF22_3\",\n  \"FBF2_1\", \"FBF2_2\", \"FBF2_3\", \"FBF6_1\", \"FBF6_2\", \"FBF6_3\", \"FBF10_1\", \"FBF10_2\", \"FBF10_3\",\n  \"FBF14_1\", \"FBF14_2\", \"FBF14_3\", \"FBF18_1\", \"FBF18_2\", \"FBF18_3\", \"FBF22_1\", \"FBF22_2\", \"FBF22_3\"\n)\n\nmetadata <- data.frame(\n  Sample = samples,\n  Sex = ifelse(substr(samples, 1, 1) == \"M\", \"Male\", \"Female\"),\n  Time = as.integer(gsub(\"^.{3}(\\\\d+)_.*$\", \"\\\\1\", samples)),  # Extract digits after first 3 characters\n  stringsAsFactors = FALSE\n)\n\nmetadata_v2 <- data.frame(\n  Sample = samples,\n  Sex = ifelse(substr(samples, 1, 1) == \"M\", \"Male\", \"Female\"),\n  Time = as.integer(gsub(\"^.{3}(\\\\d+)_.*$\", \"\\\\1\", samples)),  # Extract digits after first 3 characters\n  stringsAsFactors = FALSE\n)\n\n\nmetadata_v2 <- metadata_v2 %>%\n  column_to_rownames(var = 'Sample')\n\n\nmetadata_v2$Time <- as.factor(metadata_v2$Time)\n\n\nPCA_Uncorrected <- t(BrownFat_filtered_log2RPKM)\npca_res_uncorrected <- prcomp(PCA_Uncorrected, scale. = TRUE)\n\nautoplot(pca_res_uncorrected, data = metadata, color = \"Time\", shape = 'Sex' )\n\ncor_matrix <- cor(BrownFat_filtered_log2RPKM, method = \"pearson\")  # or \"spearman\"\n\n# Visualize with pheatmap\npheatmap(cor_matrix,\n         main = \"Sample-to-Sample Correlation\",\n         annotation_col = metadata_v2,\n          annotation_row = metadata_v2)\n\n\n```\n\n```{r setup, include=FALSE}\n\n\n\nrownames(metadata) <- colnames(Brown_Fat) \n\nmetadata$Time <- as.factor(metadata$Time)\nmetadata$Replicate <- as.factor(metadata$Replicate)\n\n\nform <- ~  (1 | Sex ) + (1 | Time) + (1 | Replicate) + (1 | Sex:Time)\n\n\nvarPart <- fitExtractVarPartModel(BrownFat_filtered, form, metadata)\n\nvp <- sortCols(varPart)\n\nlibrary(\"limma\")\n\nplotVarPart(vp)\n\n\nexpression_data <- as.matrix(log2(BrownFat_filtered + 1))\n\nmod <- model.matrix(~ Sex + as.factor(Time), data = metadata)\nmod0 <- model.matrix(~1, data = metadata)\n\nn.sv <- num.sv(expression_data,mod, method = c(\"be\"), B = 20)\n\nsvobj = sva(expression_data,mod,mod0,n.sv= 3)\n\nexpression_corrected <- removeBatchEffect(expression_data, covariates = svobj$sv, design = mod)\n\ndat_t <- t(expression_corrected)\npca_res <- prcomp(dat_t, scale. = TRUE)\n\nautoplot(pca_res, data = metadata, color = \"Time\", shape = 'Sex' )\n\ncor_matrix <- cor(expression_corrected, method = \"pearson\")  # or \"spearman\"\n\n# Visualize with pheatmap\npheatmap(cor_matrix,\n         main = \"Sample-to-Sample Correlation\",\n         annotation_col = metadata_v2,\n         annotation_row = metadata_v2)\n\n```",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612577/",
    "title": "Primer removal in amplicon sequencing",
    "question": "Hello,\nI am currently working with 16s and ITS amplicon sequencing data. I am processing this following DADA2 pipeline. I got the primer sequence for 16S as:\n16S Amplicon PCR Forward Primer = 5' TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGCCTACGGGNGGCWGCAG\n16S Amplicon PCR Reverse Primer = 5' GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGACTACHVGGGTATCTAATCC\nI think these have the overhang portion with the gene specific part. When I search for primers should I input the whole sequence or just the gene specific part? Because for full sequence search I get no primer hit, but for gene specific part I do get primer hits. Thanks!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612574/",
    "title": "GATK BQSR error",
    "question": "Hi everyone,\nI'm working with the GATK pipeline (v4.5.0.0) for variant calling on human RNA-seq data aligned to GRCh38. I'm currently stuck at the BQSR (Base Quality Score Recalibration) step due to what seems to be a mismatch between my BAM file and the reference genome FASTA file.\nMy BAM file (Control-DMSO-24h-1.marked.bam) was generated using Homo_sapiens.GRCh38.dna.primary_assembly.fa (from Ensembl). These chromosome names are like 1, 2, MT, X, etc. (no \"chr\" prefix).\nFor BQSR, I'm using GATK's recommended Homo_sapiens_assembly38.fasta as the reference, which does have chr prefixes (chr1, chrM, etc.).\nI also have known sites VCF files (dbSNP and Mills indels) provided by GATK that match the chr-prefixed reference.\nWhen I run the GATK BQSR command, I get an error like:\ngatk BaseRecalibrator \\ -I /arf/scratch/semugur/markduplicates_all/Control-DMSO-24h-1.marked.bam \\ -R /arf/home/semugur/Gatk/prostat/prostat_split/ref/Homo_sapiens_assembly38.fasta \\ --known-sites /arf/home/semugur/Gatk/prostat/prostat_split/ref/Homo_sapiens_assembly38.dbsnp138.vcf \\ --known-sites /arf/home/semugur/Gatk/prostat/prostat_split/ref/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \\ -O /arf/scratch/semugur/bqsr_prostat/Control-DMSO-24h-1_recal.table Using GATK jar /arf/home/semugur/miniconda3/envs/gatk_env/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar Running: java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /arf/home/semugur/miniconda3/envs/gatk_env/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar BaseRecalibrator -I /arf/scratch/semugur/markduplicates_all/Control-DMSO-24h-1.marked.bam -R /arf/home/semugur/Gatk/prostat/prostat_split/ref/Homo_sapiens_assembly38.fasta --known-sites /arf/home/semugur/Gatk/prostat/prostat_split/ref/Homo_sapiens_assembly38.dbsnp138.vcf --known-sites /arf/home/semugur/Gatk/prostat/prostat_split/ref/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O /arf/scratch/semugur/bqsr_prostat/Control-DMSO-24h-1_recal.table 23:36:25.769 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/arf/home/semugur/miniconda3/envs/gatk_env/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 23:36:25.928 INFO BaseRecalibrator - ------------------------------------------------------------ 23:36:25.929 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.3.0.0 23:36:25.929 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/ 23:36:25.929 INFO BaseRecalibrator - Executing as semugur@arf-ui1 on Linux v5.14.0-284.30.1.el9_2.x86_64 amd64 23:36:25.929 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21 23:36:25.929 INFO BaseRecalibrator - Start Date/Time: May 29, 2025 at 11:36:25 PM TRT 23:36:25.929 INFO BaseRecalibrator - ------------------------------------------------------------ 23:36:25.929 INFO BaseRecalibrator - ------------------------------------------------------------ 23:36:25.930 INFO BaseRecalibrator - HTSJDK Version: 3.0.1 23:36:25.930 INFO BaseRecalibrator - Picard Version: 2.27.5 23:36:25.930 INFO BaseRecalibrator - Built for Spark Version: 2.4.5 23:36:25.930 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2 23:36:25.930 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false 23:36:25.930 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true 23:36:25.930 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 23:36:25.930 INFO BaseRecalibrator - Deflater: IntelDeflater 23:36:25.930 INFO BaseRecalibrator - Inflater: IntelInflater 23:36:25.930 INFO BaseRecalibrator - GCS max retries/reopens: 20 23:36:25.930 INFO BaseRecalibrator - Requester pays: disabled 23:36:25.930 INFO BaseRecalibrator - Initializing engine 23:36:27.819 INFO FeatureManager - Using codec VCFCodec to read file file:///arf/home/semugur/Gatk/prostat/prostat_split/ref/Homo_sapiens_assembly38.dbsnp138.vcf 23:36:27.964 INFO FeatureManager - Using codec VCFCodec to read file file:///arf/home/semugur/Gatk/prostat/prostat_split/ref/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz 23:36:28.093 INFO BaseRecalibrator - Shutting down engine [May 29, 2025 at 11:36:28 PM TRT] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.04 minutes. Runtime.totalMemory()=2944401408 *********************************************************************** A USER ERROR has occurred: Input files reference and reads have incompatible contigs: No overlapping contigs found. reference contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrM, chr1_KI270706v1_random, chr1_KI270707v1_random, chr1_KI270708v1_random, chr1_KI270709v1_random, chr1_KI270710v1_random, chr1_KI270711v1_random,\nI checked my .fai and BAM headers:\n.fai from the reference has chr1, chr2, chrM, etc.\nBAM header has @SQ SN:1, @SQ SN:MT, etc.\nhow ı can solve this problem or or should I skip to the next haplotypecaller step?",
    "answers": [
      "This question has been asked many times: GATK Input files reference and features have incompatible contigs: No overlapping contigs found. , GATK: Input files reads and reference have incompatible contigs: No overlapping contigs found , GATK BaseRecalibrator error: input files reference and features have incompatible contigs\nyou can remap your bam files to the gatk genome or you can try to rename the contigs in the different resources of the GATK bundle (fasta files, VCF files (Replacing the Chr names and position notions in vcf) )"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612567/",
    "title": "Batch correction without biological covariates",
    "question": "In general, is it ever acceptable to use Combat-seq (or any other batch correction method) without providing any biological covariates?\nCombat-seq will run fine if you don't provide any biological covariates, but wouldn't this prevent the model from being able to distinguish between biological variation and batch variation?",
    "answers": [
      "Well, hopefully your batch variation has nothing to do with your biological variation.\nIf they overlap, like all your controls are one batch, and all your treateds are a second batch, there is no way to batch correct that."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612554/",
    "title": "RagTag results",
    "question": "Why is there a fixed gap size of 100bp in between the contigs using the RagTag Scaffold option?",
    "answers": [
      "That's because RagTag doesn't know the exact gap length. Many scaffolders introduce gaps with a fixed length for the same reason."
    ]
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9609844/",
    "title": "Impact of the number of PCs on the clustering in scRNA seq",
    "question": "Hello all,\nI am working with a scRNA-seq dataset. I apply a PCA and select two different number of PCs (10 and 20). Then I apply a Louvain clustering on the reduced space with a fixed resolution (0.3) and I compare the two clusterings. I get more clusters when I selected 10 PCs than when I selected 20 PCs.\nI wonder why is that and I would therefore appreciate any hint!\nThank you very much :)\n(I join the scree plot showing the percentage of variance explained by each PC)\nAlong with the two UMAP showing the clustering.",
    "answers": [
      "As much as it might feel like an unsatisfactory answer, this totally depends on your downstream needs and what meaningful biology you can attach to the clustering. Some of these clusters will be based on technical variation, some will be QC-associated, some will associate with cell cycle while others will be driven by some effect based on a set of differential genes. Find out what markers are driving the clusters and you'll get a much better understanding of the clustering 'success' yourself.\nWhen gauging the success of graphical clustering the first question to ask is how meaningful are the clusters and you do that by looking at the marker genes for that cluster and see if you can make sense of it.\nFor your larger UMAP (bottom right), for example, I'd be quite interested in finding out the set of genes that would resolve cluster 2 (green) and cluster 8 (blue). I would equally be interested in understanding the stripe running down the middle of cluster 1 (orange), cluster 0 (dark blue) and cluster 5 (brown).\nSelecting the optimal number of PCs is not a simple question and typically the most practical solution is to iterate a few times and see if you get better resolution with a larger input set.\nSince you're already on python, you can look into tools like cNMF which allow you to optimise feature selection more readily than standard graphical clustering approaches. However, I would storngly recommend exploring your data a bit more before moving on with extra technical tools. Once you've convinced yourself you understand what is going on, you'll be in a much better position to judge these kinds of things yourself."
    ]
  },
  {
    "url": "https://www.biostars.org/p/482158/",
    "title": "ATAC-seq +4 -5 shift",
    "question": "Dear all,\nI saw having the mapped reads have +4 and -5 shift in ATAC-seq is a common practice.\nSome place says \"reads should be shifted + 4 bp and − 5 bp for positive and negative strand respectively, to account for the 9-bp duplication created by DNA repair of the nick by Tn5 transposase and achieve base-pair resolution of TF footprint and motif-related analyses\"\nSome place says:\" When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site.\"\nI'm a little bit confused. Are shifting mainly to center the peak or avoid the duplication?\nDoes anyone have a good illustration on this? What will happen to the peak calls if this step is skipped?\nThank you!",
    "answers": [
      "I illustrated the molecular biology here: http://guertinlab.org/wp-content/uploads/2021/01/Tn5_illumina_adapters_mjg_2.pdf\nwe skip this step for peak calling--it really only matters for looking at the data at single nucleotide resolution composite profiles.",
      "The shifting isn't for any real purpose unless you want to plot the exact cut location (e.g., when searching for motifs), it simply harkens back to one of the first ATAC-seq papers where they performed this adjustment to account for the 9-base single-stranded over-hang on each end of the fragment. Papers since have simply followed suite. A vastly more sensible strategy would be to use the 9 bases on each end of the fragment, since these are bases that are necessarily open.",
      "Turns out +4/-4 is the correct shift. You can see that when you make a bigwig separately from the + and - strands, then the reads align with each other for +4/-4 and it's off by one for +4/-5. The main idea is that transposition events should be mapped to the same base pair regardless of strand."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612994/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612991/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612988/",
    "title": "ATAC seq: from peaks to differential analysis ??",
    "question": "Hi,\nI’m fairly new to ATAC-seq and have successfully run MACS2 separately for each of my samples. I now have individual *.narrowPeak files as output.\nMy experimental design looks like this:\nSample_ID     Cell_type     Condition     Donor\n\nSample_1      T_cells       Tumor         Donor_1\nSample_2      T_cells       Normal        Donor_1\nSample_3      T_cells       Tumor         Donor_2\nSample_4      T_cells       Normal        Donor_2\n...\nSample_11     Dendritics    Tumor         Donor_10\nSample_12     Dendritics    Normal        Donor_10\nSample_13     Dendritics    Tumor         Donor_11\nSample_14     Dendritics    Normal        Donor_11\nAs you can see, I have two cell types (T_cells and Dendritics), and for each donor, I have paired Tumor and Normal samples.\nMy goal is to perform a differential accessibility analysis (Tumor vs Normal), accounting for both Donor and Cell_type. I’m also interested in comparing Tumor (T_cells) vs Tumor (Dendritics).\nI heard that it is possible to use DESeq2 for ATAC-seq data, so my design will look like this: https://bioconductor.org/packages/3.21/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#group-specific-condition-effects-individuals-nested-within-groups\nI have a few questions:\n1) Can I use the exact same code from the vignette, as typically done for RNA-seq data? Or are there any parameters or steps specific to ATAC-seq that I should consider?\n2) I’m struggling with how to convert my individual *.narrowPeak files into a count matrix. Do you have any recommendations or tools to help with this step?\n3) Are there alternative methods to DESeq2 that would be better suited for this kind of analysis? I guess limma should work the same no ?\nThank you in advance for your help !",
    "answers": [
      "2:\n# make saf file:\nawk 'OFS=\"\\t\" {print $1\":\"$2+1\"-\"$3, $1, $2+1, $3, \"+\"}' ${sample}_peaks.narrowPeak > featureCounts_peaks.saf\n\n$featureCounts -a featureCounts_peaks.saf \\\n    -F SAF \\\n    --read2pos 5 \\\n    -p \\\n    -o ${peak_all_dir}${sample}_countMatrix.txt ${rmBL_dir}*.bam\n${rmBL_dir}*.bam: all bam files for generating narrowPeaks\n3: DEseq2 is oke. design matrix and param depends on research question, not on the ATAC-seq or RNA-seq as they are all readcount type",
      "I also recommend converting the peak to SAF file as QX shows, although I just count fragments centered on cut site. There's little difference either way though.\nFor more guidance on creating a count matrix, you do need to create a common set of peaks first. Two main ways to do this, either take the intersection of peaks present in all samples or the union.\nI use bedtools, e.g.\n# N just represents number of total samples.\nbedtools mutliinter -i ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | awk '$4 == N' | bedtools sort -i - | bedtools merge -i - > All_Samples_Intersection.bed\nOr,\ncat  ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | bedtools sort -i - | bedtools merge -i - > All_Samples_Union.bed\nThen you can convert that bed into SAF and use featurecounts to count reads from each sample and treat similar to RNA-seq.\nOne thing I would suggest, I usually use a low count filter before moving forward with DE analysis. With ATAC-seq, there's usually more noise (I use relatively lax peak calling), so I tend to use a higher count threshold."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612987/",
    "title": "CUT&RUN analysis pipeline",
    "question": "Hello, My lab has generated data using the CUT&RUN protocol. Unfortunately, Its single-end data. I tried the CUT&RUN tools and the henipipe tool. Both are for paired end data.\nAny suggestion how I can analyse these data ? Any idea how to set the parameters in case I use the classical Trimmomatic and bowtie2 ? Any other pipelines to try ?\nThanks and Regards. (Loosing my mind)",
    "answers": [
      "You don't need to do anything too special. Trim adapters with your favorite software, align to genome using BWA-MEM or bowtie2, and call peaks using MACS2.\nMost of the fancy stuff with CUT&RUN is when you have paired end data. With paired end data you get information on insert/fragment size, which can be used to infer nucleosomal and subnucleosomal fragment, which in turn lets you guess whether afragment was from nucleosome protection or TF binding."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612985/",
    "title": "Cut&Run replicates handling",
    "question": "Is there a CUT&RUN peakcaller that supports replicates? If not, what is your recommended approach for handling replicates?\nIt appears that all available peak callers, such as SEACR, are designed to call peaks from single pull-down experiments.\nOur Experiment: We have two conditions, Untreated (UTR) and Treatment (TREAT), each with two biological replicates. Each condition also has respective INPUT data for normalization.\nWe have completed:\nQuality Control (QC)\nAlignment\nDuplicate marking/removal\nSpike-in Calibration\nNext, we would like to proceed to peak calling and differential analysis. We would appreciate your recommendations on suitable peak callers and the best practices for handling replicates.",
    "answers": [
      "You can use nf-core cutandrun pipeline, it supports replicates in the step of consensus peaks calculations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612984/",
    "title": "Unable to extract count matrices from multi-layered Seurat object",
    "question": "Hello, I have a merged R object with 18 samples\nall_combined <- merge(\n  sample_1MI1_so,\n  y = c(\n    sample_1MI3_so,\n    sample_2MI1_so,\n    sample_2MI3_so,\n    sample_3C_MI2_so,\n    sample_3MI1_so,\n    sample_3MI3_so,\n    sample_4D_MI2_so,\n    sample_4MI1_so,\n    sample_4MI3_so,\n    sample_5MI1_so,\n    sample_5MI3_so,\n    sample_6MI1_so,\n    sample_6MI3_so,\n    sample_7MI1_so,\n    sample_7MI3_so,\n    sample_8MI1_so,\n    sample_8MI3_so\n  ),\n  add.cell.ids = c(\n    \"sample_1MI1_so\",\n    \"sample_1MI3_so\",\n    \"sample_2MI1_so\",\n    \"sample_2MI3_so\",\n    \"sample_3C_MI2_so\",\n    \"sample_3MI1_so\",\n    \"sample_3MI3_so\",\n    \"sample_4D_MI2_so\",\n    \"sample_4MI1_so\",\n    \"sample_4MI3_so\",\n    \"sample_5MI1_so\",\n    \"sample_5MI3_so\",\n    \"sample_6MI1_so\",\n    \"sample_6MI3_so\",\n    \"sample_7MI1_so\",\n    \"sample_7MI3_so\",\n    \"sample_8MI1_so\",\n    \"sample_8MI3_so\"\n  ),\n  merge.data = TRUE\n)\nView object\nall_combined\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 55 layers present: counts.1, counts.2, counts.3, counts.4, counts.5, counts.6, counts.7, counts.8, counts.9, counts.10, counts.11, counts.12, counts.13, counts.14, counts.15, counts.16, counts.17, counts.18, data.1, scale.data.1, data.2, scale.data.2, data.3, scale.data.3, data.4, scale.data.4, data.5, scale.data.5, data.6, scale.data.6, data.7, scale.data.7, data.8, scale.data.8, data.9, scale.data.9, data.10, scale.data.10, data.11, scale.data.11, data.12, scale.data.12, data.13, scale.data.13, data.14, scale.data.14, data.15, scale.data.15, data.16, scale.data.16, data.17, scale.data.17, data.18, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nIn the all_combined object, I', trying to extract the counts matrices from it, but I'm not sure how to do this? Below is what I've tried\nall_combined_count_matrix <- LayerData(object = all_combined, assay = \"RNA\", layer = \"counts\")\nWarning: multiple layers are identified by counts.1 counts.2 counts.3 counts.4 counts.5 counts.6 counts.7 counts.8 counts.9 counts.10 counts.11 counts.12 counts.13 counts.14 counts.15 counts.16 counts.17 counts.18\n only the first layer is used\n\n# only 1st sample counts matrix is extracted....want to extract all 18 counts matrices...all_combined object contains 18 seurat objects merged into one\n\n`\nManually combine all 18 count matrices into one\n# List all layers that start with \"counts\"\nall_counts_layers <- Layers(all_combined[[\"RNA\"]])\nall_counts_layers <- counts_layers[grepl(\"^counts\", all_counts_layers)]\n\n# # view all_count_layers\n#  [1] \"counts.1\"  \"counts.2\"  \"counts.3\"  \"counts.4\"  \"counts.5\"  \"counts.6\"  \"counts.7\"  \"counts.8\"  \"counts.9\"  \"counts.10\" \"counts.11\" \"counts.12\" \"counts.13\" \"counts.14\"\n# [15] \"counts.15\" \"counts.16\" \"counts.17\" \"counts.18\"\n\n\n\n# Extract each layer and combine\nall_count_matrices <- lapply(all_counts_layers, function(layer) {\n  LayerData(all_combined, assay = \"RNA\", layer = layer)\n})\n\n\n# View all_count_matrices (prints out all 18 count matrices. NOTE: each matrices has different number of rows/genes and different number of columns. However, total rows adds up to 23,447 genes and total columns add to 135,704 cells)\n# all_count_matrices\n\n\n\n# Combine into one gene x cell matrix\nall_combined_counts <- do.call(cbind, all_count_matrices)\n\n# Error in cbind.Matrix(x, y, deparse.level = 0L) : \n#   number of rows of matrices must match\nAny advice on how to do this effectively would be greatly appreciated. I'm using Seurat v5.3.0",
    "answers": [
      "Alright, I fixed the issue by joining the counts layers in all_combined. I saved it to a new seurat object. Hopefully this helps someone else!\nall_combined_join_layers <- JoinLayers(all_combined)\nThis joins the 18 counts layers into one layer (and also the 18 normalized data layers)\nall_combined_join_layers\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 21 layers present: data, counts, scale.data.1, scale.data.2, scale.data.3, scale.data.4, scale.data.5, scale.data.6, scale.data.7, scale.data.8, scale.data.9, scale.data.10, scale.data.11, scale.data.12, scale.data.13, scale.data.14, scale.data.15, scale.data.16, scale.data.17, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nI then extracted the counts matrix from all_combined_join_layers\nall_combined_count_matrix_2 <- LayerData(object = all_combined_join_layers, assay = \"RNA\", layer = \"counts\")\n\nall_combined_count_matrix_2\n\n23477 x 135704 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n\nENSMMUG00000023296 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692             . . . . . . . . . 1 . . . . . 1 2 . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612983/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612982/",
    "title": "usage of ChromHMM and Segway",
    "question": "I want to use ChromHMM and Segway to identify chromatin states or genomic segmentation based on existing models. Where are these models stored and what form these models are, and how can I use them for direct calculations? ChromHMM(https://ernstlab.github.io/ChromHMM/); Segway(https://segway.hoffmanlab.org/)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612980/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612979/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612978/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612977/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612976/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612975/",
    "title": "phage sequence no alignment to genome",
    "question": "I am currently working with a bacterial strain (let’s call it Strain A) for which I have a complete genome assembly (StrainA_genome.fasta). From this strain, I obtained two sets of phage-related sequences using different experimental approaches:\n1. An assembled phage genome from lab experiments: StrainA_phage_gapfilled.fasta\n2. A pair of short-read sequencing files from another phage preparation: StrainA_phage_R1.fasta and StrainA_phage_R2.fasta\nSeparately, I predicted prophage regions on StrainA_genome.fasta using Cenote-Taker 3 and PhageBoost, and collected all the predicted prophage sequences into a file: StrainA_predicted_phages.fasta.\nMy goal is to verify whether the experimentally derived phage sequences correspond to any of the predicted prophage regions, i.e., to confirm that the experimentally recovered sequences truly originate from prophages within Strain A.\nWhat I tried\nI attempted various alignments, including:\nI used bwa mem to align both the raw phage reads and the gap-filled phage assembly against the predicted prophage regions, using commands like:\n # Align short reads to predicted phages \n  bwa mem -t 4 StrainA _predicted_phages.fasta StrainA_phage_R1.fasta StrainA_phage_R2.fasta > phage_reads_vs_predicted.sam\n\n # Align assembled phage genome to predicted phages\n  bwa mem -t 4 StrainA_predicted_phages.fasta StrainA_phage_gapfilled.fasta > phage_assembly_vs_predicted.sam\nnone of the reads or assemblies mapped to any of the predicted prophage sequences.\nI then aligned the same experimental phage sequences (both reads and the assembled genome) directly to the full bacterial genome (StrainA_genome.fasta), expecting at least partial matches in the prophage regions. However, there were still no alignments.\n  # Also tried aligning both to the host genome:\n  bwa mem -t 4 StrainA_genome.fasta StrainA_phage_gapfilled.fasta > phage_vs_host.sam\nThis was very unexpected — if the prophages are indeed part of the genome, I would expect at least some reads to map to those regions.\nI also tried the same comparisons using Minimap2/bowtie2/blast, even tuned parameters to allow more mismatches and gaps. Unfortunately, there were no alignments..\nTo verify the identity of the experimentally obtained phage sequences, I submitted StrainA_phage_gapfilled.fasta to NCBI ORF Finder, extracted several of the longer predicted protein sequences, and then ran BLASTp against the virus (taxid:10239) database. These searches returned high-confidence hits to known phage proteins, supporting the idea that the experimental sequences are indeed phage-derived.\nAdditionally, I extracted CRISPR spacer sequences from Strain A using CRISPRCasTyper, and tried to align them (using BLAST) to the gap-filled experimental phage genome. Only one spacer hit was found, with a relatively low score (e-value ~1e-2).\nI am now wondering:\n1. Could this complete lack of alignment be caused by my alignment strategy?\nAre there better tools or methods specifically suited to phage–prophage or phage–host genome comparisons?\n2. Could experimental artifacts explain this?\nI do not perform the experimental work myself. Could the phage DNA isolation,amplification, or assembly methods introduce chimerism or unrelated sequences that mislead mapping?\n3. Or is it genuinely common to see such divergence between experimentally isolated phages and prophage predictions from the same strain genome?\nAny thoughts or suggestions on improving the alignment strategy or interpreting these results would be greatly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612553/",
    "title": "Tools that output positions of matching kmers",
    "question": "Hello community!\nI am trying to find a tool that takes sequencing reads as input and a genome. The tools breaks the reads into kmers and outputs the positions of the kmers within the genome for each read. As this is a standard step in almost every aligner I'm hoping I can find some tool that does it without having to write it by myself.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612520/",
    "title": "mitochondrial assamply use NOVOplasty",
    "question": "Hello there,\nI have assembled the mitochondrial genome for the species \"Gazelle using NOVO plasty .\" I have a reference sequence for this species, as well as the cytb gene sequence. However, I've noticed something unusual: the reference and query assambly sequence do not start and end at the same positions. Can someone explain why this might be the case?",
    "answers": [
      "You might want to consider trying to rotate the circular genomes to a common start position. A new-ish tool is available for this purpose: \"Rotate: A command-line program to rotate circular DNA sequences to start at a given position or string \" https://wellcomeopenresearch.org/articles/8-401"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612549/",
    "title": "Still bad phred score after fastp",
    "question": "I have performed fastp on my files, trimmed the adaptor sequences, and raw data quality has become better. But in all of my R1 reads I am observing a certain pattern in per base sequence quality in which box-and-whisker plot is going in red region at 3 base positions. In all of my R2 reads poly-G adaptor sequence is removed, but still per base sequence quality is very bad. At a lot of base positions the plot is going in red region. What should i do now, and how much should i trim it again?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612515/",
    "title": "Cytohubba problem and difference networks",
    "question": "Hi friends, i upload a file of nodes in cytoscape and use cytohubba plugin for analysis it, now i see difference rank network in every upload this file and it makes i can trust my data that get them from cytohubba, please help me for finding my problem",
    "answers": [
      "I'm not sure exactly what the problem you are seeing is. Cytohubba creates a new network after it does the analysis, but your old network is still there. Just go to the Network panel and click on it."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9611889/",
    "title": "Cytoscape KEGG plots reverting to original layout after changing style or column",
    "question": "I'm trying to use cytoscape to better plot kegg pathways. I've obtained xml files from pathview and imported via \"import network via file system\".\nProblem 1: Missing quite a lot of info especially arrows and certain symbols like small circle or rectangle. It also created many arrows that arent present from the pathview png output.\nProblem2: Probably a cytoscape issue, but whenever I change the style even just the font size, all the nodes reverts back to original position after painstakingly moving them one by one.\nIs there any workarounds for these problems?\nThanks in advance",
    "answers": [
      "I have no idea about problem 1. My best guess is that there are glyphs in the png that aren't supported by Cytoscape, but I'm not a KEGG expert. As to Problem 2, my guess is that the positions are defined by Bypass values that are passed in the node table. To see if that's the case, go to the Style panel and select \"More Properties...\". Add the X Location and Y Location properties. Then, select all of the nodes and see if there is anything in the Bypass (Byp.) column for the X Location and Y Location properties. If there is, click on it and select \"Remove Bypass\". Then you should be able to move things around without having it reset."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612541/",
    "title": "Mass-spectrometry proteomics",
    "question": "I have mass-spectrometry-based proteome data of 6 control and 3 treated sample. There are random number of valid LFQ intensity per protein in each group. For example for a random protein 2 samples in control group and 1 sample in treated group have valid values. There are sometime more or less. There are cases also that per a specific protein, only one random sample from each group have valid value. And I am looking for differentially expressed proteins between control and treated. I don’t want to loose any of data. Could you please tell me what statistical method should I use for my analysis? How to transform and impute the data?",
    "answers": [
      "One possibility in the limpa package, described here: https://www.biorxiv.org/content/10.1101/2025.04.28.651125v1\nIdeally, you would give peptide-level data to limpa. If all you have are LFQ values, then you could try this:\nlibrary(limpa)\ny.imputed <- dpcImpute(y)\ndesign <- model.matrix(~Treatment)\nfit <- dpcDE(y.imputed, design)\nfit <- eBayes(fit, robust=TRUE)\ntopTable(fit, coef=2)\nHere, y is a matrix or EList of normalized LFQ values on the log-scale, including NAs, and Treatment is a factor indicating which samples are treated and which are controls. y.imputed is an EList containing imputed values and also information about how reliable each imputed value is.\nThis approach will avoid losing any data and will, where possible, infer DE from the missing value pattern. A protein may come as DE even if it is entirely NA in one group, provided the LFQ values are sufficiently high in the other group.\nThe above code uses a default detection probability curve (DPC), but you should explore that by typing:\ndpcfit <- dpc(y)\nplotDPC(dpcfit)"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612530/",
    "title": "Biological meaning of metrics commonly used for QC in scRNA-seq?",
    "question": "Hi,\nIn single-cell RNA-seq it is widely used filtering by some metrics as a way of rough QC to remove dead cells:\nnumber of Counts\nnumber of Features\n% mitochondrial reads\n% ribosomal reads\n% unspliced reads\nAll these are used as proxy of empty droplets, doublets (excess of Counts and Features), dead cells, cells with broken membrane leaking out their cytoplasm, etc.\nHowever, most if not all of these features have been shown to also reflect actual biological differences between cell types. For example, in this paper they discuss (and later identify) how metabolically active cell types have with high % mitochondrial reads. There are cell-type-dependent differences in the number of counts and features.\nI've also found online some talk about cells with a secretory phenotype having higher % ribosomal genes. Although, [citation needed] for most of those claims, in figure 2 E&F of the ddqc method reanalysis shows B and T-cells having very high % ribosomal:\nAs for the % of unspliced reads, one can imagine many biological processes causing differences (e.g., multi-nucleation, changes in cell size). However, the standard practice is either to ignore it (high Malat1 expression in cells stressed by tissue dissociation), or to use intronic/Malat1 expression as a proxy of loss of nucleus/empty droplets made of ambient cytosol remains\nHigh Malat-1 expression in single cell data\nhttps://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-024-11015-5\nhttps://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-024-11015-5/tables/1\nOr the effect of low QC cells with no-nucleic reads:\nIMHO, even if these metrics end up being markers of low-quality cells, there is still value in identifying common characteristics of these cells. For example, in disease models and/or stressed tissues, where certain cell subtypes are enriched in low-quality cells but still show clear subtype-specific markers that make sense in the context of the tissue/disease.\nMy question is... is there any resource where this has been comprehensively studied? I'm looking for something that can be used as reference. It is a bit scary to include data in my analyses that a reviewer might deem too low quality and forces me to re-run the whole thing from step one during manuscript revision.\nAlso, do anyone know of good references about the relationship between secretory phenotypes and high % ribosomal reads? Or about the association between very low or very high intronic fraction with biological features?\nThanks in advance",
    "answers": [
      "Welcome to the arbitrary hellscape that is single cell QC.\nIn short, you've made most of the key observations and found several of the more notable publications around these topics already. Generally, it can be helpful to QC each cell type individually given the known variation in these metrics for certain cell types. I find ribosomal RNA % a less useful metric most of the time as the variable range of \"healthy\" cells definitely seems higher than mito %. I don't know about the supposed association between it and secretory cell types, however.\nLow MALAT1 expression has actually proven about the most robust metric in my experience, but it still varies between cell types. And you still have to make some sort of relatively arbitrary decision on cutoff, but the distribution at least appears more bimodal than the others which makes it feel less gross.\nThe intronic fraction association also varies widely by cell type, e.g. neurons tend to have a larger fraction than most glial cell types. This has obvious implications for analyses like velocity that propose to use that ratio is a meaningful way, but I've yet to see any actual insights yielded from RNA velocity that couldn't be made by just hard-ordering cells along a line drawn along cluster centroids. So take what you will from that.\nUltimately, I'd recommend taking a holistic view of the QC metrics for each cell type and erring on the side of stringency. Even if \"usable\", lower quality cells will tend to cluster separately and can muddy the waters of downstream analyses. I often do a conservative first-pass with arbitrary thresholds to remove the lion's share of obvious crap, and then annotate and such and do another round of filtering after taking a closer look at the QC distributions.\nThe (supposed) upside for you (and to the detriment of science as a whole) is that many reviewers' ability to assess scRNA analyses seems to extend to the depth of the introductory Seurat vignette at best, so the bar is very low."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612525/",
    "title": "Eukaryotic eDNA Metabarcoding Course (14–18 July, Online)",
    "question": "Dear all, We’re excited to announce that there are only a few seats left for our upcoming course: Eukaryotic Metabarcoding\nDates: 14–18 July 2025\nCourse website: https://www.physalia-courses.org/courses-workshops/course4/\nThis hands-on course is perfect for researchers and students interested in biodiversity, ecological monitoring, or environmental genomics. You’ll gain practical skills in analyzing real Illumina and Nanopore datasets, designing metabarcoding projects, and running complete pipelines—from raw reads to interpretable results.\nBy the end of the week, you’ll be able to:\n(1) understand the potential and capabilities of metabarcoding, using Illumina and Nanopore data,\n(2) run complete analyses of metabarcoding pipelines and obtain diversity inventories and ecologically interpretable data from raw next-generation sequence data\n(3) design your own metabarcoding projects, using bespoke primer sets and custom reference databases.\nShould you have any questions, please do not hesitate to contact us.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612417/",
    "title": "miRNA low mapping(Qiagen miRNA Library Prep)",
    "question": "So I have the raw data and the kit used to prepare this is the qiagen mirna library prep kit. It has, accoording to my understanding and everything i read online the following structure: [biological seqeuence]-[constant_region]-[umi]-[adapter]. I orignally used this:\nFor umi ectraction and adapter discard:\numi_tools extract --extract-method=regex --stdin=$read1 --bc-pattern=\".+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})(?P<discard_2>.*)\" --stdout umiE-${sample_id}.fastq.gz\nand in a later step i had the following\n fastp -i $umiE_read1 -o \"comptrim-${sample_id}.fastq.gz\" -A -Q -L --low_complexity_filter\nfollowed by:\ntrim_galore -q 28 --phred33 --length 16 --basename \"$sample_id\" --fastqc -o . \"$complexity_trimmed_read1\"\nthe percetage of total reads obtained are arounf 58% after everything is said and done, from what ive read this is tyoical for miRNA data since its mostly adapter.But when I align against miRbase(previously extracted hsa) i get low mapping, when checking with seqkit. Help is very greatly appreciated. The umi extraction is based off my understanding of the prep method and from what ive found online. what could be my issue?",
    "answers": [
      "This might be a mapping quality issue since many identical sequences are in mirBase.\nSuggestions\nalign vs the genome they were generated from. Human? Quantify using a miRNA gtf gff3 etc file later with featureCounts etc.\nset max number of mappings to very high in bowtie (1 or 2 by the way? ). Remove any mapping quality MQ filters"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612388/",
    "title": "STAR alignment not detecting some transcripts / custom reference",
    "question": "Hi all!\nI have a scRNA-seq dataset and I use STAR for the alignment.\nIn the experimental conditions, my colleague added a certain gene and I would like to detect its transcripts levels. Therefore I added this bacterial gene to my genome annotation (I am working with the zebrafish) as follows at the end of the gtf file:\n77      AddedGenes      exon    1       1422    .       +       0       gene_id \"dcm\"; gene_name \"dcm\"; transcript_id \"dcm\"; exon_id \"dcm\"; exon_number \"1\";\nMy final output is a count matrix, but here is the problem: I don't get to see the \"dcm\" gene name in the list of genes (while even genes with 0 expression are still reported in the count matrix).\nTo debug the source of the problem, I manually aligned my fastq files to a fasta file containing just the dcm gene using bwa. This reported matches.\nBut then I tried with STAR as this is the aligner I use in my workflow. Doing the following:\nSTAR --runMode genomeGenerate --genomeDir star_dcm_index --genomeFastaFiles dcm.fa --runThreadN 4 --genomeSAindexNbases 5\n\nSTAR --genomeDir star_dcm_index --readFilesIn myfile_R1.fastq.gz --readFilesCommand zcat --runThreadN 4 --outSAMtype SAM --outFileNamePrefix star_alignment/\n\nsamtools view star_alignment/Aligned.out.sam\nBut this does not report any match. So I think the problem of not detecting the dcm in the list of genes in the count matrix might come from here.\nHas anyone any idea about what is occurring? I guess someone experienced with aligners would understand the cause of the issue.\nThank you very much for your help and time:)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612518/",
    "title": "eHOMD database: Ref seq Aligned FASTA vs Full RefSeq?",
    "question": "I am working with 16S rRNA amplicon sequencing data (targeting the V3–V4) and would like to use the eHOMD database for taxonomic assignment. I noticed that eHOMD provides \"Ref seq Aligned FASTA File \" and full RefSeq Version of the database. Which version is more appropriate for 16S amplicon analysis?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612450/",
    "title": "pysam header and body",
    "question": "Hi there, I started to work with pysam to get a BAM file into a Jupyther notebook. I mostly followed what indicated in the manual for importing the file:\nfile_bam = pysam.AlignmentFile(\"<file>.bam\", \"rb\")\nand to then visualize the header with:\nprint(file_bam.header)\nThough, I wonder is there an easy command to print the body of the file as in samtools view with all associated info? I looked up a bit and found this pysam.view(\"-S\", \"<file>.bam\") but it seems quite slow and couldn't understand the meaning of the -S flag (which I could not find in the manual). Apologies, for the question but I was looking for something fast&snappy like samtools; thanks in advance!",
    "answers": [
      "Each AlignedSegment object has a to_string() method that converts a BAM record into the SAM line you see from samtools view so all you need to do is iterate over all the reads in the file and convert them to strings:\nfor read in file_bam.fetch(until_eof=True):\n    print(read.to_string())\nHere, until_eof=True specifies that you want to read every read in the file until the end of file, rather than just the reads aligned to a specific region. It is the only sort of fetch that works without an index, and it is the only fetch that will return unmapped reads.\nHowever, this is almost always the wrong thing to do, and is generally only used for debugging.\nIf you wish to operate on the values in the columns of the body of the BAM file, then use the methods and attributes provided by the AlignedSegment object, such as get_reference_name, reference_start, cigar_tuples, is_reverse or get_tag. See https://pysam.readthedocs.io/en/latest/api.html#pysam.AlignedSegment for all the options.\nIf you wish to output reads from your code, then create a stream:\nout_bam = pysam.AlignmentFile(\"out.bam\", \"w\", template=bam_file) # output to a file\nout_stream = pysam.AlignmentFile(\"-\", \"w\", template=bam_file) # output to standard out.\nand then write to it:\nout_bam.write(read)",
      "Also this option?\nfile_bam = pysam.AlignmentFile(\"<file>.bam\", \"rb\")\nfor aln in file_bam:\n    print(aln.to_string())\nHowever, printing the whole bam file is most likely going to flood your screen.\npysam.view(\"-S\", \"<file>.bam\") couldn't understand the meaning of the -S flag\nI think the arguments you pass to pysam.view(...) are the same you would pass to samtools view .... So looking at samtools view help: -S Ignored (input format is auto-detected)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612451/",
    "title": "Discrepency in assembly sizes",
    "question": "Hello,\nThe haploid genome size estimated from my diploid plant is about 480 Mb. This is consistent with the number from K-mer analysis using GenomeScope2. However, my assembly size, produced from HiFi PacBio reads is about 780 Mb.\nMy first guess was that my assembly contains uncollapsed haplotypes. I looked at the distribution of coverage of one of my bam files (Illumina reads mapped to the genome assembly). I observe one pick around 1.5 which would be a coverage of about 30X, what I would expect for my sample. If I had uncollapsed haplotypes, I would expect to see a peak around 15X as well but I don't. I wonder where this discrepancy could come from and what other tests I could do to check this?\nJust some numbers: The total length of scaffolds with coverage between 10X and 20X is only about 23 Mb. And total length of scaffolds with no reads mapped to them is only about 6 Mb. The genome is repeat-rich, using repeatmodeler and repeat masker, about 50% of the genome was masked as repeats.\nThank you!",
    "answers": [
      "This looks like contamination or symbionts. When I assemble plant genomes, I often see low-coverage contigs from fungi, bacteria, insects, or something else. However, in most cases, the total length of low-coverage contigs is smaller than what you have.\nMy preferred method to detect and remove sequences of contamination and symbionts currently is aligning all contigs by Megablast to NCBI nt and analyzing top 5 matches, see How can I remove contaminants from an assembled genome?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612500/",
    "title": "fasterq-dump fails before completing lookup",
    "question": "mannda1@LAPTOP-DAPQ7ISB:/mnt/e/Mannda/fastq.files$ fasterq-dump SRR23490337 --include-technical --split-files --progress\nlookup :|-------------------------- 52.06%mannda1@LAPTOP-DAPQ7ISB:/mnt/e/Mannda/fastq.files$ \n\" i personally suspect that it could be storage becease i have 350gb of storage available the sra file i have are aroung 8gb i have 6 of them, initially i downloded one sra file anf the lookup terminated around 74%, now that i downloaded 6 of the SRA file its terminating at 52%\"\nhelp pls",
    "answers": [
      "Looks like this is a 10x single cell dataset. You should download the BAM file linked from https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=SRR23490337&display=data-access\nThen use bamtofastq utility provided by 10x to regenerate the fastq files: https://github.com/10XGenomics/bamtofastq"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612488/",
    "title": "ATACSEQ normlization for bigwig files",
    "question": "Hello everyone! I’m working on an ATAC-seq project with four samples (no replicates), each sequenced to a different depth (e.g. 200 M, 300 M, 400 M paired-end reads, etc.). After marking duplicates on the BAMs, I’m generating BigWig tracks with deeptools bamCoverage using RPGC normalization (as recommended here: https://groups.google.com/g/deeptools/c/th96gaftAXQ). When I run computeMatrix and plot TSS enrichment (cluster of 5 genes), I still see fluctuations instead of a smooth curve that I haven’t been able to explain.\nCould someone advise on:\nWhether RPGC is the best normalization strategy when you have no replicates but varying library sizes.\nHow to calculate and apply the correct scale factors (e.g. using the --scaleFactor option) if RPGC alone isn’t sufficient.\nAny tips on achieving truly comparable BigWig tracks (and hence TSS plots) across samples would be hugely appreciated!\nThank you!",
    "answers": [
      "See for code and discussion ATAC-seq sample normalization"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612486/",
    "title": "finding gene sequence from WGS data",
    "question": "I have performed variant calling and annotation analysis from WGS data. Now I need to get sequences of few genes containing variants.\nHow can I get sequences of particular variant containing genes?",
    "answers": [
      "It's tricky to get full sequences out of a bam, so your best bet is to make a fixed consensus sequence using your original reference fasta and your vcf."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612487/",
    "title": "Question about Protein-protein-DNA complex prediction",
    "question": "Dear All,\nI am currently working on a project to optimize protein-protein-DNA interactions. The goal is to use AI/ML tools to facilitate protein design. Specifically, I aim to optimize the linker between two proteins to create a single fusion protein and would like to know if there are any tools capable of performing this task. The objective is to use these tools to predict the binding affinity of DNA and the fusion protein with different linkers, then select the top five designs for in vitro assays, such as EMSA, to evaluate protein-protein-DNA complex formation. If anyone has experience in this area and is willing to share or discuss, please feel free to reach out here or via my email: mickley413@gmail.com. Thank you for your attention!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9538520/",
    "title": "Supergnova stuck in tutorial",
    "question": "I am trying to use Supergnova (https://github.com/YiliangTracyZhang/SUPERGNOVA) to calculate local genetic correlations from multiple GWAS summary statistics. To start with, I'm trying to run the tutorial they provide, but the final command:\npython3 supergnova.py ./data/sumstats/ASD.txt.sumstats.gz ./data/sumstats/CP.txt.sumstats.gz \\\n--N1 46351 \\\n--N2 257828 \\\n--bfile data/bfiles/eur_chr@_SNPmaf5 \\\n--partition data/partition/eur_chr@.bed \\\n--out results.txt\ngets stuck without showing any error after it has been running for more than 24 hours. From what I have read, the expected process time is between 30 min and 1 hour, which clearly suggests that something is not working properly.\nI have created a conda environment with the packages required to use Supergnova (Python 3, numpy scipy, pandas, sklearn, bitarray) but since there are no previous steps and I am simply following all the steps in the tutorial, I don't know what might be going on. I would appreciate if someone could suggest me something I could try to do to solve it.\nThanks.",
    "answers": [
      "Thanks for all the suggestions. I created an issue but, unfortunately, was not answered.\nI managed to solve the issues I was having changing the versions of the Python packages SUPERGNOVA requires. I will let them noted here in case anyone needs them:\nNumpy 1.19.5 Scipy 1.5.4 pandas 0.25.3 bitarray 2.1.0 sklearn = scikit-learn 0.24.2"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612334/",
    "title": "RNAseq meta-analysis to identify “consistently expressed” genes",
    "question": "Hi all,\nI am performing an RNAseq meta-analysis, using multiple publicly available RNAseq datasets from NCBI (same species, different conditions).\nMy goal is to identify genes that are expressed - at least moderately - in all conditions.\nCurrent Approach:\nNormalisation: I've normalised the raw gene counts to Transcripts Per Million (TPM) to account for sequencing depth and gene length differences across samples.\nExpression Thresholding: For each sample, I calculated the lower quartile of TPM values. A gene is considered \"expressed\" in a sample if its TPM exceeds this threshold.\nConsistent Expression Criteria: Genes that are expressed (as defined above) in every sample across all datasets are classified as \"consistently expressed.\"\nKey Points:\nI'm not interested in differential expression analysis, as most of the datasets I'm using lack appropriate control conditions.\nI'm also not focusing on identifying “stably expressed” genes based on variance statistics – eg identification of housekeeping genes\nMost RNA-seq meta-analysis methods that I’ve read about so far, rely on differential expression or variance-based approaches, which don't align with my needs.\nThere seems to be a lack of standardised methods for identifying consistently expressed genes without differential analysis. OR maybe I am over complicating it??\nMy questions:\nCan anyone tell me if my current approach is appropriate/robust/publishable?\nAre there other established methods or best practices for identifying consistently expressed genes across multiple RNA-seq datasets without relying on differential expression or variance analysis?\nAny advice hugely appreciated, TIA",
    "answers": [
      "You may find the normalisation approach used in the Bgee project useful: https://bioconductor.org/packages/release/bioc/html/BgeeCall.html"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612475/",
    "title": "pddg+odgi+vg to perform pangenome analysis to cyanobacteria",
    "question": "I'm using a bioinformatics tool for the first time for an assignment, and I'm feeling completely lost. I want to perform a pangenome analysis using PGGB + ODGI + VG tools on five cyanobacteria genomes whose FASTA files I downloaded from NCBI. However, when I open my pangenome graph PDF, all I see is a small empty square. I've been struggling with various errors constantly (like my running step does not move even after an hour etc). Could you please help me? I'm open to any tutorials, articles, or any kind of guidance that could help me understand and complete this analysis.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612471/",
    "title": "Comparing sets of genes for similarity",
    "question": "I am running a series of NMF-based clustering and marker selection on clusters from a large scRNASeq experiment across many comparisons. I want to compare the cluster output from those comparisons by seeing how similar the market sets are in order to assign a coherent clustering across the entire group.\nEg. Cluster A in Group 1 has 80% shared markers with Cluster D in Group 2 and 73% shared markers with Cluster C in Group 3 etc.\nI'm currently using Jaccard similarity but I wonder if there are more elegant ways of doing this? Dummy example R code below:\nlibrary(ComplexHeatmap)\nlibrary(EnsDb.Hsapiens.v86)\nlibrary(cluster)\n\n## get some dummy gene lists\nens.genes <- genes(EnsDb.Hsapiens.v86)\nens.genes <- ens.genes[ens.genes$gene_biotype==\"protein_coding\"]\nens.genes <- ens.genes[1:50]\n\nset.seed(1)\nset.1 <- sample(ens.genes$symbol, 25, replace = FALSE)\nset.seed(2)\nset.2 <- sample(ens.genes$symbol, 25, replace = FALSE)\nset.seed(3)\nset.3 <- sample(ens.genes$symbol, 25, replace = FALSE)\nset.seed(4)\nset.4 <- sample(ens.genes$symbol, 25, replace = FALSE)\nset.seed(5)\nset.5 <- sample(ens.genes$symbol, 25, replace = FALSE)\n\nmarkers.list <- list(set.1, set.2, set.3, set.4, set.5)\nnames(markers.list) <- c(\"Set1\", \"Set2\", \"Set3\", \"Set4\", \"Set5\")\n\n## set up jaccard similarity\njaccard <- function(a, b) {\n  intersection <- length(intersect(a, b))\n  union <- length(a) + length(b) - intersection\n  return(intersection/union)\n}\n\njaccard.mat <- matrix(data = NA, \n                           nrow = length(markers.list), \n                           ncol = length(markers.list),\n                           dimnames = list(names(markers.list),\n                                           names(markers.list)))\n\nfor (i in rownames(jaccard.mat)){\n  for (j in colnames(jaccard.mat)){\n    jaccard.mat[i,j] <- jaccard(markers.list[[i]], markers.list[[j]])\n  }\n}\n\n## get the distance \njaccard.dist <- as.dist(1-jaccard.mat)\nsilhouette.res <- numeric()\n\n## parameter search for PAM\nfor (k in 2:4) {\n  pam.fit <- pam(jaccard.dist, k, diss = TRUE)\n  silhouette.res[k] <- mean(silhouette(pam.fit)[,\"sil_width\"])\n}\n\nsilhouette.res <- silhouette.res[-1]\nnames(silhouette.res) <- 2:4\n\nsilhouette.df <- as.data.frame(silhouette.res)\nsilhouette.df$k <- rownames(silhouette.df)\ncolnames(silhouette.df)[1] <- \"silhouette\"\nsilhouette.df$k <- factor(silhouette.df$k, levels = c(2:4))\n\nggplot(silhouette.df,\n       mapping = aes(x = k, y = silhouette, group = 1)) + \n  geom_line() +\n  geom_point() +\n  theme_bw() +\n  theme(panel.grid.major.x = element_line(linetype = \"dotted\"),\n        panel.grid.minor.y = element_line(linetype = \"dotted\"),\n        panel.grid.major.y = element_line(linetype = \"dotted\")) +\n  labs(x = \"k\", y = \"Silhouette\", title = \"PAM clustering silhouette score\")\n\n## run PAM\nk4.pam <- pam(jaccard.dist, k = 4, diss = TRUE, cluster.only = TRUE)\n\n## set up heatmap \nanno.df <- data.frame(\"PAM\" = k4.pam, row.names = colnames(jaccard.mat))\nanno.col <- list(\"PAM\" = pals::kelly(4))\nnames(anno.col$PAM) <- 1:4\n\n## using ComplexHeatmap::pheatmap \npheatmap(jaccard.mat, \n         border_color = NA,\n         cellwidth = 12, cellheight = 12,\n         fontsize_row = 10, fontsize_col = 10,\n         clustering_distance_rows = \"euclidean\", \n         clustering_distance_cols = \"euclidean\",\n         clustering_method = \"ward.D2\",\n         annotation_row = anno.df, \n         annotation_col = anno.df,\n         annotation_colors = anno.col)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612464/",
    "title": "Network Analysis in Systems Biology - 2 seats left",
    "question": "Dear all, there are only 2 seats left for the Physalia upcoming online course: Network Analysis in Systems Biology with R/Bioconductor\nDates: 2–5 June\nCourse website:https://www.physalia-courses.org/courses-workshops/network-in-systems-biology/\nThis hands-on course will introduce participants to the inference and analysis of biological networks from RNA-seq data using R and Bioconductor. Topics include gene co-expression and regulatory networks (GCNs and GRNs), network comparison, and integration with genetic markers for gene prioritization.\nThe course is targeted to researchers and students that would like to learn how to use R and Bioconductor to infer and analyze networks for systems biology projects.Attendees need to have a working knowledge of R (R syntax, commonly used functions, basic data structures such as data frames, vectors, matrices and their manipulation).\nFor the full list of our courses and workshops, please visit: https://www.physalia-courses.org/courses-workshops/",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612457/",
    "title": "Running Unicycler with a merged fastq file",
    "question": "I was wondering what the reason behind the --kmers parameter in the unicycler pipeline. ( I know it is used in the SPAdes assembly, but I want to better understand what it is needed for).\nThe reason for that is our inout fastq files. We have samples which were sequenced twice, the second time was done to increase the sequencing depth. Unfortunately the second run was done on a different machine and with a different read length.\nWhen I try to run unicycler on the merged fastq files, it fails when it calculating the --kmers automatically, trying to also use a length of 61nt. The first sequencing run produced fastq file of only 60nt length.\nDoes it make more sense to give the too a specific list of parameters e.g. --kmers 13,25,33,39,45,49,53,57 or is it better to work with the files separately. What does it means for the analysis, if I use kmers not as long as the reads can give me?\nthanks\nAssa\nP.S. I also asked it on the github repo, but I don't think it is still very active there.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612410/",
    "title": "Assemblers for short read bacterial isolates data",
    "question": "Hello all, I am looking for assemblers for shotgun short reads data from bacterial isolates (permafrost). There are many articles where assemblers use Nanopore and ILLUMINA data, but I only got ILLUMINA data. I tried spades and megahit, but using kraken taxomony showed that one isolate have a few dozen different species, although 68% no hints. This is really weird, sure it could be a contamination, but is there anything else I could try? Should I maybe do a binning? This is my first time working with this type of data. I would appreciate any ideas! Thanks. Alla",
    "answers": [
      "Both SPAdes and MEGAHIT do a good job with Illumina data. One potential problem could be if you have a coverage that is too deep say > 1000x. If you have deep coverage, digitally reducing it to 200-300x might produce an assembly that behaves better.\nIf there are different species in your assembly, binning should easily separate them. However, it won't work nearly as well for the strains that share very high identity."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612440/",
    "title": "Biopython get_terminals() doesn't get all terminals",
    "question": "Hello,\nI downloaded the bac120.tree from GTDB. It has 136646 terminals. When I run\ntree = Phylo.read(\"bac120.tree\", \"newick\")\ntips = tree.get_terminals()\nI get a list of size one, which is one of the 136646 terminals. I was hoping to get a list of all terminals.\nWhat am I doing wrong?\nThanks and best regards,",
    "answers": [
      "Sorry, my bad. The tree wasn't imported correctly. It works now."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612447/",
    "title": "Covariate Selection for DESeq2/ANCOM-BC: Alpha vs. Beta Diversity Findings",
    "question": "Hi, I'm trying to understand the best approach for covariate selection in differential abundance analysis (ANCOM-BC/DESeq2). My dataset involves clinical outcomes (Success/Failure), 3 timepoints, and a single treatment. Alpha diversity models (lm models) didn't flag any of my clinical covariates as significant. However, PERMANOVA on beta diversity showed many of these same covariates (e.g., Age, specific clinical signs) are significantly associated with overall community composition, alongside my main factors of interest (Outcomes and TimePoints). If a covariate significantly impacts beta diversity (overall community structure) but not alpha diversity, is it still important to include it in taxon-level differential abundance models (ANCOM-BC/DESeq2) to control for its influence?\nWhat's the rationale? Any insights or best practice recommendations would be appreciated!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612446/",
    "title": "NF core rna-seq pipeline",
    "question": "I have been trying to use the nf rna seq pipeline but everytime i use it i withstand such errors when i ran this command\n nf-core/rnaseq   -profile conda   --input samplesheet_mouse_rnaseq.csv   --outdir results   --gtf /data/sata_data/home/rajdeep/TrimGalore/STAR/gencode.vM37.basic.annotation.gtf   --fasta /data/sata_data/home/rajdeep/TrimGalore/STAR/GRCm39.genome.fa   --star_index false   --save_reference   -resume**\n\n\n> Blockquote\n\n**Execution cancelled -- Finishing pending tasks before exit\n-[nf-core/rnaseq] Pipeline completed with errors-\nWARN: Directive `process.shell` cannot contain new-line characters - offending value: [bash\nset -e # Exit if a tool returns a non-zero status/exit code\nset -u # Treat unset variables and parameters as an error\nset -o pipefail # Returns the status of the last command to exit with a non-zero status or zero if all successfully execute\n]\nERROR ~ Error executing process > 'NFCORE_RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (GRCm39.genome.fa)'\nCaused by:\n  Process `NFCORE_RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (GRCm39.genome.fa)` terminated with an error exit status (126)\nCommand executed:\n  samtools faidx GRCm39.genome.fa\n  cut -f 1,2 GRCm39.genome.fa.fai > GRCm39.genome.fa.sizes\n\n  cat <<-END_VERSIONS > versions.yml\n  \"NFCORE_RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES\":\n      getchromsizes: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')\n  END_VERSIONS\nCommand exit status:\n  126\nCommand output:\n  (empty)\nCommand error:\n  .command.run: line 301: .command.run: Permission denied\nWork dir:\n  /data/sata_data/home/rajdeep/NF/work/6b/4895e66914ebeee4d0803301ac0432\nTip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`\n -- Check '.nextflow.log' file for details\nERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting\n -- Check '.nextflow.log' file for details**\nHow will i overcome this error?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612444/",
    "title": "Need Access to RadGraph2 Dataset from PhysioNet",
    "question": "I am currently working on a research project related to medical knowledge graph extraction and I need access to the RadGraph2 dataset hosted on PhysioNet. I have already completed all the required steps on the PhysioNet platform and received credentialed access approval.\nHowever, I am stuck at the final requirement — completing the CITI Program training. Unfortunately, I am unable to pay for the course as an independent learner, and despite sending multiple emails to both PhysioNet and CITI support, I have not received any response so far.\nIs there any alternative way to access the dataset or complete the CITI training (possibly free for academic/research purposes)? Any help, advice, or shared experience would be highly appreciated.\nThank you in advance!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612437/",
    "title": "Both up- and down-regulated gene sets return the same GO pathway analysis results?",
    "question": "Hi,\nI'm performing Gene Ontology (GO) enrichment analysis on differentially expressed genes (DEGs) from ATAC-seq data, using R (clusterProfiler). After separating my DEGs (DEseq2) into up-regulated and down-regulated gene sets based on log2 fold change and adjusted p-value thresholds, I noticed that the GO pathway analysis for both sets returns similar or identical enriched pathways.\nHas anyone encountered this issue? What could be causing the same GO terms to appear in both up- and down-regulated gene sets? As output from DEseq2, the genes from the two list cannot be shared.",
    "answers": [
      "There is absolutely no reason why you cannot have an enrichment of both up and down regulated genes in the same pathway. Consider a pathway with 20 genes. 10 of those genes are up regulated and 10 downregulated. Consider also that you have 1000 significantly upregulated genes and 1000 significantly downregulated genes in total. 1000/20000 or 5% of all genes are DE up and 5% DE down. 50% of pathway genes are up, which is a 10 fold enrichment (50%/5%), and 50% of pathway genes are down, which is also a 10 fold enrichment (50%/5%).\nWe must be careful when interpreting the results of enrichment analyses. An enrichment of upregulated genes does not mean that activity from the pathway is upregulated. For example, it could be inhibitors of the pathway that are upregulated. In your analysis, its possible that pathway activators are upregulated and pathway inhibitors are down regulated. Or it could be the other way around. Most likely it is not as clean cut as that though - biology moves in mysterious ways, and most genes can't be cleanly divided into activators and repressors.\nRemember that enrichment analysis is a purely hypothesis generating technique. It tells you what subjects/functions you should pay more attention to. It doesn't tell you what is happening to them."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612438/",
    "title": "Discrepancies calculatin average coverage with Bedtools and Mosdepth",
    "question": "Hi,\nI would like to calculate the average coverage of specific genomic regions. However I get different results with mosdepth and bedtools:\nbedtools\nbedtools coverage -a u1.narrowPeak -b u1_bwa.bam -mean > MeanCoverageu1.bedgraph\n\nchrY 57216300 57217350 u1_1 4 4105 .       0 0 0 825 18933.2226562\nmosdepth\nmosdepth -t 8 -n --by u1.narrowPeak u1_mosdepth_output u1_bwa.bam\n\nchrY 57216300 57217350  u1_1 1 5015.89",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/448874/",
    "title": "Michigan imputation server error",
    "question": "I am trying to upload my vcf files on Michigan imputation server. I have a single vcf file containing genotype data. I used these commands:\nbgzip example.vcf\ntabix example.vcf.gz\ntabix -l example.vcf.gz | parellel -j 5 'tabix -h  example.vcf.gz {} > {}.vcf'\nIt generated separate chromosome.vcf files (1..22, X, Y, MT). Again I used bzip to obtain zip files for uploading. I tried to upload all 25 files (chromosome.vcf.gz files:1..22, X, Y, MT) all together to the server and got the error\nError : No valid chromosomes found!\nIf I am uploading single vcf.gz file of single chromosome, it is working perfectly fine.\nCan anyone figure out what is the issue here. I am not able to understand. How can I solve it. How can I upload my all vcf.gz files for all chromosomes together as it is really very annoying to upload single single file every time.\nAny help would be really appreciated.",
    "answers": [
      "Please take a look at the comments and answers in this previous thread, and try the solutions suggested: problem with chromosomes in michigan imputation server\nAlso note: https://imputationserver.readthedocs.io/en/latest/prepare-your-data/\nKevin",
      "The HRC panel does not have chromosome Y, and it does not have the mitochondrial genome. The entire job fails because one or both of these vcf files fail the pipeline. Exclude them."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612355/",
    "title": "Proposal on CNV (Copies Variants Numer) analysis tools",
    "question": "Hello everyone, hoping you all doing well. If anyone can recommend a tool for analyzing copy number variants on somatic data, please let me know.\nI'm working with CNVkit, but it doesn't provide very clear documentation. So, if anyone can recommend another tool(s), please let me know.\nThanks.",
    "answers": [
      "DeepSomatic, which is the equivalent of Deepvariat for somatic variant calling, could be a good alternative (Not tested though)."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612429/",
    "title": "Error with bigWigAverageOverBed",
    "question": "Hi,\nI'm trying to calculate average coverage from narrowPeak files, using bigWigAverageOverBed command. However, I get the following error:\nbigWigAverageOverBed coverage_u1.bw u1.narrowPeak average_u1.txt \n\ninvalid unsigned integer: \"type=narrowPeak\"\nI'm following a guideline from someone who has done this process before. However, I haven’t been able to find any resources addressing this specific error.\nAny insight or resources would be greatly appreciated",
    "answers": [
      "It chokes because there is a header line in the narrowPeak file that needs to be removed. The tool expects a BED file which is at minimum chr-start-end without any headers. Transform your narrowPeak file in that way. At simplest it is just skipping the first line sed \"1d\" < your.narrowPeak > your.bed-",
      "check the formats of your u1.narrowPeak and average_u1.txt they're most proably NOT bed file.\nshow us the output of\nhead u1.narrowPeak average_u1.txt | tr \"\\t\" \"%\""
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612416/",
    "title": "de novo chromosome level assembly",
    "question": "I have the contigs assembly of a non model species. the assembly itself is made de novo since there is no close reference genome for this genus. I have used RagTag before for chromosome level assembly but it requires reference chromosome assembly, which in this case, is from a very distant genus. What would be your suggestion, do I use that distant reference chromosome or run de novo chromosome assembly? and in the latter case, I found CSA (Chromosome-Scale Assembler), are there better suggestions in terms of robustness?",
    "answers": [
      "I would personally NOT use the distant one to do reference guided scaffolding, the risk that things have changed (chromosome wise) over such an evolutionary distance is just too high to rely on the distant reference.\nMy suggestion would thus be to go for denovo scaffolding (if feasible off course, == if you have the data for it). Depending on your available data there is likely several options so best to first fill us in on what data you might have before we can do tool suggestion."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612407/",
    "title": "How is PLINK obtaining p-values and coefficients for linear regression",
    "question": "I'm a beginner in PLINK, and I'm now learning about association analysis with quantitative traits. Are the p-values and coefficient estimates in the PLINK output generated from conducting multiple \"simple linear regression\" across all sites, or are they obtained from conducting one \"multiple linear regression\" which includes genotypic information from all sites?",
    "answers": [
      "From the top of https://www.cog-genomics.org/plink/2.0/assoc#glm :\n--glm is PLINK 2.0's primary association analysis command.\nFor quantitative phenotypes, --glm fits the linear model\ny = G\\beta_G + X\\beta_X + e\nfor every variant (one at a time)"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612412/",
    "title": "bcftools not filtering snps",
    "question": "Dear all,\nI have called variants through freebayes.\nNow I am filtering SNPs and Indels.\nOpening SNPs file I found other variants also.\nI used this command to filter snps:\nbcftools view -v snps -Ov -o snps_only.vcf variants.vcf.gz\nWhy bcftools not giving desired output please guide to get only snps.\nThank you",
    "answers": [
      "ah yes, you're right. I don't understand the reason bcftools treat this as \"SNP\". May be you could ask the question in https://github.com/samtools/htslib/issues .\nyou want:\n bcftools view -i 'strlen(ALT)==1 && strlen(REF)==1'   -Ov -o snps_only.vcf variants.vcf.gz"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612404/",
    "title": "There are a way to align phage reads in to bacterial genomes?",
    "question": "Hi guys. Can I use bwa, star or any alignment prog to try to align some phage reads/dna against bacterial genomes? I though It could be interesting way to search for foreign dna in bacteria. What your thoughts about it? Or it is more easy to use any tool out there? Thanks. Paulo\nPS- I ask because I use some chr dna from an Actinobacter in Blast searches against the virus database and I found a lot of phages sequences with high similarity to this bacterial genome.",
    "answers": [
      "Can I use bwa, star or any alignment prog to try to align some phage reads/dna against bacterial genomes?\nAligners are data agnostic so they can use data from any source. Any aligner would work for this purpose as long as you are comfortable with command line. bwa or bbmap from BBMap suite would be fine to use.\nI found a lot of phages sequences with high similarity to this bacterial genome.\nBacteriophages infect bacteria and can integrate into bacterial genomes. It is not surprising to find sequences with similarity to them in bacterial genomes. Phages are responsible for horizontal gene transfer and are drivers of evolution. e.g. https://pmc.ncbi.nlm.nih.gov/articles/PMC515249"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612374/",
    "title": "No index in the undetermined fastq file",
    "question": "The data was generated in MiSeq , demux was done in LRM but it didn't yeild any data assigned to the sample, but the other issue is in the undermined files we are not able to see in any index in the header of the undermined fastq files, which seems bit strange.\nAny suggestion how to troubleshoot ,since SAV looks all fine the index read have good Q30 scores as adding the information\nzcat Undetermined_S0_L001_R1_001.fastq.gz | head @M06594:100:000000000-DN78N:1:1101:16087:1338 1:N:0:0 TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTNTNNNNNTTTTTTTTTNNNNNNNNTTTTTTTTNNTTTTTTTTTTTT + 1111>1>10000AA//AA/>///////<<<</<//-------:-9-9-9-999-9--9-9-9-9-------9-9-999-9-9--9------9-----99--9---#:#####99--9----########--999---##9----9----99 @M06594:100:000000000-DN78N:1:1101:15529:1349 1:N:0:0 TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTCTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTNNNNNNNNTTTTTTTTTTTTTTTTTTTTTT + 11111>1>>>>0///AA/////>////<<//////--<<-----9-9--9---9----99----9-999--999-------99//999:-9-99-------999----;---9--------########-----;---99-----9---9- @M06594:100:000000000-DN78N:1:1101:15215:1371 1:N:0:0\nAny suggestion how to troubleshoot, what could be the issue?",
    "answers": [
      "we are not able to see in any index in the header of the undermined fastq files, which seems bit strange.\nIf phiX was spiked in then that is not indexed. So the undetermined reads can at least partly be phiX. Rest could be \"junk\" data as they all appear to be poly-T in example you show.",
      "Run bcl2fastq or bcl-convert yourself, and the indices might turn up."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612402/",
    "title": "A Practical Introduction to R with the Tidyverse - Online 2–4 June",
    "question": "Dear all,\nReady to start your R journey? Join us for our hands-on, beginner-friendly online course:“A Practical Introduction to R with the Tidyverse”2–4 June | 14:00–19:00 CET | Online\nCourse website: https://www.physalia-courses.org/courses-workshops/r-tidyverse/\nDesigned for newcomers, this course will walk you through data wrangling, visualization with ggplot2, and creating beautiful reports with Quarto—all in the powerful and user-friendly tidyverse ecosystem. Expect live coding, interactive exercises, and practical examples tailored to real-world scientific research.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612401/",
    "title": "TCGA Germline allelic fraction distribution",
    "question": "I doing germline variant calling on TCGA data, however and I started noticing something strange.\nAs a test I did the following: I downloaded one tumor/normal genome bam file pair. First I ran variant calling using Strelka (starting from the alignment file and using the various TCGA reference files) and noticed that the distribution of allelic fractions did not look right, it looked skewed (see right figure below, WGS example) and did not reflect the homozygocity or heterozygocity bimodal distribution that I would expect (homozygous would have one big peak at one, heterozygous a distribution centered around 0.5 - see left figure below). I thought I did something wrong and then converted the bam to fastq and ran the analysis from scratch but got the same thing. Below is a figure of the distribution that I would expect and what I have observed in other projects, and what I am seeing on a TCGA tumor/normal pair.\nCan you explain? Any advice appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612371/",
    "title": "Issue in circular heatmap, the rows value are getting interchanged.",
    "question": "I have a vst file for which I am generating a circular heatmap. The heatmap is being generated but in the image the values of \"A\" is being changed with \"E\".\nLike for example; some of my values in \"A\" are positive like fat.1, fat.2 etc. but in the heatmap they are depicted as negative.\nI am sharing my R script for which I have generated , but I am not sure why the column values are getting interchanged.\nR script:\n# Load required libraries\nlibrary(circlize)\nlibrary(ComplexHeatmap)\nlibrary(dendextend)\nlibrary(grid)\n\n# ------------------- Data Preprocessing -------------------\n#read the scaled data\nvst_scaled <- read.csv(\"ALL/L2FC1/VST_scaled_for_heatmap_updated.csv\", row.names = 1)\nmat_t <- as.matrix(vst_scaled)\n\n# ------------------- Clustering and Ordering -------------------\n# Cluster genes (rows) and reorder\ndend <- as.dendrogram(hclust(dist(mat_t)))\ngene_order <- order.dendrogram(dend)\nmat_t <- mat_t[gene_order, ]\n\n# Color mapping function\ncol_fun <- colorRamp2(c(-3, 0, 3), c(\"blue\", \"white\", \"red\"))\n\n# Prepare legends\n# Prepare legends with bold, larger fonts\nlgd_links <- Legend(\n  at = c(-3, -1.5, 0, 1.5, 3),\n  col_fun = col_fun,\n  title = \"z-score\",\n  direction = \"horizontal\",\n  title_position = \"topleft\",\n  labels_gp = gpar(fontsize = 16, fontface = \"bold\"),\n  title_gp = gpar(fontsize = 18, fontface = \"bold\")\n)\n\nlgd_annotation <- Legend(\n  labels = c(\"High\", \"Low\"),\n  title = \"Expression\",\n  legend_gp = gpar(fill = c(\"red\", \"blue\")),\n  direction = \"horizontal\",\n  title_position = \"topleft\",\n  labels_gp = gpar(fontsize = 16, fontface = \"bold\"),\n  title_gp = gpar(fontsize = 18, fontface = \"bold\")\n)\n\n# Add a custom sample legend for treatments\nlgd_samples <- Legend(\n  labels = c(\"A = Control\", \"B = NN-Extract\", \"C = NN@AuNPs\", \"D = Au@NPs\", \"E = Orlistat\"),\n  title = \"Treatments\",\n  direction = \"horizontal\",\n  title_position = \"topleft\",\n  legend_gp = gpar(fill = NA),\n  labels_gp = gpar(fontsize = 16, fontface = \"bold\"),\n  title_gp = gpar(fontsize = 18, fontface = \"bold\")\n)\n\n# Combine all legends vertically\nlgd_combined <- packLegend(lgd_links, lgd_annotation, lgd_samples, direction = \"vertical\")\n\n# Save to high-resolution PNG (600 DPI)\npng(\"Circos_Heatmap_VST_scaled.png\", width = 16, height = 10, units = \"in\", res = 600)\n\n# Set up layout: 75% plot (left), 25% legend (right)\npushViewport(viewport(layout = grid.layout(nrow = 1, ncol = 2, widths = unit(c(3, 1), \"null\"))))\n\n# Circos heatmap (left)\npushViewport(viewport(layout.pos.col = 1))\ncircos.clear()\ncircos.par(start.degree = 0, gap.after = 10)\n\ncircos.heatmap(\n  mat_t,\n  col = col_fun,\n  na.col = \"grey80\",\n  cluster = TRUE,\n  dend.side = \"none\",\n  rownames.side = \"outside\",\n  track.height = 0.5,\n  rownames.font = 2,\n  rownames.cex = 1.5,\n  rownames.col = \"black\"\n)\n\n# Add sample names on outer track\ncircos.track(track.index = 2, panel.fun = function(x, y) {\n  cn <- colnames(mat_t)\n  n <- length(cn)\n  if (CELL_META$sector.numeric.index == 1) {\n    circos.text(rep(CELL_META$cell.xlim[2], n) + convert_x(1, \"mm\"),\n                1:n - 0.5, cn,\n                cex = 1.3, adj = c(0.5, 1), facing = \"clockwise\", font = 2)\n  }\n}, bg.border = NA)\n\nupViewport()  # Exit heatmap\n\n# Legend (right)\npushViewport(viewport(layout.pos.col = 2))\ndraw(lgd_combined, just = \"left\")\nupViewport()\n\npopViewport()  # Exit main layout\ndev.off()\nCan anyone please help in figuring out the issue?",
    "answers": [
      "Open an issue here\nIn circos.heatmap vignette, on the first heatmap, you can guess the row R71 in the block \"a\". With C10/C9 as positive, C3/C4/C5/C6 as negative and C1/C2/C7/C8 around 0.\nOn section 6.3 on the circlize heatmap, R71 is on the right side of the plot, third to last.\nGuessing by the colors, one would expect C1/C2 next to the outside border and C9/C10 on the inside.\nBut in section 6.8 on the circlize heatmap, you can see that the labels are flipped as well."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612382/",
    "title": "How to preserve full FASTQ header through alignment and CRAM recovery",
    "question": "I'm working with a pipeline where I delete FASTQ files after alignment to save space, so it's important that I can recover them exactly from the resulting CRAM files if needed.\nThe issue: my current workflow loses part of the original FASTQ header, specifically this section:\n<read>:<is filtered>:<control number>:<sample number>\nHere’s an example of a typical FASTQ field 1 line:\n@EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG\nI align reads per lane like so:\nbwa mem \\\n  -K 100000000 \\\n  -Y \\\n  -R \"@RG\\\\tID:FC706VJ:2\\\\tSM:ATCACG\\\\tPL:ILLUMINA\\\\tPU:FC706VJ.2.ATCACG\" \\\n  {input.ref} \\\n  {input.fastq1} \\\n  {input.fastq2}\nAfter alignment and CRAM generation, I recover FASTQs using:\nsamtools sort -n \\\n  -@ 24 \\\n  --reference $ref \\\n  -T $tmp \\\n  -o $queryname_sorted_cram \\\n  $cram_path\n\n\nsamtools fastq -@ 4 -n \\\n  -1 >(gzip > $r1_fastq) \\\n    --reference $ref \\\n  -2 >(gzip > $r2_fastq) \\\n  -0 /dev/null -s /dev/null \\\n  $queryname_sorted_cram\nBut the recovered reads look like this:\n@EAS139:136:FC706VJ:2:2104:15343:197393\nThe trailing read info (1:Y:18:ATCACG) is missing from field 1. I believe this information isn't preserved in the same format in the alignment output.\nA workaround I'm considering is to collapse the two fields of the header before alignment using sed:\nzcat {input.fastq1} | sed 's/ /|/'  # turns the header into one field\nSo it becomes:\n@EAS139:136:FC706VJ:2:2104:15343:197393|1:Y:18:ATCACG\nThis seems hacky though and I am afraid of running that sed command across all my fastq files and not sure if this will break downstream tools",
    "answers": [
      ": samtools sort -n | samtools fastq should be replaced with samtools collate| samtools fastq . Furthermore, samtools fastq is able to gzip the output fastq .\nand\ngunzip -c ${fastq} | tr \" \" \"|\" \nand I don't know anyway to keep the original names but you can find the information about your fields https://en.wikipedia.org/wiki/FASTQ_format\n    1   the member of a pair, 1 or 2 (paired-end or mate-pair reads only)  => found in the SAM flag\n    Y   Y if the read is filtered (did not pass), N otherwise => found in the sam flag\n    18  0 when none of the control bits are on, otherwise it is an even number => in the sam flag\n    ATCACG  index sequence => can be set using the READ GROUPS"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612376/",
    "title": "error in the output of bedops",
    "question": "Hi,\nI've encountering issues with bedtools intersect. However, I think the problem reside in bedops which has been used to find common genomic regions across samples. However, it encountered an error.\nbedops -m c1_macs3 u2_macs3 c3_macs3 u4_macs3 > merged_peaks.bed \nbedtools intersect -a merged_peaks.bed -b u1_macs3 -wa > common_u1.bed\n\nError: unable to open file or unable to determine types for file merged_peaks.bed\n\n- Please ensure that your file is TAB delimited (e.g., cat -t FILE).\n- Also ensure that your file has integer chromosome coordinates in the expected columns (e.g., cols 2 and 3 for BED).\nI suspect the problem lies in the file obtained from BEDOPS. The header contains some incorrect entries, such as a genomic position labeled with \"hr1\":\ntrack   1   0\nype=narrowPeak  1   0\name=\"u2_macs3\"  1   0\name=\"u4_macs3\"  1   0\name=\"c1_macs3\"  1   0\name=\"c3_macs3\"  1   0\nescription=\"u2_macs3\"   1   0\nescription=\"u4_macs3\"   1   0\nescription=\"c1_macs3\"   1   0\nescription=\"c3_macs3\"   1   0\nextItemButton=on    1   0\nhr1 9950    10800\nAny insight in this topic would be greatly appreciated.",
    "answers": [
      "Can you show what is in the *macs* files?\nYou should also try adding --header flag to your bedops command since it appears to be mangled in the output you show (first character is missing in all lines).\nEdit: Based on the response from @Alex adding the --header option to the bedops command is the solution."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612366/",
    "title": "Bioinformatician - SAGC at SAHMRI Adelaide, South Australia",
    "question": "Join the South Australian Genomics Centre (SAGC) – Bioinformatician Opportunity- Full-Time, 2-Year Contract\nThe South Australian Genomics Centre (SAGC) is a national, multi-institutional facility supported by Bioplatforms Australia through the Australian Government’s National Collaborative Research Infrastructure Strategy (NCRIS) initiative. Based in Adelaide, we bring together a team of ~16 genomics and bioinformatics specialists working across agriculture, healthcare, ecology and more. We’re committed to delivering innovative, high-impact genomics solutions.\nAbout the Role\nWe’re looking for an entry-level postdoctoral researcher or experienced MSc-level bioinformatician to join our collaborative team. In this role, you'll:\nConduct end-to-end bioinformatics analyses on a wide range of projects\nDevelop reproducible, standardised workflows and pipelines\nCollaborate on research publications and grant applications\nEngage with diverse data from cutting-edge genomics technologies Build deep expertise in a specialist area of bioinformatics\nYou’ll thrive in a fast-paced, multi-project environment with the support and mentorship to grow professionally.\nKey Responsibilities\nProvide bioinformatics support across client projects\nBuild and maintain analysis pipelines using best practices\nDraft scientific reports and contribute to publications -Work closely with genomics staff and researchers from varied disciplines\nAbout You The successful candidate will possess the following key attributes:\nEssential:\nMSc, PhD or equivalent in bioinformatics, computer science, statistics, physics, or related field\nExperience with scripting languages (e.g., Python, R, bash, Perl)\nFamiliar with version control, containerisation (Docker/Singularity), and workflow systems (e.g., Nextflow)\nExcellent organisation, time management, and communication skills\nAble to manage multiple projects and work both independently and in teams\nPrior experience with HPC environments\nDesirable:\nExpertise in one or more of the following: RNA-seq, genome assembly, metagenomics, multi-omics, or data visualisation\nExperience in client-focused roles\nFamiliarity with cloud computing, Linux admin, or additional programming languages (e.g., C++, Java, Rust)\nBackground in biomedical, agricultural, or environmental research\nWhat We Offer\nCompetitive salary with salary packaging up to $15,900 through Maxxia\nFlexible work arrangements to support work life balance\nExposure to diverse research fields and technologies\nCareer development support, including training, publishing, and grant opportunities\nA collaborative, values-driven work environment\nA chance to contribute to real-world impact through genomics research\nOur Culture and Values Everything we do is underpinned by our core values and our institute is dedicated to grow a culture that pursues, enables and demands research excellence. We are proud of the work we do and work hard as a team to make a positive difference to the community. Our values of excellence, innovation, courage, integrity and teamwork are what help us achieve our goals. If these are also your values and goals, apply today.\nApply Now If you are passionate about advancing genomics research and thrive in a collaborative environment, we want to hear from you.\nFor a confidential discussion, please contact Sen Wang, SA Genomics Centre Manager at Sen.Wang@sahmri.com\nPlease view the vacancy on our website to view a copy of the position description - https://sahmri.org.au/careers-and-study/working-at-sahmri/vacancies\nTo apply, please use this link: https://aurecruitment.actionhrm.com/myrecruit/newApplicationWizard.htm?pc=PiFGl1rvUtmXihP8P_bQrQxx\nApplications close: Sunday 22nd June 2025. Early applications are encouraged as the role may close earlier if filled.\nAboriginal and Torres Strait Islander people are strongly encouraged to apply To learn more about salary packaging, visit the Maxxia website.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/220465/",
    "title": "ClusterProfiler : What is GeneRatio and BgRatio?",
    "question": "Question is in the title.\nGeneRatio is like M/N where M is the number of genes from your input list that match the GO term. But I don't see what is N ?\nBgRatio is like A/B where B is all genes in database but I'm not sure what A corresponds to ... Is it the number of genes specific in the database of this GO term ?\nTell me if I'm wrong. Thanks.",
    "answers": [
      "I will give an example to explain this that helped me understand it. I also was looking for the answer and Guangchuang link helped.\nLet is suppose I have a collection of genesets called : HALLMARK Now let is suppose there is a specific geneset there called: E2F_targets\nBgRatio, M/N.\nM = size of the geneset (eg size of the E2F_targets); (is the number of genes within that distribution that are annotated (either directly or indirectly) to the node of interest).\nN = size of all of the unique genes in the collection of genesets (example the HALLMARK collection); (is the total number of genes in the background distribution (universe)\nGeneRatio is k/n.\nk = size of the overlap of 'a vector of gene id' you input with the specific geneset (eg E2F_targets), only unique genes; (the number of genes within that list n, which are annotated to the node.\nn = size of the overlap of 'a vector of gene id' you input with all the members of the collection of genesets (eg the HALLMARK collection),only unique genes; is the size of the list of genes of interest",
      "see https://bioconductor.org/packages/release/bioc/vignettes/DOSE/inst/doc/enrichmentAnalysis.html#over-representation-analysis\nCorresponding to the formula, geneRatio is k/n.",
      "Or perhaps in simpler terms GeneRatio = genes of interest in the gene set / total genes of interest. Most often I use it on lists of differentially expressed genes and so GeneRatio is also the fraction of differentially expressed genes found in the gene set.\nI have struggled to find the right words to explain this to others, so I hope this helps!",
      "My first post. I struggled with this and I hope my post can help others. LLM/ChatGPT responses were incorrect. I manually checked the articles published and the code in ClusterProfiler to be sure. Please correct me if I'm mistaken.\nN is the total number of genes in the background distribution/universe. M is the number of genes within that distribution that are annotated (either directly or indirectly) to the gene set of interest. BgRatio = M/N.\nFor example, in the hallmark collection for mouse, there are 50 gene sets. The Hallmark_angiogenesis gene set has 36 genes. In the hallmark collection (accessed in 2025), there are 4393 unique genes. So for hallmark_angiogenesis, BgRatio = M/N = 36/4393\nThe next 2 terms, n and, k requires user input and is dependent on the specific experiment data, e.g. after conducting DEG (differential expressed genes between 2 experimental conditions)\nn is the size of the list of genes of interest, and k is the number of genes within that list which are annotated to each gene set. geneRatio = k/n. For example, in my data, I have 1352 DEGs, and 5 of these are in the Hallmark_angiogenesis gene set.So, k = 5 , n = 1352. and geneRatio = k/n = 0.0037. (Note: for Hallmark_angiogenesis gene set, k cannot exceed 36, because there are 36 genes annotated in that gene set, i.e k is never bigger than M. )\nThere are two other terms. A richFactor is defined as the ratio of input genes (e.g., DEGs) that are annotated in a term to all genes that are annotated in this term. richFactor = k/M For my angiogenesis example, richFactor = k/M = 5/36.\nThe fold enrichment is defined as the ratio of the frequency of input genes annotated in a term to the frequency of all genes annotated to that term, and mathematically, is Fold enrichment = GeneRatio/BgRatio.\n(For RichFactor, there was a line of code in the Wu et al paper. y <- mutate(x, richFactor = Count / as.numeric(sub(\"/\\d+\", \"\", BgRatio))) Please do not confuse this as richFactor = count/BgRatio. It is not. BgRatio is a string \"36/4393\". The code deletes anything after the \"/\", i.e. /4393, so 36 is left behind. And so, richFactor = 5/36. Again, richFactor = k/M. Also, examine Figure 5A, notice richFactor varies between 0 - 0.15. Which fits in most cases.)\nReferences:\nhttps://yulab-smu.top/biomedical-knowledge-mining-book/enrichment-overview.html\nclusterProfiler 4.0: A universal enrichment tool for interpreting omics data. Wu et al 2021. DOI: 10.1016/j.xinn.2021.100141\nBoyle, Elizabeth I, Shuai Weng, Jeremy Gollub, Heng Jin, David Botstein, J Michael Cherry, and Gavin Sherlock. 2004. “GO::TermFinder–open Source Software for Accessing Gene Ontology Information and Finding Significantly Enriched Gene Ontology Terms Associated with a List of Genes.” Bioinformatics (Oxford, England)20 (18): 3710–15. https://doi.org/10.1093/bioinformatics/bth456."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612089/",
    "title": "GSEA and Cytoscape Enrichment maps",
    "question": "Hi everyone,\nI'm working with human RNA-seq data and performed a GSEA using the software (GSEA 4.3.3). When I try to export the results as an enrichment map to Cytoscape, it asks me to set two cutoffs: one for the p-value and one for the FDR. Since the FDR is calculated from the p-value, I'm wondering — wouldn't applying both filters be redundant? Thanks in advance for your help!",
    "answers": [
      "p-value and FDR are working in pairs.\nGenes pvalue from null are expected to be distributed uniformly (even with pvalue < 0.05), so the more (True Positive) your gene will be the smallest the pvalue it will have. FDR in the other end, set a fraction of comparisons for which the null is true.\nFrom here\nEnrichmentMap tips on parameter choice",
      "Both options are available in the Enrichment map mostly for convenience. In some cases, where signals are weak, none of the gene sets pass any reasonable FDR threshold, there is still the option to filter the results by p-value.\nAlso, the FDR in GSEA is not just the corrected p-value.\nThe p-value in GSEA is the empirical p-value calculated when comparing the ES score of a given geneset to the background distribution of ES scores from N randomizations of the data or genesets.\nThe ES is subsequently normalized using this randomized distribution to account for gene-set size to generate the NES (normalized enrichment score). NES = ES/(mean(abs(ES null))\nThe FDR, on the other hand, is the probability that a gene set with a given NES is a false positive and is calculated using the following formula - FDR(NES(obs)) = number of null NES >= NES(obs) / number of observed NES >= NES(obs)"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612261/",
    "title": "CNVs annotation after CNVkit and clinical databases",
    "question": "Hello everyone, hope you're well. I am currently working on a CNVs analysis with CNVKit and I would like to know if there is someone who could help me on how to annotate my CNVs but also query clinical databases such as COSMIC, OncoKb, CIVIC in relation to my CNVs.\nThanks camarade !",
    "answers": [
      "AnnotSV: an integrated tool for structural variations annotation\nhttps://academic.oup.com/bioinformatics/article/34/20/3572/4970516\nhttps://github.com/lgmgeo/AnnotSV\nmaybe this tool can help you."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612354/",
    "title": "The Biostar Herald for Thursday, May 22, 2025",
    "question": "The Biostar Herald publishes user submitted links of bioinformatics relevance. It aims to provide a summary of interesting and relevant information you may have missed. You too can submit links here.\nThis edition of the Herald was brought to you by contribution from Istvan Albert, and was edited by Istvan Albert,\nAssessing genomic reproducibility of read alignment tools | bioRxiv (www.biorxiv.org)\nGenomic research relies on accurate and reproducible computational analyses of DNA sequencing data to draw reliable biological conclusions. Read mapping, the process of aligning reads to a reference genome, is central to many applications, including variant detection and comparative genomics. While several tools have been developed for this task, genomic reproducibility1, defined as the consistency of results across replicates, remains underexplored. Here, we address this question by introducing a methodology based on synthetic replicates of sequencing data, generated by perturbing the original reads through shuffling, reverse complementing, or combined shuffling and reverse complementing.\nsubmitted by: Istvan Albert\nUltrafast and accurate sequence alignment and clustering of viral genomes | Nature Methods (www.nature.com)\nViromics produces millions of viral genomes and fragments annually, overwhelming traditional sequence comparison methods. Here we introduce Vclust, an approach that determines average nucleotide identity by Lempel–Ziv parsing and clusters viral genomes with thresholds endorsed by authoritative viral genomics and taxonomy consortia. Vclust demonstrates superior accuracy and efficiency compared to existing tools, clustering millions of genomes in a few hours on a mid-range workstation.\nsubmitted by: Istvan Albert\nEfficient evidence-based genome annotation with EviAnn | bioRxiv (www.biorxiv.org)\nFor many years, machine learning-based ab initio gene finding approaches have been the central components of eukaryotic genome annotation pipelines, and they remain so today. The reliance on these approaches was originally sustained by the high cost and low availability of gene expression data, a primary source of evidence for gene annotation along with protein homology. Existing annotation packages often underutilize these data sources, which prompted us to develop EviAnn (Evidence-based Annotation), a novel evidence-based eukaryotic gene annotation system. EviAnn takes a strongly data-driven approach, building the exon-intron structure of genes from transcript alignments or protein-sequence homology rather than from purely ab initio gene finding techniques. We show that when provided with the same input data, EviAnn consistently outperforms current state-of-the-art packages including BRAKER3, MAKER2, and FINDER, while utilizing considerably less computer time. Annotation of a mammalian genome can be completed in less than an hour on a single multi-core server.\nsubmitted by: Istvan Albert\nDetection of viral sequences at single-cell resolution identifies novel viruses associated with host gene expression changes | Nature Biotechnology (www.nature.com)\nWe introduce a method that accurately and rapidly detects viral sequences in bulk and single-cell transcriptomics data based on the highly conserved RdRP protein, enabling the detection of over 100,000 RNA virus species. The analysis of viral presence and host gene expression in parallel at single-cell resolution allows for the characterization of host viromes and the identification of viral tropism and host responses. We apply our method to peripheral blood mononuclear cell data from rhesus macaques with Ebola virus disease and describe previously unknown putative viruses. Moreover, we are able to accurately predict viral presence in individual cells based on macaque gene expression.\nsubmitted by: Istvan Albert\nThe Sources of Researcher Variation in Economics | NBER (www.nber.org)\nWe use a rigorous three-stage many-analysts design to assess how different researcher decisions—specifically data cleaning, research design, and the interpretation of a policy question—affect the variation in estimated treatment effects. A total of 146 research teams each completed the same causal inference task three times each: first with few constraints, then using a shared research design, and finally with pre-cleaned data in addition to a specified design. We find that even when analyzing the same data, teams reach different conclusions.\nsubmitted by: Istvan Albert\nWant to get the Biostar Herald in your email? Who wouldn't? Sign up righ'ere: toggle subscription",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612352/",
    "title": "Correcting Technical Artifacts in Cross-Dataset Pseudobulk Analysis",
    "question": "Dear all,\nI’m performing pseudobulk analysis using my scRNA-seq data along with publicly available scRNA-seq datasets with DEseq2.\nNaturally, there are batch effects between the two datasets (e.g., nFeatureCount, percent mitochondrial/ribosomal RNA), and my dataset shows lower mitochondrial and ribosomal RNA percentages.\nI'm performing DEG analysis from the raw count matrix from all data. In the initial DEG analysis, many mitochondrial/ ATP/ translation-related pathways appeared enriched among the downregulated pathways in my dataset.\nSo, in the next step, I calculated the mean mitochondrial and ribosomal RNA percentage per sample and included them in matrix and as covariates in the design formula : design = ~ percent_mito + percent_ribo + group\nThis reduced—but did not eliminate—the mitochondrial and ATP-related pathways from the downregulated results.\nNow I’m unsure whether these findings reflect true biological differences or technical artifacts.\nIn such cases, what is the best way to correct for batch effects?\nThanks for reading.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9581493/",
    "title": "ADMIXTURE Freezes Right Before Completion",
    "question": "Hello, I am trying to run admixture for k = 4 on a .ped file, with cross validation. I've been successful before in running admixture on the same dataset with different values of k. My input is simply: ./admixture --cv myfile.ped 4\nThe program runs fine and prints it's progress as it goes. However, I'm encountering an error where it seems to stall at the very end, before writing any output files. After the block relaxation iterations converged and Fst values were calculated between populations, admixture appears to freeze. Using top I can see it's using the cpu but it is not printing any progress/results and I'm not sure if there is anything more to be performed after between population Fst values are estimated. Moreover, no output files appear in my directory. I've waited for many hours to see if anything would happen but to no avail. Here's a screenshot of what this looks like:\nHas anyone else encountered something similar? If so, what would you recommend I do? I greatly appreciate any advice!\nThank you!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9609844/",
    "title": "Impact of the number of PCs on the clustering in scRNA seq",
    "question": "Hello all,\nI am working with a scRNA-seq dataset. I apply a PCA and select two different number of PCs (10 and 20). Then I apply a Louvain clustering on the reduced space with a fixed resolution (0.3) and I compare the two clusterings. I get more clusters when I selected 10 PCs than when I selected 20 PCs.\nI wonder why is that and I would therefore appreciate any hint!\nThank you very much :)\n(I join the scree plot showing the percentage of variance explained by each PC)\nAlong with the two UMAP showing the clustering.",
    "answers": [
      "As much as it might feel like an unsatisfactory answer, this totally depends on your downstream needs and what meaningful biology you can attach to the clustering. Some of these clusters will be based on technical variation, some will be QC-associated, some will associate with cell cycle while others will be driven by some effect based on a set of differential genes. Find out what markers are driving the clusters and you'll get a much better understanding of the clustering 'success' yourself.\nWhen gauging the success of graphical clustering the first question to ask is how meaningful are the clusters and you do that by looking at the marker genes for that cluster and see if you can make sense of it.\nFor your larger UMAP (bottom right), for example, I'd be quite interested in finding out the set of genes that would resolve cluster 2 (green) and cluster 8 (blue). I would equally be interested in understanding the stripe running down the middle of cluster 1 (orange), cluster 0 (dark blue) and cluster 5 (brown).\nSelecting the optimal number of PCs is not a simple question and typically the most practical solution is to iterate a few times and see if you get better resolution with a larger input set.\nSince you're already on python, you can look into tools like cNMF which allow you to optimise feature selection more readily than standard graphical clustering approaches. However, I would storngly recommend exploring your data a bit more before moving on with extra technical tools. Once you've convinced yourself you understand what is going on, you'll be in a much better position to judge these kinds of things yourself."
    ]
  },
  {
    "url": "https://www.biostars.org/p/482158/",
    "title": "ATAC-seq +4 -5 shift",
    "question": "Dear all,\nI saw having the mapped reads have +4 and -5 shift in ATAC-seq is a common practice.\nSome place says \"reads should be shifted + 4 bp and − 5 bp for positive and negative strand respectively, to account for the 9-bp duplication created by DNA repair of the nick by Tn5 transposase and achieve base-pair resolution of TF footprint and motif-related analyses\"\nSome place says:\" When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site.\"\nI'm a little bit confused. Are shifting mainly to center the peak or avoid the duplication?\nDoes anyone have a good illustration on this? What will happen to the peak calls if this step is skipped?\nThank you!",
    "answers": [
      "I illustrated the molecular biology here: http://guertinlab.org/wp-content/uploads/2021/01/Tn5_illumina_adapters_mjg_2.pdf\nwe skip this step for peak calling--it really only matters for looking at the data at single nucleotide resolution composite profiles.",
      "The shifting isn't for any real purpose unless you want to plot the exact cut location (e.g., when searching for motifs), it simply harkens back to one of the first ATAC-seq papers where they performed this adjustment to account for the 9-base single-stranded over-hang on each end of the fragment. Papers since have simply followed suite. A vastly more sensible strategy would be to use the 9 bases on each end of the fragment, since these are bases that are necessarily open.",
      "Turns out +4/-4 is the correct shift. You can see that when you make a bigwig separately from the + and - strands, then the reads align with each other for +4/-4 and it's off by one for +4/-5. The main idea is that transposition events should be mapped to the same base pair regardless of strand."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612994/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612991/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612988/",
    "title": "ATAC seq: from peaks to differential analysis ??",
    "question": "Hi,\nI’m fairly new to ATAC-seq and have successfully run MACS2 separately for each of my samples. I now have individual *.narrowPeak files as output.\nMy experimental design looks like this:\nSample_ID     Cell_type     Condition     Donor\n\nSample_1      T_cells       Tumor         Donor_1\nSample_2      T_cells       Normal        Donor_1\nSample_3      T_cells       Tumor         Donor_2\nSample_4      T_cells       Normal        Donor_2\n...\nSample_11     Dendritics    Tumor         Donor_10\nSample_12     Dendritics    Normal        Donor_10\nSample_13     Dendritics    Tumor         Donor_11\nSample_14     Dendritics    Normal        Donor_11\nAs you can see, I have two cell types (T_cells and Dendritics), and for each donor, I have paired Tumor and Normal samples.\nMy goal is to perform a differential accessibility analysis (Tumor vs Normal), accounting for both Donor and Cell_type. I’m also interested in comparing Tumor (T_cells) vs Tumor (Dendritics).\nI heard that it is possible to use DESeq2 for ATAC-seq data, so my design will look like this: https://bioconductor.org/packages/3.21/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#group-specific-condition-effects-individuals-nested-within-groups\nI have a few questions:\n1) Can I use the exact same code from the vignette, as typically done for RNA-seq data? Or are there any parameters or steps specific to ATAC-seq that I should consider?\n2) I’m struggling with how to convert my individual *.narrowPeak files into a count matrix. Do you have any recommendations or tools to help with this step?\n3) Are there alternative methods to DESeq2 that would be better suited for this kind of analysis? I guess limma should work the same no ?\nThank you in advance for your help !",
    "answers": [
      "2:\n# make saf file:\nawk 'OFS=\"\\t\" {print $1\":\"$2+1\"-\"$3, $1, $2+1, $3, \"+\"}' ${sample}_peaks.narrowPeak > featureCounts_peaks.saf\n\n$featureCounts -a featureCounts_peaks.saf \\\n    -F SAF \\\n    --read2pos 5 \\\n    -p \\\n    -o ${peak_all_dir}${sample}_countMatrix.txt ${rmBL_dir}*.bam\n${rmBL_dir}*.bam: all bam files for generating narrowPeaks\n3: DEseq2 is oke. design matrix and param depends on research question, not on the ATAC-seq or RNA-seq as they are all readcount type",
      "I also recommend converting the peak to SAF file as QX shows, although I just count fragments centered on cut site. There's little difference either way though.\nFor more guidance on creating a count matrix, you do need to create a common set of peaks first. Two main ways to do this, either take the intersection of peaks present in all samples or the union.\nI use bedtools, e.g.\n# N just represents number of total samples.\nbedtools mutliinter -i ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | awk '$4 == N' | bedtools sort -i - | bedtools merge -i - > All_Samples_Intersection.bed\nOr,\ncat  ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | bedtools sort -i - | bedtools merge -i - > All_Samples_Union.bed\nThen you can convert that bed into SAF and use featurecounts to count reads from each sample and treat similar to RNA-seq.\nOne thing I would suggest, I usually use a low count filter before moving forward with DE analysis. With ATAC-seq, there's usually more noise (I use relatively lax peak calling), so I tend to use a higher count threshold."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612987/",
    "title": "CUT&RUN analysis pipeline",
    "question": "Hello, My lab has generated data using the CUT&RUN protocol. Unfortunately, Its single-end data. I tried the CUT&RUN tools and the henipipe tool. Both are for paired end data.\nAny suggestion how I can analyse these data ? Any idea how to set the parameters in case I use the classical Trimmomatic and bowtie2 ? Any other pipelines to try ?\nThanks and Regards. (Loosing my mind)",
    "answers": [
      "You don't need to do anything too special. Trim adapters with your favorite software, align to genome using BWA-MEM or bowtie2, and call peaks using MACS2.\nMost of the fancy stuff with CUT&RUN is when you have paired end data. With paired end data you get information on insert/fragment size, which can be used to infer nucleosomal and subnucleosomal fragment, which in turn lets you guess whether afragment was from nucleosome protection or TF binding."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612985/",
    "title": "Cut&Run replicates handling",
    "question": "Is there a CUT&RUN peakcaller that supports replicates? If not, what is your recommended approach for handling replicates?\nIt appears that all available peak callers, such as SEACR, are designed to call peaks from single pull-down experiments.\nOur Experiment: We have two conditions, Untreated (UTR) and Treatment (TREAT), each with two biological replicates. Each condition also has respective INPUT data for normalization.\nWe have completed:\nQuality Control (QC)\nAlignment\nDuplicate marking/removal\nSpike-in Calibration\nNext, we would like to proceed to peak calling and differential analysis. We would appreciate your recommendations on suitable peak callers and the best practices for handling replicates.",
    "answers": [
      "You can use nf-core cutandrun pipeline, it supports replicates in the step of consensus peaks calculations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612984/",
    "title": "Unable to extract count matrices from multi-layered Seurat object",
    "question": "Hello, I have a merged R object with 18 samples\nall_combined <- merge(\n  sample_1MI1_so,\n  y = c(\n    sample_1MI3_so,\n    sample_2MI1_so,\n    sample_2MI3_so,\n    sample_3C_MI2_so,\n    sample_3MI1_so,\n    sample_3MI3_so,\n    sample_4D_MI2_so,\n    sample_4MI1_so,\n    sample_4MI3_so,\n    sample_5MI1_so,\n    sample_5MI3_so,\n    sample_6MI1_so,\n    sample_6MI3_so,\n    sample_7MI1_so,\n    sample_7MI3_so,\n    sample_8MI1_so,\n    sample_8MI3_so\n  ),\n  add.cell.ids = c(\n    \"sample_1MI1_so\",\n    \"sample_1MI3_so\",\n    \"sample_2MI1_so\",\n    \"sample_2MI3_so\",\n    \"sample_3C_MI2_so\",\n    \"sample_3MI1_so\",\n    \"sample_3MI3_so\",\n    \"sample_4D_MI2_so\",\n    \"sample_4MI1_so\",\n    \"sample_4MI3_so\",\n    \"sample_5MI1_so\",\n    \"sample_5MI3_so\",\n    \"sample_6MI1_so\",\n    \"sample_6MI3_so\",\n    \"sample_7MI1_so\",\n    \"sample_7MI3_so\",\n    \"sample_8MI1_so\",\n    \"sample_8MI3_so\"\n  ),\n  merge.data = TRUE\n)\nView object\nall_combined\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 55 layers present: counts.1, counts.2, counts.3, counts.4, counts.5, counts.6, counts.7, counts.8, counts.9, counts.10, counts.11, counts.12, counts.13, counts.14, counts.15, counts.16, counts.17, counts.18, data.1, scale.data.1, data.2, scale.data.2, data.3, scale.data.3, data.4, scale.data.4, data.5, scale.data.5, data.6, scale.data.6, data.7, scale.data.7, data.8, scale.data.8, data.9, scale.data.9, data.10, scale.data.10, data.11, scale.data.11, data.12, scale.data.12, data.13, scale.data.13, data.14, scale.data.14, data.15, scale.data.15, data.16, scale.data.16, data.17, scale.data.17, data.18, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nIn the all_combined object, I', trying to extract the counts matrices from it, but I'm not sure how to do this? Below is what I've tried\nall_combined_count_matrix <- LayerData(object = all_combined, assay = \"RNA\", layer = \"counts\")\nWarning: multiple layers are identified by counts.1 counts.2 counts.3 counts.4 counts.5 counts.6 counts.7 counts.8 counts.9 counts.10 counts.11 counts.12 counts.13 counts.14 counts.15 counts.16 counts.17 counts.18\n only the first layer is used\n\n# only 1st sample counts matrix is extracted....want to extract all 18 counts matrices...all_combined object contains 18 seurat objects merged into one\n\n`\nManually combine all 18 count matrices into one\n# List all layers that start with \"counts\"\nall_counts_layers <- Layers(all_combined[[\"RNA\"]])\nall_counts_layers <- counts_layers[grepl(\"^counts\", all_counts_layers)]\n\n# # view all_count_layers\n#  [1] \"counts.1\"  \"counts.2\"  \"counts.3\"  \"counts.4\"  \"counts.5\"  \"counts.6\"  \"counts.7\"  \"counts.8\"  \"counts.9\"  \"counts.10\" \"counts.11\" \"counts.12\" \"counts.13\" \"counts.14\"\n# [15] \"counts.15\" \"counts.16\" \"counts.17\" \"counts.18\"\n\n\n\n# Extract each layer and combine\nall_count_matrices <- lapply(all_counts_layers, function(layer) {\n  LayerData(all_combined, assay = \"RNA\", layer = layer)\n})\n\n\n# View all_count_matrices (prints out all 18 count matrices. NOTE: each matrices has different number of rows/genes and different number of columns. However, total rows adds up to 23,447 genes and total columns add to 135,704 cells)\n# all_count_matrices\n\n\n\n# Combine into one gene x cell matrix\nall_combined_counts <- do.call(cbind, all_count_matrices)\n\n# Error in cbind.Matrix(x, y, deparse.level = 0L) : \n#   number of rows of matrices must match\nAny advice on how to do this effectively would be greatly appreciated. I'm using Seurat v5.3.0",
    "answers": [
      "Alright, I fixed the issue by joining the counts layers in all_combined. I saved it to a new seurat object. Hopefully this helps someone else!\nall_combined_join_layers <- JoinLayers(all_combined)\nThis joins the 18 counts layers into one layer (and also the 18 normalized data layers)\nall_combined_join_layers\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 21 layers present: data, counts, scale.data.1, scale.data.2, scale.data.3, scale.data.4, scale.data.5, scale.data.6, scale.data.7, scale.data.8, scale.data.9, scale.data.10, scale.data.11, scale.data.12, scale.data.13, scale.data.14, scale.data.15, scale.data.16, scale.data.17, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nI then extracted the counts matrix from all_combined_join_layers\nall_combined_count_matrix_2 <- LayerData(object = all_combined_join_layers, assay = \"RNA\", layer = \"counts\")\n\nall_combined_count_matrix_2\n\n23477 x 135704 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n\nENSMMUG00000023296 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692             . . . . . . . . . 1 . . . . . 1 2 . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612983/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612982/",
    "title": "usage of ChromHMM and Segway",
    "question": "I want to use ChromHMM and Segway to identify chromatin states or genomic segmentation based on existing models. Where are these models stored and what form these models are, and how can I use them for direct calculations? ChromHMM(https://ernstlab.github.io/ChromHMM/); Segway(https://segway.hoffmanlab.org/)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612980/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612979/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612978/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612977/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612976/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612975/",
    "title": "phage sequence no alignment to genome",
    "question": "I am currently working with a bacterial strain (let’s call it Strain A) for which I have a complete genome assembly (StrainA_genome.fasta). From this strain, I obtained two sets of phage-related sequences using different experimental approaches:\n1. An assembled phage genome from lab experiments: StrainA_phage_gapfilled.fasta\n2. A pair of short-read sequencing files from another phage preparation: StrainA_phage_R1.fasta and StrainA_phage_R2.fasta\nSeparately, I predicted prophage regions on StrainA_genome.fasta using Cenote-Taker 3 and PhageBoost, and collected all the predicted prophage sequences into a file: StrainA_predicted_phages.fasta.\nMy goal is to verify whether the experimentally derived phage sequences correspond to any of the predicted prophage regions, i.e., to confirm that the experimentally recovered sequences truly originate from prophages within Strain A.\nWhat I tried\nI attempted various alignments, including:\nI used bwa mem to align both the raw phage reads and the gap-filled phage assembly against the predicted prophage regions, using commands like:\n # Align short reads to predicted phages \n  bwa mem -t 4 StrainA _predicted_phages.fasta StrainA_phage_R1.fasta StrainA_phage_R2.fasta > phage_reads_vs_predicted.sam\n\n # Align assembled phage genome to predicted phages\n  bwa mem -t 4 StrainA_predicted_phages.fasta StrainA_phage_gapfilled.fasta > phage_assembly_vs_predicted.sam\nnone of the reads or assemblies mapped to any of the predicted prophage sequences.\nI then aligned the same experimental phage sequences (both reads and the assembled genome) directly to the full bacterial genome (StrainA_genome.fasta), expecting at least partial matches in the prophage regions. However, there were still no alignments.\n  # Also tried aligning both to the host genome:\n  bwa mem -t 4 StrainA_genome.fasta StrainA_phage_gapfilled.fasta > phage_vs_host.sam\nThis was very unexpected — if the prophages are indeed part of the genome, I would expect at least some reads to map to those regions.\nI also tried the same comparisons using Minimap2/bowtie2/blast, even tuned parameters to allow more mismatches and gaps. Unfortunately, there were no alignments..\nTo verify the identity of the experimentally obtained phage sequences, I submitted StrainA_phage_gapfilled.fasta to NCBI ORF Finder, extracted several of the longer predicted protein sequences, and then ran BLASTp against the virus (taxid:10239) database. These searches returned high-confidence hits to known phage proteins, supporting the idea that the experimental sequences are indeed phage-derived.\nAdditionally, I extracted CRISPR spacer sequences from Strain A using CRISPRCasTyper, and tried to align them (using BLAST) to the gap-filled experimental phage genome. Only one spacer hit was found, with a relatively low score (e-value ~1e-2).\nI am now wondering:\n1. Could this complete lack of alignment be caused by my alignment strategy?\nAre there better tools or methods specifically suited to phage–prophage or phage–host genome comparisons?\n2. Could experimental artifacts explain this?\nI do not perform the experimental work myself. Could the phage DNA isolation,amplification, or assembly methods introduce chimerism or unrelated sequences that mislead mapping?\n3. Or is it genuinely common to see such divergence between experimentally isolated phages and prophage predictions from the same strain genome?\nAny thoughts or suggestions on improving the alignment strategy or interpreting these results would be greatly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612333/",
    "title": "Remapping assembled contigs from Flye output file",
    "question": "I have a bacterial assembled fasta file done using flye. I wanted to know how I can remap this assembled contigs against a reference genome as the number of contigs are very high.",
    "answers": [
      "Plenty of tools to choose from: MUMmer, Mauve, Lastz, .... just to name a few.\nGoogle with the term 'genome to genome alignment' will give you several other options.",
      "You can map contigs vs a related reference genome with a tool like Ragtag https://github.com/malonge/RagTag. This scaffolding makes more sense than just general contig mapping."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612340/",
    "title": "Where to find hg38 panel-of-normals and germline VCFs without 'chr' prefix?",
    "question": "I’m currently performing somatic variant calling with Mutect2 using hg38 as the reference genome. However, my BAM files were aligned using a version of the reference genome without 'chr' prefixes in the contig names (e.g., 1, 2, ..., instead of chr1, chr2, ...).\nI understand that the GATK-provided PON (1000g_pon.hg38.vcf.gz) and germline resource (af-only-gnomad.hg38.vcf.gz) files contain 'chr'-prefixed contigs, which causes a mismatch during Mutect2 execution.\nCould you please let me know:\nIf non-‘chr’-prefixed versions of these files are available for download?\nIf not, is there an official recommendation for converting the contig names in the VCF files or in the BAM/reference to ensure consistency?\nThank you in advance for your help.\nBest regards,",
    "answers": [
      "If not, is there an official recommendation for converting the contig names in the VCF files or in the BAM/reference to ensure consistency?\nnot \"official\" but you should replace the chromosome names with bcftools annotate --rename-chrs and set the dictionary (lines starting with ##contig= ) in the header using bcftools reheader ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612341/",
    "title": "Contigs Input For Phylogenetic Tree",
    "question": "Hello,\nI want to create a nextstrain phylogenetic tree from multifastas (on bacteria strains).\nThe problem is my assemblies are contigs.\nIs concatenate the contigs is a good (and easier) way to get the complete sequence, or I need scaffolding (and if scaffolding needed, what tool do you advise)?\nThank you very much",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612336/",
    "title": "assembly tool for plant data",
    "question": "I have to generate assembly of polyploid and aneuploid sugarcane wgs data. I have only short reads. Can you please suggest any short read assembly tool compatible for polyploid data?\nAlso annotation tool. I need to annotate the assembly afterwards.\nThanks",
    "answers": [
      "I don't think that's possible - or if it is possible, it will not be satisfactory, informative or publishable. In short, sugar cane is one of the hardest plant genomes around and it is likely a huge amount of work even with all required data.\nI would use a modern, Pacbio Hifi or ONT assembled genome with Hi-C like this https://www.nature.com/articles/s41467-024-47390-6 and align your short reads to it.\nI would also try to educate your PI about genomics and realism."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612332/",
    "title": "Will using different Illumina manifest files (with only chr/position changes) affect genotype calling in GSA microarrays?",
    "question": "Hi all,\nI'm working with Illumina GSA (Global Screening Array) microarray data and have a question regarding genotype calling when using different manifest files.\nI ran the same IDAT files through the analysis pipeline using two different manifest files. The only differences between these manifests are the chromosome and genomic position annotations — the probe IDs, probe sequences, and other fields remained identical.\nMy assumption is that the chr and position information in the manifest are only used for annotation, and therefore should not affect genotype calling, which is driven by signal intensity clustering (e.g., via the GenCall algorithm in GenomeStudio or similar).\nHowever, I noticed that some genotype calls changed between the runs. For example:\nA homozygous call changed to heterozygous, or vice versa.\nSome SNPs previously called as '0' (no call) were later assigned genotypes, and vice versa.\nThis raises a few questions:\nCan differences in the manifest file that are limited to chromosome and position fields really cause such changes in genotype calls?\nOr is it more likely that the changes are due to technical issues, such as a wet-lab error, or mistakes in re-generating result files using different manifest versions?\nIf anyone has experience with this or can confirm how the manifest impacts genotype calling, I’d really appreciate your input.\nThanks in advance!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612331/",
    "title": "tteximeta problem: couldn't find matching transcriptome, returning non-ranged SummarizedExperiment",
    "question": "Hi all,\nI built the salmon index using the Gencode genome and transcriptomic sequence files. I then quantified my RNA fastq files and tried to use Teximeta (V1.20.3) to load the result to DESeq2. I encountered a problem related to the built-in taxomes. How can I fix it? I know that Teximeta supports Gencode format/files.\n#code for building salmon index\n\n grep \"^>\" <(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 > decoys.txt\n\n sed -i.bak -e 's/>//g' decoys.txt\n\ncat gencode.v48.transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz > grantome.fa.gz\n\n#create index\nsalmon index --gencode -t /work/tmagdy/heps_nutr_rnaseq/PA/ref/salmon/gentrome.fa.gz -i /work/tmagdy/heps_nutr_rnaseq/PA/ref/salmon/salmon.indx --decoys /work/tmagdy/heps_nutr_rnaseq/PA/ref/salmon/decoys.txt -k 31\n\nfile1<-file.path(\"/Volumes/T7/ipsc_heps_rnaseq/salmon/quant/Control-1\",\"quant.sf\") \nfile2<-file.path(\"/Volumes/T7/ipsc_heps_rnaseq/salmon/quant/Control-2\",\"quant.sf\") \nfile3<-file.path(\"/Volumes/T7/ipsc_heps_rnaseq/salmon/quant/Control-3\",\"quant.sf\") \nfile4<-file.path(\"/Volumes/T7/ipsc_heps_rnaseq/salmon/quant/PA-100-1\",\"quant.sf\") \nfile5<-file.path(\"/Volumes/T7/ipsc_heps_rnaseq/salmon/quant/PA-100-2\",\"quant.sf\")\nfile6<-file.path(\"/Volumes/T7/ipsc_heps_rnaseq/salmon/quant/PA-100-3\",\"quant.sf\") \n\nfiles<- c(file1,file2,file3,file4,file5,file6)\nfile.exists(files)\n\ncoldata<-data.frame(files, names= c(\"cnt_1\",\"cnt_2\",\"cnt_3\",\"pa_1\",\"pa_2\",\"pa_3\"), condition=c(rep(\"control\",3),rep(\"pa\",3)),stringsAsFactors=FALSE)\ncoldata$files\n\nse <- tximeta(coldata)\nimporting quantifications\nreading in files with read_tsv\n1 2 3 4 5 6 \ncouldn't find matching transcriptome, returning non-ranged SummarizedExperiment\n\n> packageVersion(\"tximeta\")\n[1] '1.20.3'",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612319/",
    "title": "Heatmap of rnaseq data with z-score scale",
    "question": "I have a doubt/question regarding the heatmap visualization of gene expression data obtained with bulk RNA-seq technology from different datasets, with z-score row scaling. By using the same list of genes, when the heatmap generated by using only samples from the same datasets heatmap highlights difference in the gene expression between patients vs controls (Figure1) but when the matrix include also samples from different datasets differences between patients and controls seem to disappear, while it seems to be opposite expression trends between samples from different datasets (Figure2). can you give me some suggestions on how to solve this problem?",
    "answers": [
      "The z-score is the raw score minus the mean of all samples, divided by the standard deviation so if you add samples, then you change the z-score. You should make a choice of the samples you want to compare because z-scores will vary depending on what you want to show and cannot be compared from a heatmap to another heatmap. Adding samples from a different RNA-seq dataset may not be relevant because there are inherent batch/biological differences, but it all depends on the nature of your datasets.",
      "There isn't really a problem. Your data is what it is, but I think adding data from a totally different data set is a bad idea. RNASeq is too sensitive to batch effects, those batch effects are diminishing the comparisons you want to highlight. Just don't combine your data like that."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612323/",
    "title": "Z-score for scRNA-seq data?",
    "question": "Hi,\nI was reading up on Z-score and how it is commonly used for pathway analysis comparisons in bulk-seq/pseudobulk data. I want to make a heatmap for my pathway analysis results using z-scores but I did not pseudobulk my scRNA-seq data during GSEA analysis.\nIt seems like it is not possible to calculate z-scores without pseudobulking my data, is that correct? If that is so, is there an equivalent metric I can use to generate a similar heatmap? Would the results be better if I pseudobulk?\nThank you!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612316/",
    "title": "OncodriveFML warnings",
    "question": "Hi, I am running a cancer driver gene analysis with oncodrivefml. I am using a maf file from TCGA biolinks and a cohort-level maf file from GDC portal. For an elements file that is required as input for oncodriveFML command line tool, I used a default elements file provided by the repository (cds.tsv). I have warnings when I run the command line tool, saying \"Background mismatch at position xxx at \"GENE_NAME\"\" for a significant number of genes.\nI tried running it with a web-based tool, but as the default build parameter for this is hg19, I moved to a command line tool. If anyone has an experience with running oncodrivefml on command-line, could you please help me with this issue or help me understand what this warning means? or if anyone can suggest an alternative cancer driver gene selection analysis tool for hg38, that would also be very helpful, thanks! I have results from dndscv and mutsigcv for now.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612231/",
    "title": "European Variation Archive submission eva-sub-cli validation failure: \"Sample name concordance check\"",
    "question": "UPD: After I added more samples to the submission metadata file, the validation completed successfully and the \"sample name mismatch\" error no longer appeared.\nHello,\nI'm trying to submit data to the European Variation Archive (EVA) using eva-sub-cli, and I encounter a validation failure related to the \"Sample name concordance check\". The exact error is:\nSample name concordance check: sample name mismatch\nI've verified the following:\nSample names in the VCF files match exactly (copy-pasted) the ones in the sample metadata Excel file.\nAnalysis Alias also matches.\nAll names are alphanumeric, with no special characters or whitespaces.\nDespite these checks, the concordance check fails. The validator report shows only this issue.\nQuestions:\nWhat else can trigger the sample name mismatch error in EVA submissions?\nCould incorrect linkage between the analysis and sample Excel elements cause this?\nIs there any way to debug this more deeply?\nAny insights from people who have successfully passed this validation stage would be greatly appreciated!\nbest regards, Askhat",
    "answers": [
      "Just for the sake of anyone else having the same issue. You need to annotate each of your files in the \"Files\" tab with an analysis alias. Here the report states:\nYour GWAS analysis has a sample described in the metadata but no sample in the VCF files\nAnother section with \"no analysis\" specified has a sample described in the VCF Files but not in the metadata"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612300/",
    "title": "Project Research Scientist-I (Bioinformatics – Next Generation Sequencing Data Analysis) - India",
    "question": "Job Title Project Research Scientist-I Bioinformatics Next Generation Sequencing Data Analysis\nFunded by Indian Council of Medical Research\nWorkplace Department of Biosciences and Bioengineering, IIT Roorkee, Uttarakhand, 247667, India\nSupervisor Dr. Deepak Sharma (Associate Professor)\nJob Details Salary INR 56,000.00 + HRA\nDuration 1 Year Temporary Project based\nEligibility Criteria PhD or MSc in Bioinformatics or Computational Biology\nExperience Minimum 2 years of experience in the relevant field Python R QIIME2 DADA2 etc.\nContact for Application Contact Person Dr. Devraj JP Scientist D Clinical Epidemiology Division ICMR-NIN Hyderabad Telangana State India\nPhone +91-80749286200\nEmail jpdevraj26@gmail.com",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612291/",
    "title": "online course: AI for Genomics: from CNNs and LSTMs to TRANSFORMERS",
    "question": "Dear all, We are excited to announce our upcoming online course: AI FOR GENOMICS: FROM CNNs AND LSTMs TO TRANSFORMERS\nDates: 9–11 September\nCourse website: https://www.physalia-courses.org/courses-workshops/ai-for-genomics/\nThis hands-on course will dive into the application of modern AI models—including Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformers—to genomic and metagenomic sequence data.\nParticipants will work through practical coding exercises and interactive notebooks, learning to:\nModel and classify genomic sequences\nPredict functional genomic elements\nPerform metagenomic source tracking\nApply attention mechanisms and train transformer-based models\nInterpret biological results using AI outputs",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612290/",
    "title": "Anyone familiar with studies on computational neuroscience focused on consciousness, particularly in anatomical areas such as ARAS?",
    "question": "Hello, I am a first year PhD student in computational biology, interested in diverging my research into computational neuroscience. Currently, I am interested in studies on consciousness in computational neuroscience/biology (specifically in anatomical areas of Ascending Reticular Activating System (ARAS), but there are very little data available publicly in terms of things like ATAC-seq, scRNA-seq, single cell resolution electrophysiology, optogenetics, and calcium imaging, etc., for primates or mouse. If anyone is familiar with this area of study, or anything of related nature... What are some solutions to this? Am I not looking in the right place, or perhaps there are data out there and I just need to look for it in unknown corners online? Although I can take a look at other anatomical areas for its microscopic environment, if possible, the ARAS would be the best.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612287/",
    "title": "Evaluating Large CRISPR/Cas9 Deletions (~300 kb) in F0 Xenopus Embryos",
    "question": "I am working with Xenopus and using CRISPR/Cas9 with two sgRNAs to induce a large genomic deletion (~300 kb) by injecting RNP complexes into fertilized eggs. Since these are F0 embryos, I expect mosaicism and would like to assess the deletion efficiency and mosaic rate directly in F0, without generating F1s.\nFor small deletions (~1 kb), I typically use PCR followed by Sanger sequencing and ICE or DECODR analysis. But for large deletions, PCR bias and low abundance of the deletion allele make this approach unreliable.\nHas anyone successfully quantified large deletions (~300 kb) in F0 Xenopus embryos? Are there reliable strategies—such as ddPCR, long-range PCR combined with nanopore sequencing, or other approaches—that work well in this context?\nAny advice, references, or protocol recommendations would be greatly appreciated.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612254/",
    "title": "Trim CDS to ORF",
    "question": "Does anyone know how to extract CDS from transcripts? I have transcripts structured like this:\nXspecies1234-mRNA transcript offset:272 AED:0.06 eAED:0.06 QI:272|1|1|1|0.8|0.83|6|66|318\nNotice the transcript offset part. I have the issue that excluding the 272 bases prior to the offset start codon is not all I need to do to isolate the coding sequence, because the stop codon is somewhere before the end of the transcript. I can fix them manually, but going through thousands of transcripts is extremely time-consuming. Any ideas of how to automate this for a multi-fasta? I'm looking for a way to not only cut off the part prior to the offset but also the part after a within-reading-frame stop codon.",
    "answers": [
      "I haven't found an ORF finder that uses the offsets in the headers. Someone far better at coding than me helped me with this solution:\n    library(Biostrings)\n\n    # Load sequences\n    fasta_file <- \"C:/Users/data/XXspecies.fasta\"\n    seqs <- readDNAStringSet(fasta_file)\n\n    # Function to extract transcript offset\n    extract_offset <- function(header) {\n      match <- regexpr(\"transcript offset:\\\\d+\", header)\n      if (match == -1) return(0)  # If not found, don't trim\n      offset_str <- regmatches(header, match)\n      as.integer(sub(\"transcript offset:\", \"\", offset_str))\n    }\n\n    # Extract offsets\n    offsets <- sapply(names(seqs), extract_offset)\n\n    # Trim off the first 'offset' nucleotides\n    trimmed_seqs <- DNAStringSet(mapply(function(seq, offset) {\n      subseq(seq, start = offset + 1)\n    }, seqs, offsets, SIMPLIFY = FALSE))\n\n    names(trimmed_seqs) <- names(seqs)\n\n    # Function to trim a sequence to the first ATG and in-frame stop codon\n    trim_to_coding_region <- function(seq) {\n      seq_char <- as.character(seq)\n      atg_pos <- regexpr(\"ATG\", seq_char)[1]\n      if (atg_pos == -1) return(DNAString(\"\"))  # No start codon\n\n      # Scan in-frame triplets starting from ATG\n      for (i in seq(from = atg_pos, to = nchar(seq_char) - 2, by = 3)) {\n        codon <- substr(seq_char, i, i+2)\n        if (codon %in% c(\"TAA\", \"TAG\", \"TGA\")) {\n          return(DNAString(substr(seq_char, atg_pos, i+2)))  # Inclusive of stop codon\n        }\n      }\n      return(DNAString(\"\"))  # No in-frame stop codon found\n    }\n\n    # Apply to all sequences\n    trimmed_seqs_2 <- DNAStringSet(lapply(trimmed_seqs, trim_to_coding_region))\n    names(trimmed_seqs_2) <- names(trimmed_seqs_2)\n\n    # Write to new FASTA\n    writeXStringSet(trimmed_seqs_2, \"trimmed_sequences.fasta\")\nHowever, this code doesn't work on sequences that don't contain a stop codon. I'd be interested in any other solutions out there.",
      "Why not use an ORF finder tool?\neg. getORF , longestORF, transdecoder, FrameD, ... many options. Unless yo have prior knowledge on your transcripts/ORFs you likley can't do a better job than any of those tools.",
      "One could write a very quick and easy script with BioPython/Biostrings (other Bio* modules for other languages) to do this.\nChop off the first 271 nucleotides, then move through 3 bases at a time looking for a stop codon."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612275/",
    "title": "Multi-omic trajectory on CITE-Seq data",
    "question": "I have 10x single-cell multiomic CITE-Seq data with RNA gene expression and cell surface markers. I used Seurat to create a WNN UMAP that combines RNA and surface marker data. Now, I would like to create a trajectory on this UMAP.\nActually, I am able to create a trajectory on this UMAP using Slingshot for example, but the trajectory is only based on RNA. The WNN UMAP is quite different from the RNA only UMAP and I think the RNA trajectory doesn't do justice.\nDoes anyone know of any tools/methods that can create a joint trajectory using both omics?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612219/",
    "title": "STAR aligner problem on local laptop",
    "question": "Hello everyone,\nI'm facing an issue with the STAR aligner on my personal computer.\nI have a MacBook Pro with an M3 chip and 40GB of RAM, which I believe should be more than sufficient to run this tool. I don't encounter any errors during genome indexing or mapping. However, the BAM or SAM file I get at the end is empty.\nI've tried changing many settings, but I'm starting to suspect that there may be some compatibility issues between STAR and my MacBook.\nHas anyone successfully run STAR locally on a similar setup? Do you have any idea why the output BAM files might be empty?\nThank you!",
    "answers": [
      "However, the BAM or SAM file I get at the end is empty\nIf you get a SAM file in the end but it's empty, that could mean that there aren't any reads in your FASTQ file. Trying displaying the first few lines of your FASTQ file (using the unix less or cat command) to ensure that your FASTQ file actually contains read sequences."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9596400/",
    "title": "Mageck RRA is listing negative controls as high fold change in the gene summary",
    "question": "I've been running MageckRRA and providing a list of negative control sgRNAs using --norm-method control and --control-sgrna negative_ctrls.txt\nIn my gene_summary file I see these negative controls appearing as the top high and low fold change genes. For one of the sets of samples I ran RRA on the log file includes a lot of 'Skipping gene ... for permutation ...' messages, but for another set of samples it apparently didn't do this.\nMy colleague said the negative controls should not appear in the gene summary. Are they correct? Is there something I need to change to get the negative controls out of the results? Is this likely to be a code issue or an issue of poor quality data?\nThank you!",
    "answers": [
      "Most likely, your negative control sgRNAs specified are using the wrong IDs or in the wrong format somehow.\nIf provided appropriately, you should see messages in the log like:\nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg09 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg05 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg10 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg04 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg01 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg02 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg08 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg06 for permutation ... \nINFO  @ Wed, 28 Sep 2022 16:48:26:   Skipping gene neg07 for permutation ...\nAnd:\nINFO  @ Wed, 28 Sep 2022 16:48:26:   Total # control sgRNAs: 51\nAnd yes, the control guides/genes should not be in the summary files if done properly. I've found it useful to compare the normalized counts for the negative control guides/genes between timepoints to assess how consistent they are."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612263/",
    "title": "RNA-seq analysis",
    "question": "I have paired-end FASTQ files. I used HISAT2 for alignment and got a good alignment percentage. Then, I used featurecounts for getting the raw counts for the genes. I excluded the chimeric fragments using -C command, but I see that most of my reads fall under unassigned chimera and very few are assigned. What could be the issue with it?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612241/",
    "title": "Obtaining Q30/Q60 Values from Fastq Files",
    "question": "Hello,\nI've run FASTQC on my FASTQ files and would like to extract the Q30/Q60 (Phred) Quality Scores. Does FASTQC provide this information directly, or do I need to calculate it separately? I'm unsure how to retrieve this information. Any guidance would be greatly appreciated!",
    "answers": [
      "would like to extract the Q30/Q60 (Phred) Quality Scores\nand\ndo I need to calculate it separately? I\nAs in average (which does not make a lot of sense) for the entire dataset?\nFastq format encodes scores for each base in the record for every sequence in a position that corresponds to the sequenced base. Q60 scores appear to be valid only for PacBio HiFi data otherwise max for Illumina is in the Q40-Q43 range."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9597370/",
    "title": "Phylogeny aware alignment input",
    "question": "Hello,\nI want to perform a multiple sequence alignment with a phylogeny aware program. I am curious about the number of homologous sequences I should input for the most accurate results.\nSome advice related to the subject regarding protocols used in such alignments is welcome.\nCheers,",
    "answers": [
      "I think the number of sequnences you will use to make the tree is the same as the number of input sequences in the MSA."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612239/",
    "title": "The allele with index 2 is not defined in the REF/ALT columns\" error after AF filtering with gnomAD Exome",
    "question": "I'm running a germline variant calling pipeline with bcftools, and I annotate/filter variants based on allele frequency from gnomAD exomes (gnomad.exomes.r2.1.1.sites.vcf.bgz). After filtering, I get an error in IGV when loading the VCF:\nError loading features for interval: chr1:114940611-114940651 htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 2 is not defined in the REF/ALT columns in the record\nchr1 114940632 . C CA 115.328 PASS INDEL;IDV=32;...;AC=1,1;AN=2;... GT:PL 1/2:176,86,79,72,0,93 This genotype 1/2 suggests two alternate alleles, but only one ALT allele CA is listed.\nHere are the key steps I use:\n    # 1. Variant calling\nbcftools mpileup -Ou -f genome.fa sample.bam |\nbcftools call -mv -Ou |\nbcftools filter -s LowQual -e 'QUAL<30 || DP<10' -Ov -o sample_raw.vcf\n\n# 2. Compress + index\nbgzip sample_raw.vcf && tabix -p vcf sample_raw.vcf.gz\n\n# 3. Annotate with gnomAD (AF)\nbcftools annotate \\\n  -a gnomad.exomes.r2.1.1.sites.vcf.bgz \\\n  -c CHROM,POS,REF,ALT,INFO/AF \\\n  sample_raw.vcf.gz -Oz -o sample_annotated.vcf.gz\ntabix -p vcf sample_annotated.vcf.gz\n\n# 4. Normalize (split multiallelics)\nbcftools norm -m -any sample_annotated.vcf.gz -Oz -o sample_norm.vcf.gz\ntabix -p vcf sample_norm.vcf.gz\n\n# 5. Filter by AF and quality\nbcftools view -f PASS -i 'INFO/AF<0.05' sample_norm.vcf.gz -Oz -o sample_filtered.vcf.gz\ntabix -p vcf sample_filtered.vcf.gz\n\n# 6. (Optionally) Normalize again?\nbcftools norm -m -any sample_filtered.vcf.gz -Oz -o sample_final.vcf.gz\ntabix -p vcf sample_final.vcf.gz\nI ALREADY CHECK THIS: VCF headers are intact (#CHROM, INFO, FORMAT, etc.)\nbcftools view doesn't complain about the file.\nIGV loads unfiltered/raw VCFs without issues.\nManual checks show many sites with 1/2 genotypes but only one ALT allele.\nNormalization (bcftools norm -m -any) doesn't seem to resolve the mismatch in some cases.\nI dont know where is my mistake. Any advice is appreciated, especially from anyone who's successfully integrated gnomAD Exome annotations into a clinical/exome pipeline without IGV errors!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612228/",
    "title": "Drastic drop in RNA-seq read mapping rate when disabling gaps in Bowtie2",
    "question": "Hi all,\nI have some RNA-seq reads from a grasshopper species. I mapped them to a transcriptome reference (from the same species) using Bowtie2.\nWhen I ran Bowtie2 with its default settings (allowing gaps), I observed an overall mapping rate of >95%.\nHowever, when I disabled gap alignment (i.e., using the settings RSEM uses by default when calling Bowtie2: --bowtie2 --sensitive --dpad 0 --gbar 99999999 --mp 1,1 --np 1 --score-min L,0,-0.1 --no-mixed --no-discordant), the mapping rate dropped drastically to ~18%.\nSome context:\nThe transcriptome reference was assembled from whole-body RNA of individuals collected at a different location.\nMy reads come from a specific tissue.\nI expected some reduction when no gap is allowed, but does a drop from >95% to ~18% seem too extreme? Is this expected when disallowing gaps, or could it suggest other issues?\nAny insights or suggestions would be greatly appreciated!\nThanks!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612212/",
    "title": "RNASeq bulk transcriptomics analysis",
    "question": "The commands i used for my RNASeq bulk data analysis are\n 1. trim_galore   --quality 30   --length 30   --cores 8   -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA   -a2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT   --basename CC1   --paired   --fastqc   CC1_S16_L002_R1_001.fastq.gz   CC1_S16_L002_R2_001.fastq.gz\n 2. STAR --runThreadN 8 --runMode genomeGenerate --genomeDir Index --genomeFastaFiles GRCm39.genome.fa --sjdbGTFfile gencode.vM37.basic.annotation.gtf --sjdbOverhang 149\n 3. STAR   --genomeDir /Index   --runThreadN 10   --readFilesIn CC1_val_1.fq.gz CC1_val_2.fq.gz   --outFileNamePrefix CC1_   --readFilesCommand zcat   --outSAMtype BAM SortedByCoordinate   --outSAMunmapped Within   --quantMode GeneCounts\nUsing these commands i am getting only 37%mapped and others are not...37% is way too less a number...Maybe i am making an error somewhere, can anyone please suggest what more i can include to get a better and more mapped result.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612209/",
    "title": "SSL and Syntax Errors when using fasterq-dump (SRA Toolkit)",
    "question": "Hi everyone,\nI’m having trouble downloading SRA data using fasterq-dump from the SRA Toolkit. Here is the error message I get:\nerr: /home/username/.ncbi/user-settings.mkfg:1:20: error: token='false', msg='syntax error'\nsys: connection failed while opening file within cryptographic module - mbedtls_ssl_handshake returned -9984 ( X509 - Certificate verification failed )\nsys: mbedtls_ssl_get_verify_result returned 0x4008 ( The certificate is not correctly signed by the trusted CA )\nsys: connection failed - Failed to create TLS stream for 'trace.ncbi.nlm.nih.gov'\nI suspect the main problem is with my configuration file ~/.ncbi/user-settings.mkfg where I tried to disable SSL verification by adding:\n/http/ssl/verify = false\nHowever, the SRA Toolkit complains that the value false causes a syntax error. After checking the docs, I learned that boolean values in the config file must be numeric.\nSo I changed the file to:\n/http/ssl/verify = 0\nBut I still get SSL/TLS handshake errors and connection failures.\nWhat I have tried:\nDeleting and recreating user-settings.mkfg with /http/ssl/verify = 0\nUsing prefetch with --ngc /dev/null option\nChecking my internet connection and firewall settings (which seem fine)\nEnsuring I’m running the latest toolkit version (2.9.6)\nMy thoughts and questions: Could this be caused by the NCBI server’s certificate using a hash algorithm unsupported by mbedtls on my system?\nIs there a way to bypass SSL verification or an alternative method to download SRA data?\nHas anyone found a solution for the mbedtls_ssl_handshake returned -9984 error?\nAny help or advice from those who have encountered similar issues or know how to configure SSL/TLS in SRA Toolkit would be greatly appreciated.\nThanks in advance!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612206/",
    "title": "Download GenomicSEM reference file",
    "question": "Hello everyone,\nI want to know where to download the reference file -\"reference.1000G.maf.0.005.txt\", I see many scripts mention this file, but I can not find it in anywhere of GenomicSEM github.\nThanks.",
    "answers": [
      "Sorry for that,I have found it."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612198/",
    "title": "How common is it to split fastq files prior to bwa mem to increase parallelization?",
    "question": "I’m working with 150bp paired end human whole-genome sequencing (WGS) reads, with each sample sequenced across 8 lanes. I get two fastqs for each lane, which are ~4GB a piece\nI’m currently aligning the FASTQ files in parallel using bwa mem, one process per lane. Even after increasing the -t (threads) parameter, the alignment step remains slow and typically taking around two days per sample.\nI'm considering further parallelizing by splitting each lane-based FASTQ file using a tool like seqkit split2 -p 8, and then aligning the resulting chunks in parallel. This seems like it should provide a near-linear speedup, but I’m a bit cautious since splitting introduces an extra step where things could potentially go wrong.\nIs splitting like this a common strategy for accelerating WGS alignments? Are there any caveats or best practices I should be aware of when using this approach?\nThanks",
    "answers": [
      "Splitting is a good strategy for speeding up processing. I use it all the time in my Snakemake workflows. Processing multiple files in parallel (using a couple of threads per file) is faster than processing one huge file using a large number of threads.\nI don’t think there are really any caveats aside from testing to make sure that your “split file” pipeline produces the same results as your “non-split-file” pipeline."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612203/",
    "title": "UCSC Xena Webinar Series: Visualize and Analyze Bulk Sequencing Data on Xena",
    "question": "UCSC Xena is launching an online quarterly webinar series! Each session in the series will present the same overview of Xena's core functionalities for bulk sequencing data. The first webinar will be held on:\nJune 5th, 2025 10am-12pm PT\nhttps://ucsc-xena.gitbook.io/project/tutorials/webinars\nMore dates coming soon! Sign up for updates from the newsletter: https://xena.ucsc.edu/#whatsnew\nIf you have suggestions for future webinars please contact genome-cancer@soe.ucsc.edu",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612201/",
    "title": "Deseq2 DE analysis of host-pathogen samples (model separately or jointly?)",
    "question": "Hello,\nwe're working on a differential expression (DEG) analysis using DESeq2 for a dataset involving an eukaryotic host experimentally infected with a virus.\nDataset: Our design includes comparisons across different infection treatments, between infected vs. non-infected controls, and across four time points. This leads to 16 different \"conditions\" including these three variables, and we have 4-5 biological replicates per condition after QC.\nRead quantification: We quantified transcript expression using Salmon, with a combined index that includes both host and viral transcriptomes. We used tximport to map the transcripts to host and viral genes. Across all samples, about 99% of reads map to the host.\nOur goal is to analyze differential expression in both host and viral genes. This leads to a key question:\nShould we perform DEG analysis on the combined host + virus transcriptome in DESeq2, or analyze host and viral genes separately?\nA different post here suggested that this is often the best choice unless \"the inter-sample variability (e.g. the spread of points as you could see in a PCA plot -- see vignette) is vastly different across subsets.\" This is certainly the case here, if I understand this point correctly. A PCA of vst counts separates time point 3 from everything else in PC1 (~35% var explained) when looking at this joint analysis or just the host (these two very similar) but the virus-only PCA1 looks very different (~ 84%, separating time point 1). This makes sense (biologically) to us, but we're not sure if it qualifies as one of these situations that warrants separate DE analysis.\nWe've noticed substantial differences in results depending on the approach. For example, in a particular contrast between infected treatments, we identify 172 significantly viral DEGs when analyzing the full dataset (all of them in the same direction compared to the control), but only a handful when restricting the analysis to viral genes alone.\nSubsetting to viral genes results in much smaller library sizes, and we're also considering that host and viral genes may be influenced by different biological processes or technical factors. Given these considerations, what would be the most appropriate strategy for this kind of analysis?\nThanks in advance for your insights!\nCross-posted from Bioconductor.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612197/",
    "title": "Does vg stats support gaf file?",
    "question": "Hi, I am trying to get alignment stats from a gaf file using vg stats -a <gaf> and got a long error. May I ask if I need to convert gaf to gam to use vg stats?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612189/",
    "title": "Longest Protein per Gene from gpff file",
    "question": "I have a file with gpff format containing protein sequences (downloaded from NCBI), some genes have several isoform of proteins. how can I extract longest protein per gene? Of course, I want the proteins of all genes, and some genes may only have one version of the protein.\na part of .gpff file\nLOCUS       NP_001347130            1069 aa            linear   PLN 30-JAN-2018\nDEFINITION  uncharacterized protein LOC111828501 [Oryza sativa Japonica Group].\nACCESSION   NP_001347130\nVERSION     NP_001347130.1\nDBSOURCE    REFSEQ: accession NM_001360201.1\nKEYWORDS    RefSeq.\nSOURCE      Oryza sativa Japonica Group (Japanese rice)\n  ORGANISM  Oryza sativa Japonica Group\n            Eukaryota; Viridiplantae; Streptophyta; Embryophyta; Tracheophyta;\n            Spermatophyta; Magnoliopsida; Liliopsida; Poales; Poaceae; BOP\n            clade; Oryzoideae; Oryzeae; Oryzinae; Oryza; Oryza sativa.\nCOMMENT     VALIDATED REFSEQ: This record has undergone validation or\n            preliminary review. The reference sequence was derived from\n            AK064183.1 and AP014959.1.\nFEATURES             Location/Qualifiers\n     source          1..1069\n                     /organism=\"Oryza sativa Japonica Group\"\n                     /db_xref=\"taxon:39947\"\n     Protein         1..1069\n                     /product=\"uncharacterized protein LOC111828501\"\n                     /calculated_mol_wt=121529\n     CDS             1..1069\n                     /gene=\"LOC111828501\"\n                     /coded_by=\"NM_001360201.1:90..3299\"\n                     /db_xref=\"GeneID:111828501\"\nORIGIN      \n        1 madpedaaaa aaagneddve dlyadlddqv aaalaaages ggsnpatdge aeaeapgahh\n       61 teadaneavd lgdgtagyis sdeeseddlh ivlnedgaap pppppagrce egseegevsg\n      121 scvkglstdg grgklgelhr kglfekttap itgqgdrshq hafqkefnff lprnrtvfdv\n      181 dieafqekpw rqhgvdltdy fnfgldeesw rkycfdmehf rhgtrtlane lsglqqefhy\n      241 nlglsksvpk seiysvlkeg ngiakpkgra ihveggmher lpsadmwppr qrdsdviqvn\n      301 mmfppsnrss sddrstvndk cittkrcgps nnhpgvdeyl ketssvvdrv vdkevhkrgs\n      361 sectrsktvl gdsacagaqs stpdnsdmls eestedfhfk rkrgksnsna fyvetnrkde\n      421 hvlsdfcrha sksdqesskg eshrytpspa ddryhkatkr qrmdeagaci ssrslnncqs\n      481 dhhlhesghr akkelkrqsl aggkhalfer qenttdnyss ryarkhkhkr ssstflgtny\n      541 rvhnqlcekq eylplgraal rndeqcsady nqrhrrswre inddedivgc ysarrwqqrh\n      601 ddlhgshsml kaevcddidg hmyrerryee trkirhdrng ddeffhytdy rfgkvldped\n      661 rrrcrsqsae scdehfrrse hlvfdhfthp dqlmlshqan dnhrksekgw pgpaasltfm\n      721 rsrnrfidne riqngkmkyn hdgyyekkrq hdsvfdvddi qqpalytgsv aetgqcirpv\n      781 krrvhadhsm nrkdrfnssy qkgrrlmhgw smisdrdlyv aemhnspkdi dveamcspnd\n      841 mrnsnnipni ydkirhevvn lqprdtdnml lihrkrkfkr qgieirrvve sdsegclpad\n      901 sdlhgskhkn ihqkvrkpra frisrnqase kseqqkqqhv snnqeyeeie egelieqdhq\n      961 dtasrsksnh qrkvvlksvi eassacqggv inatskdadc sngatgecdn khilevmkkm\n     1021 qkrserfkas iatqkeeded rkeslavtcd vddiknqrpa rkrlwgcsg",
    "answers": [
      "Have a look ar AGAT , with special focus on the agat_sp_keep_longest_isoform.pl subcommand.\nYou might first need to convert the genbank file to GFF (or GTF) , but there is functionality for that as well in AGAT"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9610923/",
    "title": "Modify read groups in BAM file",
    "question": "Hello, I have a set of BAM files that were generated using the DRAGEN Somatic analysis pipeline. The BAM file read groups are formatted as below:\n@RG ID:index.1  LB:UnknownLibrary   PU:1    SM:sampleID\n@RG ID:index.2  LB:UnknownLibrary   PU:2    SM:sampleID\n@RG ID:index.3  LB:UnknownLibrary   PU:3    SM:sampleID\n@RG ID:index.4  LB:UnknownLibrary   PU:4    SM:sampleID\n@RG ID:index.5  LB:UnknownLibrary   PU:5    SM:sampleID\n@RG ID:index.6  LB:UnknownLibrary   PU:6    SM:sampleID\n@RG ID:index.7  LB:UnknownLibrary   PU:7    SM:sampleID\n@RG ID:index.8  LB:UnknownLibrary   PU:8    SM:sampleID\nI want to update all the PU fields to include the flow cell ID while maintaining the other information as well as the lane indications like this:\n@RG ID:index.1  LB:UnknownLibrary   PU:XXXXXXX.1    SM:sampleID\n@RG ID:index.2  LB:UnknownLibrary   PU:XXXXXXX.2    SM:sampleID\n@RG ID:index.3  LB:UnknownLibrary   PU:XXXXXXX.3    SM:sampleID\n@RG ID:index.4  LB:UnknownLibrary   PU:XXXXXXX.4    SM:sampleID\n@RG ID:index.5  LB:UnknownLibrary   PU:XXXXXXX.5    SM:sampleID\n@RG ID:index.6  LB:UnknownLibrary   PU:XXXXXXX.6    SM:sampleID\n@RG ID:index.7  LB:UnknownLibrary   PU:XXXXXXX.7    SM:sampleID\n@RG ID:index.8  LB:UnknownLibrary   PU:XXXXXXX.8    SM:sampleID\nI am struggling to find a quick way to do this via samtools or picard but will be happy to be proven wrong. Feel free to point me to existing answers for this specific problem, I have not found one yet.\nTIA for the help.",
    "answers": [
      "I agree that the syntax and efficiency of read group handling in these tools are surprisingly convoluted.\nIt would take me many tries to get the syntax right, and I expect to find that it takes \"forever \"to run and maybe needs multiple passes.\nWhat I would do instead is bypass the whole thing, rewrite the header manually like so:\n# generate the header\nsamtools view -H alignment.bam  > head.sam\n\n# generate the body\nsamtools view alignment.bam > body.sam\nnow edit the head.sam to so the PU fields contain what you want. Then, rejoin the SAM files into BAM.\ncat head.sam body.sam | samtools view -b > out.bam\nIf some sort of automation is desired, I would write a Python program that does the above by running samtools as a process :-)",
      "As you're not modifying the records (the ID doesn't change), this is just a header change. Try samtools reheader. If you have a CRAM it's even possible to do in-place in minimal time (provided the header isn't too much larger - some extra room is allocated for growth in Samtools created cram files). For BAM you'd need to read/write the data, but it won't need decompression / recompression so should be a similar speed to unix cat."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612181/",
    "title": "Integration of NanoString CosMx FOVs",
    "question": "Hi,\nI'm working with a dataset generated using the NanoString CosMx platform with the 1000-gene panel. Since CosMx analyzes tissue samples using multiple adjacent fields of view (FOVs), I was wondering whether batch effects can occur between different FOVs, even when they are part of the same run.\nIs it possible to observe batch effects between FOVs in this context?",
    "answers": [
      "To have a more suitable answer to your question I would have a look at how the readout of RNA probes from CosMx is done.\nOn the same run, I don't see any particular reason why the capture of probes on the side of the slide would be more problematic than on the center, or the other way around, or from top to bottom.\nHowever the field is quite new, so maybe we will find out in a couple of weeks, some biais between FOVs.\nOne can also plot cells on a UMAP and check for potential clusters specific to a FOV, it will give some hits."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612183/",
    "title": "Bulk RNA-seq differential gene expression analysis on haplotype‑resolved diploid plant assembly",
    "question": "I’m working on a haplotype‑resolved diploid assembly of a plant genome, where each chromosome is represented by two FASTA/GTF pairs rather than a single consensus. I want to carry out Bulk RNA-seq count‑based differential expression analysis with Bioconductor (e.g. DESeq2, edgeR) but I’m unsure how to adapt the standard workflow for this dual‑sequence setup.\nExperimental Design (basic) Organism: Plant. Samples: 3 replicates of Condition Aand 3 replicates of Condition B. Data: Paired-end RNA-seq reads (150 bp, 30 millions reads for sample) aligned to a haplotype-resolved genome assembly. Goal: Identify DE genes between Conditions A and B, accounting for haplotype-specific expression.\nWhat I would like to know:\nShould I concatenate the two haplotype FASTAs (and GTFs) into one “merged” reference, or keep them separate and run two parallel alignments?\nHow to avoid double‑counting of genes that appear in both GTFs? Should I rename features (e.g. geneA_hap1 vs. geneA_hap2) or collapse them after quantification?\nQuantification strategy\nDoes it make sense to use allele‑aware quantifiers (e.g. Salmon with ––keepDuplicates) to retain haplotype information?\nHow to handle multi‑mapping reads that span identical exons on both haplotypes?\nCount matrix construction\nBest practice for building the count matrix:\nOption A: Separate counts per haplotype (two columns per sample) and then sum counts for downstream DE?\nOption B: Sum at the gene level before DE and ignore haplotype origin?\nOption C: Test allele‑specific expression by including haplotype as a factor in the design?\nStatistical modelling in DESeq2/edgeR\nIf I keep haplotypes separate, can I simply aggregate counts (geneA_hap1 + geneA_hap2) into a single count per sample? Any known caveats?\nIf I wish to model allele‑specific changes (e.g. hap1 vs. hap2 expression bias across conditions), what design formulas or contrasts are recommended?\nAny suggestions (reference to a published/unpublished work, practical tips, pipelines) would be much appreciated! Thanks in advance.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612175/",
    "title": "q-value threshold for omics data when performing univariate cox analysis",
    "question": "As the title informs, I'm currently working with a high dimensionality DNA methylation data. After performing batch correction, normalization and missing value imputation. I'm left with a significantly high dimensionality data, so I was looking for dimensionality reduction techniques and came across this literature. So as stated in this work I tried to perform univariate cox analysis to reduce the feature count but I'm not sure what cut-off value should I use for q-value so that I can eliminate features which are false positive and at the mean time not increasing false negatives.\nThank you for reading through my thread, if you need any additional detail please feel free to ask me. I'll respond within few minutes.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612169/",
    "title": "Cross-species differential gene expression, finding orthologs",
    "question": "I've set up a pipeline to test for differential gene expression across different species (corals).\nCross-species testing is not trivial, and my strategy is to take the raw gene counts per each species (alignment to each genome done with STAR and assembly with Stringtie) and normalize by gene length (I would extract this info from the stringtie merged.gtf). I would then find orthologs for all genes (using Orthofinder and not NCBI Homologene, as corals are not well represented there) and use the tool xSpecies described in this paper, which allows to incorporate gene homology relationships to align expression profiles between species, rather than relying solely on sequence similarity or orthology predictions (several papers instead find orthologs and just take the ones with a 1-to-1 correspondence, but this might not always be the case for all species). I would then use the normalized counts with the homology info for differential expression using limma, which can take already normalized data (Deseq does not).\nMy question is, does this approach for cross-species differential gene expression analysis look appropriate?\nAnother thing that I would like to do is compare which genes are differentially expressed and which pathways are enriched in the cross-species comparison (all corals at a control condition) with those found in within-species comparisons (each species is also exposed to a treatment) - e.g., stress gene A is upregulated in species Y compared to the other 2 species at the control condition (cross-species) and it's also upregulated in species Y with increasing temperature (within species). But would this comparison be correct? As I would use different methods to analyze the data, xSpecies+limma for the cross-comparison and Deseq for the within-comparison.\nThanks a lot!! Any feedback would be super appreciated!",
    "answers": [
      "I faced this problem previously. While I can't comment here on the relevant merits of your suggested approach, I can outline what I did. In my study I ended up:\nInvestigating differential expression within species using DESeq2\nInferring orthogroups with Orthofinder, then mapping differentially expressed genes to these orthogroups\nTesting for overlaps in differentially expressed orthogroups between species (and assessing significance)\nCross-species DEG analysis is still a bit of a wild west, but this approach returned some interesting results."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612171/",
    "title": "Unexpected negative correlation between gene-length and counts",
    "question": "I'm wondering if anyone has coming across anything like the following? I'm looking at gene-length bias in the DepMap RNA-seq counts matrix processed by the GTEx pipeline, and noticed that there is a negative relationship between gene length and counts at high gene lengths. At low gene lengths, the expected gene-length bias is seen with counts increasing with increasing gene length. At high gene lengths (>50k), this relationship inverts and counts starts decreasing with increasing gene length. Anyone seen this before or have any ideas why this may be?\nThanks :)\nLeft is pre-normalisation w/ EDASeq, right is post-normalisation.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612166/",
    "title": "Perturb seq",
    "question": "Hi I am new to single cell CRISPR analysis. I ran the sequencing files on 10x cloud and have two donors (15 & 16) with three timepoints - baseline, early and late. I have filtered h5ad files for each sample and protospacer calls per cell file for each sample. how do I assign gRNA to cell , is there a tool to this? can I run scmageck or scepter on this dataset , I don't know how to assign gRNA prior to running scmageck. is there a threshold method how does it work and does anyone have code or links to tools that do this?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612120/",
    "title": "Where can I find somatic whole-genome or exome FASTQ files (from tumor samples) with validated variants and corresponding VCFs publicly available?",
    "question": "I'm testing my somatic variant calling pipeline and I'm looking at Cancer Genome in a Bottle (GIAB) data. I found FASTQ files from the HG008-T sample (a pancreatic ductal adenocarcinoma), but they were generated using Hi-C sequencing:\nHG008-T_HiC_PhaseGenomics_20241211_R1.fastq.gz\nHG008-T_HiC_PhaseGenomics_20241211_R2.fastq.gz\nhttps://42basepairs.com/browse/web/giab/data_somatic/HG008/NIST/HG008-T_bulk/20240508p21/PhaseGenomics_HiC-ILMN_20241211\nSince Hi-C isn't ideal for small variant calling (like with Illumina, Thermo Fisher, or Nanopore WGS/WES), I was wondering:\nAre these the correct validated VCFs for that sample? https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data_somatic/HG008/Liss_lab/analysis/NIST_HG008-T_somatic-stvar_DraftBenchmark_V0.3-20250220/\nAny advice on how to proceed?",
    "answers": [
      "Check this spreadsheet.\nhttps://docs.google.com/spreadsheets/d/14lrNr9B2vCGLIBXwnrgKgZGneDVSnHd1x_tlXMrmbM8/edit?gid=0#gid=0\nSource: https://www.nist.gov/programs-projects/cancer-genome-bottle"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612164/",
    "title": "How important are allele fractions when calling variants?",
    "question": "Hello! Newbie to bioinformatics, here. I'm comparing the GATK-For-Microbes workflow and BCFTools on bacterial WGS data. Strangely, both mostly returned different variants after filtering. I recognize GATK isn't the preferred variation caller on bacterial data, but it's widely used for variation calling on microbial data – so I find the difference in results interesting.\nAnyways.... a colleague has asked me to filter out the results based on the allele fraction of each site. This has me wondering: how important is the allele fraction for a site when determining if a variant exists or not? I've noticed that, post-filtering, many of the variants have an allele fraction less than ~.5. Experimentally, why wouldn't an allele be present on most of the reads for a site that we deem a true variant?\nThank you!",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612163/",
    "title": "Two postdoctoral positions in generative AI and blood-based cancer detection, Aarhus University, Denmark",
    "question": "Two postdoctoral positions in generative AI and blood-based cancer detection, Aarhus University, Denmark\nWe have two open postdoc positions in generative AI and blood-based cancer detection (application deadline: 25 May 2025). Please share with anyone you think may be interested.\nFor more information and to apply: https://international.au.dk/about/profile/vacant-positions/job/two-postdoctoral-positions-in-generative-ai-modeling-for-blood-based-cancer-detection\nLinkedIn post: https://www.linkedin.com/posts/jakobskoupedersen_two-postdoctoral-positions-in-generative-activity-7321175362262999041-7Mfa?utm_source=share&utm_medium=member_desktop&rcm=ACoAAACM-BAByuloQPsyaFZhnyrU7QcAjrYHcs4\nBRIEF DESCRIPTION: We seek outstanding candidates to develop generative-AI-based statistical methods for blood-based cancer detection. You will apply the methods to unprecedented genome-wide circulating cell-free DNA data sets from cancer patients and healthy individuals. The data contains rich information on cancer presence, type, and epigenomic regulation. You will develop predictive statistical methods to improve our ability to detect and characterize cancer from blood and other liquid biopsies, characterize chromatin structure, and improve nucleosome maps. Candidates must have a PhD in computer science, statistics, bioinformatics, or a related discipline.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9495768/",
    "title": "Analysing Problem Seen In MEGA X",
    "question": "When I align some sequence by using muscle tool in MEGA X an error seen which say \"Application error finalizing muscle alignment: Unable to open file \"C:\\User\\ankur\\Appdata....\\MuscleResultForSelectedData13528.fas\" \" How solve this problem?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612159/",
    "title": "discrepancies in ORA results between g:Profiler (gost) and clusterProfiler (enrichGO)",
    "question": "Hi, everyone\nWhen performing enrichment analysis using ORA, even when using the same DEGs, background set, and statistical method, the results are highly discrepant between g\\:Profiler (gost) and clusterProfiler (enrichGO). Even when using genORA, the results are similar to clusterProfiler but still differ significantly from gost. The discrepancy exceeds threefold when analyzing the same gene set. What could explain this difference? Which package should be considered more reliable?\nThanks",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612160/",
    "title": "DESeq2 outliers-should I be concerned?",
    "question": "Hello,\nI ran DESeq2 for 2 projects recently. One of them ran fine but the other gave me this message:\n-- replacing outliers and refitting for 35 genes  \n-- DESeq argument 'minReplicatesForReplace' = 7  \n-- original counts are preserved in counts(dds)\nShould I be concerned? I have 35772 genes in total and DESeq2 completed without error. The only weird thing was that message.\nThank you",
    "answers": [
      "It means that DESeq2 has replaced outliers in 35 genes because they had a large Cook's distance and you had more than 7 replicates so a certain confidence could be applied that the outlier is indeed an outlier rather than a potential genuine effect or group spread. The replacement tries to denoise the fit. Nothing to worry about I would say, it's an established built-in function that exists for a long time. You can turn it off, see vignette, if you feel like so."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612127/",
    "title": "Trimmomatic command",
    "question": "The command i am using for trimming my adaptors from the sample data is\njava -jar /data/sata_data/home/Rajdeep/Rajdeep/Trimmomatic-0.39/trimmomatic-0.39.jar PE -phred33 CC1_R1.fastq.gz CC1_R2.gz CC1_R1_paired.fastq.gz CC1_R1_unpaired.fastq.gz CC1_R2_paired.fastq.gz CC1_R2_unpaired.fastq.gz ILLUMINACLIP:/data/sata_data/home/rah=jdeep/Rajdeep/Trimmomatic-0.39/adaptors/TruSeq3-PE.fa:2:30:10 MINLEN:25\nIs it correct to use this command before aligning with STAR or Hisat2?",
    "answers": [
      "Keep in mind for RNASeq\n1) if the library was done properly, it's very unlikely you have many fragments shorter than your read length. You probably won't see much adapter at all. 2) splice-aware aligners don't mind soft trimming to make an alignment. Even if you had adapters on a read, it will probably still align to the right place, and be counted as belonging to the correct gene.\nNow, if you were doing DNASeq, or do novo assembly, then removing adapters would be more important. But your typical RNASeq experiment, where all you want are gene/transcript counts? Don't agonize. It's not going to be much a problem, even if you did nothing."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612130/",
    "title": "Setaria Pan-Genome Variation: Unraveling the Story of Domestication",
    "question": "An article titled “A graph-based genome and pan-genome variation of the model plant Setaria” [1], published in Nature Genetics, de novo assembled 110 reference-grade genomes for cultivated Setaria accessions, and examined genome evolution in the context of foxtail millet domestication and improvement.\nSetaria italica (foxtail millet) is an ideal model for C4 photosynthetic crop plants. Despite its favorable traits, genomic diversity and potential for genetic enhancement remain relatively understudied. Through high throughput sequencing, 110 core-set reference-level genomes were assembled, facilitating the establishment of a complete pan-genome and the construction of a graph-based genome for Setaria. Large-scale genetic studies across 68 traits in 13 environments identified potential genes for millet improvement in diverse geographic locations. These findings can inform marker-assisted breeding, genomic selection, and genome editing to expedite crop improvement under various climatic conditions.\nBackground\nFoxtail millet (Setaria italica), as one of the oldest domesticated grain crops in the world, has excellent drought and low soil-nutrient tolerance thriving in temperate, tropical, and arid environments. Significantly, Setaria species employ C4 photosynthesis, characterized by high photosynthetic efficiency and environmental adaptability, crucial for global agricultural grain and biofuel production. Because of the complexity of most C4 crop plant genomes and the limited high-efficiency transformation systems, Setaria makes it an attractive model for the study characterized with compact diploid genomes (~420 Mb), short life cycles (~70 d), and efficient transformation systems. The study can elucidate genotyping for crop domestication and genetic improvement, paving the way for foxtail millet research and breeding, and serving as a model for 'breeding by design' in other crops.\nStudy Design and Methods\nThe genomic resequencing data for 630 wild, 829 landrace, and 385 modern cultivated accessions from the Setaria genus were collected by the researchers, with an average sequencing depth of approximately 15×. After aligning reads to the foxtail millet ‘Yugu1’ reference genome, ~60 million SNPs and 6.7 million InDels were identified. Phylogenetic and population structure analyses were conducted to reveal genetically differentiated subpopulations within wild accessions (W1-W4) and cultivated accessions (C1–C3).\nTo capture the complete spectrum of genetic diversity of Setaria which may be overlooked by short-read resequencing approaches, 110 representative Setaria accessions (35 wild, 40 landrace and 35 modern cultivated accessions) were assembled and assessed by K-mer-based analysis. PacBio and Illumina reads were used to further refine three representative genomes.\nA pan-genome of foxtail millet was constructed, and SV distribution analysis was performed. Phylogenetic analysis incorporating SVs and Presence-absence variants (PAV) frequency comparisons was conducted to elucidate domestication associations. Genome-wide selection signatures for domestication were identified using SNP data.\nTo identify seed-shattering loci, they performed quantitative trait loci (QTL) analysis and bulked segregant analysis sequencing (BSA-seq) using an RIL population. Through this approach, three major QTLs associated with seed shattering were identified. The SV-based GWAS was conducted to elucidate the relationship between SV and grain yield increase. Subsequently, overexpression experiments in foxtail millet were performed to validate the functional role of candidate genes. A graph-based reference genome of Setaria was constructed by integrating insertions, deletions and inversions across 112 accessions into the reference genome. This resource accounts for pan-genome variation and provides a valuable tool for breeding programs.\nResults and Discussion\nEvolution and De novo Assembly of Setaria: According to phylogenetic and population structure analyses using 4,934,413 high-quality SNPs, W1 subgroup emerges as the closest population to cultivated foxtail millet, suggesting it as the wild progenitor for all cultivated accessions. Among cultivated subgroups, the broad distribution of C3 subgroup worldwide implies potential adaptation to a wider range of climates. 110 representative Setaria accessions were de novo assembled and three accessions—Me34V (wild), Ci846 (landrace) and Yugu18 (modern cultivar)—were further selected to construct high-quality reference genome assemblies for Setaria. The three genome assemblies have greater contiguity than existing reference genomes, with a mean contig N50 length of >20 Mb and LTR assembly index >20.\nPan-genomic Variation in Foxtail Millet Domestication and Improvement: Comprising over 100 Setaria accessions, the pan-genome suggested 23.8% were core genes, 42.9% were soft core genes, 29.4% were dispensable and 3.9% were private genes. Most PAVs were found to overlap with transposable elements (TEs), indicating TE activity as an important mechanism for SV generation in genomes. A set of SVs was identified within promoters or gene bodies of functionally important loci, and SVs occur more frequently in genes with low expression level. This pattern mirrors observations in rice [2] and aligns with a stabilizing model of gene expression evolution, wherein lowly expressed genes are under weaker selection pressure and thus more prone association with PAVs. Similar to the studies of other crops, SVs play a crucial role in determining foxtail millet traits. This is exemplified by the study of two key domestication genes, SiGW3 and sh1.\nGraph-based Genome Facilitates Breeding of Foxtail Millet: Using the graph-based genome, researchers genotyped SVs in a large population via short-read resequencing. Subsequently, they conducted GWAS and genomic selection (GS) studies in 680 foxtail millet accessions across 68 traits in 13 distinct geographic locations with varying climatic conditions. SNPs and SVs significantly associated with diverse phenotypes were identified, enhancing genomic prediction accuracy for foxtail millet across different environments. This prediction accuracy is substantially higher than observed in tomato12 possibly due to species or trait specificity. Leveraging the graph-based genome, the potential breeding values for yield and grain quality-related traits can be estimated, offering avenues for foxtail millet breeding strategies tailored to climate change adaptation.\nReference [1] He, Q., Tang, S., Zhi, H. et al. A graph-based genome and pan-genome variation of the model plant Setaria. Nat Genet 55, 1232–1242 (2023). [2] Qin, Peng et al. “Pan-genome analysis of 33 genetically diverse rice accessions reveals hidden genomic variations.” Cell vol. 184,13 (2021).",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9610861/",
    "title": "Facing issue with output of nextflow pipeline",
    "question": "I create a nexflow pipeline to run rna-seq preprocessing.\nThis is the error i am facing. Can anyone please help me to resolve this ?\nERROR ~ Error executing process > 'FastQC (1)'\n\nCaused by:\n\n      Missing output file(s) `*` expected by process `FastQC (1)` (note: input files are not included in the default matching set)\n\nCommand executed:\n\n  mkdir -p /home/PDX_Data/data/output/fastqc\n  fastqc --threads 12 -o /home/PDX_Data/data/output/fastqc ERR1084768_1.fastq.gz ERR1084768_2.fastq.gz 2> /home/PDX_Data/data/output/fastqc/error.log\n\nCommand exit status:\n  0\n\nCommand output:\n  application/gzip\n  application/gzip\n  Analysis complete for ERR1084768_1.fastq.gz\n  Analysis complete for ERR1084768_2.fastq.gz\n\nWork dir:\n  /home/PDX_Data/data/work/a3/0ad1935f61de5cc612bae18d56a242",
    "answers": [
      "fastqc is not creating output (missing output files) which are expected by your output pattern.\nBugfixing\ncheck the fastqc work directory to see what files are being created\ntry to set the output file expected to *.html",
      "With nextflow you don't want to manage paths manually. Instead of creating a hardcoded path you should let it output the results into whatever directory it wants to, and catch them with an appropriate output declaration. Further processes should receive output by <process_name>.out or using the declared emit name. If you want to get the final results in a more convenient location the you should specify it using publishDir, preferentially using 'link' or 'symlink' modes (so you don't copy over large files).\nHere's an example. I define FlyeTest process that will assemble CLR reads using flye. Flye's outputs the assembly into <dir>/assembly.fasta where you can specify <dir> with -o option, in my case it will be asm_out/assembly.fasta, and I tell nextflow to take this file as output. publishDir will create a hard link to FlyeTest output and place it in results directory.\nA plain fromPath channel would create separate instances of FlyeTest() for each input file, with .collect() I can pass them as 1 array and .join(' ') them into a space separated string of paths.\nI pass the value for read_path parameter as: 'data/Cell-?/seq-??.fastq.gz'. ? matches any digit, hence I can have up to 10 directories within data Cell-0 to Cell-9, each with up to 100 fastq files from seq-00 to seq-99.\n    process FlyeTest {\n\n    publishDir 'results', mode: 'link'\n\n    input:\n    path read_list\n\n    output:\n    path 'asm_out/assembly.fasta'\n\n    script:\n    \"\"\"\n    flye --pacbio-raw ${read_list.join(' ')} -o asm_out --threads ${task.cpus}\n    \"\"\n}\n\nparams.reads_path = './'\nworkflow {\n    read_ch = channel.fromPath(params.reads_path)\n        .collect()\n        .view()\n    FlyeTest(read_ch)\n}\n\n\n$ nextflow run FlyeTest.nf --reads_path 'data/Cell-?/seq-??.fastq.gz'\nTo make things cleaner I suggest setting params to generally acceptable default values (and if that's not possible adding checks) and writing a separate run.sh script with nextflow run <file_name>.nf --<param_name> <param_value> ...",
      "This is a common issue that many learners encounter when working with Nextflow output management.\nUse publishDir(\"${params.outdir}/...\", mode: 'copy') to copy output files to a designated directory.\nUse stdout instead of specifying individual output files(inlucding *)—this helps avoid missing any files that are generated dynamically or unexpectedly.\n    process RNAseq_quality_check {\n    publishDir (\"${params.outdir}/01_quality_check\", mode: 'copy') \n    input:          \n        tuple val(sample_id), path(reads)\n    output:\n        stdout\n    script:\n    \"\"\"\n    mkdir -p ${params.outdir}/01_quality_check;\n    fastqc ${reads[0]} ${reads[1]} -o ${params.outdir}/01_quality_check -t 8;\n    \"\"\"\n    }"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612148/",
    "title": "Pie Charts: to use or not to use",
    "question": "Hello! I’m a bit confused and was hoping to get some clarification. In several of my statistics lectures, we were strongly advised against using pie charts to represent data. The argument was that they can easily misrepresent proportions, exaggerate differences, and generally distort interpretation—essentially, that they should be avoided altogether. In fact, making pie charts in some statistical packages is not supported or deprecated.\nHowever, I’ve noticed that pie charts are still frequently used in biology published articles, including in high-impact journals, particularly to illustrate proportions/distributions.\nGiven this, I wanted to ask: Is it ever acceptable to use pie charts to explain proportions in scientific work, or are they generally discouraged across the board? Thank you in advance :)",
    "answers": [
      "Your first paragraph is answering the latter and as said in the second paragraph, one cannot force people not to use it.\nIt is like RPKM/FPKM for read counts, it is not optimal, it skews the interpretation, better methods exist. Nevertheless, some people (not to mention old PI) are attached to it and still rely on it.\nTo bring some contrast, the only application I would see for pie charts in \"Science\" is for popularization, a pie chart talks to everyone, from a grandchild to a grandparent.",
      "As with any visualisation method, there is a time and a place for them. They are often misused, or used when a different method would be better, but I don't think there is anything inherently wrong with them if done well (3D or stacked pie charts however are entirely unnecessary)\nI use them from time to time in our biotech pitch decks (e.g. to show relative market share of different technologies), but this a more 'soft' use case."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612145/",
    "title": "Minimum Number of Cells for Downstream Analysis after subsetting a particular cell type (scRNAseq)?",
    "question": "Suppose if I have two groups and i want to look into particular subtypes say Alveolar Macrophages.\nWhat should be the minimum number of cells for the downstream analysis? I can see many reasons and numbers in the literature.\nPlease give reasons for your answer.",
    "answers": [
      "There is no answer to this. Some people will say 10, other 3, others 500. Generally, since single-cell is noisy and has dropouts, even for canonical marker genes, you want some cells to be sure that you can redundantly find expression in not just a handful of cells. Marker detection or differential expression is generally poor on single-cell level, and even more so if you just have a couple of cells. I don't think there is this one single answer to satisfy you. You also don't give any details what \"look into\" means."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612142/",
    "title": "Discrepancy Between Reported Nitrogen-Fixing Ability and Genomic Evidence in Acinetobacter guillouiae",
    "question": "Acinetobacter guillouiae has been reported to possess nitrogen-fixing genes (DOI: 10.1007/s40011-020-01168-0). However, I was unable to identify any nif genes (nifH, nifB, nifE, etc.) or nitrogenase proteins in the genomic data available on the NCBI website (https://www.ncbi.nlm.nih.gov/datasets/genome/?taxon=106649). Could I be overlooking something?",
    "answers": [
      "The specific strain described in that paper (Acinetobacter guillouiae EU-B2RT.R1) doesn't seem to be available in NCBI. It could be as simple as that. Did you test all 46 strains that can be found at NCBI for these genes?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/453527/",
    "title": "kraken2table, an analogous to kaiju2table",
    "question": "Hi everyone,\nIf you, like me, work with metagenomic data, you probably have used kaiju2table in the past. It's a tool provided with the Kaiju source code. It produces tsv tables that can easily be handled later on, for example for plotting.\nThe input data is the classic output format of Kaiju and also of Kraken:\nC   A00700:50:HF7LGDRXX:1:1101:1000:10144#CCGGCATCATCTACGA  1578    100 1578:66\nC   A00700:50:HF7LGDRXX:1:1101:1000:19225#CCGGCATCATCTACGA  186802  100 0:11 186802:55\nC   A00700:50:HF7LGDRXX:1:1101:1000:23234#CCGGCATCATCTACGA  1578    100 1578:66\nAnd can be from as many files as you need, which will be combined in one file containing percentages, like this:\nfile            percent             reads    taxon_id  taxon_name\nF14_A_R1.s.out  59.89815343509682   7161673  0         Unclassified\nF14_A_R1.s.out  1.080231644647389   129157   1301      Streptococcus\nF14_A_R1.s.out  2.129960840275143   254667   1350      Enterococcus\nF14_A_R1.s.out  1.3252716093792982  158455   1485      Clostridium\nF14_A_R1.s.out  6.908532882384414   826013   1578      Lactobacillus\nF14_A_R1.s.out  1.880613565083921   224854   204475    Gemmiger\nF14_A_R1.s.out  3.163456075511585   378236   572511    Blautia\nF14_A_R1.s.out  1.4769725746433902  176593   946234    Flavonifractor\nF14_A_R1.s.out  1.0168598167829042  121580   1017280   Pseudoflavonifractor\nAs far as I could find, there is no such tool made for Kraken2, which is perhaps more used than Kaiju as a tool. You could, of course, try to use kaiju2table with the kraken results, but you would have to install Kaiju to have it.\nHence, for my own convenience I have made a tool called kraken2table that converts the *.out files produced by Kraken2 (mpa format) to *.tsv tables that resemble those produced by kaiju2table.\nYou can find it here: https://github.com/MatteoSchiavinato/Utilities/blob/master/kraken2table\nIt depends on:\nete3\ndask[complete]\nThe options are quite simple:\nusage: kraken2table [-h] -i [INPUT_FILES [INPUT_FILES ...]] -o OUTPUT_FILE\n                    [-p THREADS] [-r RANK] [-m MIN_FRAC] [-c MIN_COUNT] [-u]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i [INPUT_FILES [INPUT_FILES ...]], --input-files [INPUT_FILES [INPUT_FILES ...]]\n                        Name of input files (SPACE-separated).\n  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n                        Name of output file.\n  -p THREADS, --threads THREADS\n                        Number of parallel threads\n  -r RANK, --rank RANK  Taxonomic rank to be output, all lowercase (Default:\n                        species)\n  -m MIN_FRAC, --min-frac MIN_FRAC\n                        Number in [0, 100], denoting the minimum required\n                        percentage for the taxon (except viruses) to be\n                        reported (default: 0.0)\n  -c MIN_COUNT, --min-count MIN_COUNT\n                        Integer number > 0, denoting the minimum required\n                        number of reads for the taxon (except viruses) to be\n                        reported (default: 0)\n  -u, --exclude-unclassified\n                        Unclassified reads are not counted for the total reads\n                        when calculating percentages for classified reads.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/487151/",
    "title": "Help needed with LOHHLA pipeline",
    "question": "Hi,\nI am running LOHHLA pipeline on some cancer samples. I have successfully set up the pipeline at my end with all the dependencies installed. I was also able to run the pipeline using the example dataset.\nNow for my data, I have prepared all the required files. But after running the pipeline, I get an error that relates to count.events function in the code. Here is the error line from the terminal.\nError in editDistance - indelTotals :\n  non-numeric argument to binary operator\nCalls: count.events\nExecution halted\nHere is the snippet from the code which belongs to this count.events function -\ncount.events <- function(BAMfile, n){\n  x              <- scanBam(BAMfile, index = BAMfile, param=ScanBamParam(what = scanBamWhat(), tag = 'NM'))\n  readIDs        <- x[[1]][['qname']]\n  cigar          <- x[[1]][['cigar']]\n  editDistance   <- unlist(x[[1]][['tag']])\n  insertionCount <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'I', x = unlist(strsplit(boop, split = '')))))} )\n  deletionCount  <- sapply(cigar, FUN = function(boop) {return(length(grep(pattern = 'D', x = unlist(strsplit(boop, split = '')))))} )\n  indelTotals    <- sapply(cigar, FUN = function(boop) {\n    tmp <- unlist(strsplit( gsub(\"([0-9]+)\",\"~\\\\1~\",boop), \"~\" ))\n    Is  <- grep(pattern = 'I', x = tmp)\n    Ds  <- grep(pattern = 'D', x = tmp)\n    total <- sum(as.numeric(tmp[(Is-1)])) + sum(as.numeric(tmp[Ds-1]))\n    return(total)\n  })\n  misMatchCount <- editDistance - indelTotals\n  eventCount <- misMatchCount + insertionCount + deletionCount\n  names(eventCount) <- 1:length(eventCount)\n  passed     <- eventCount[which(eventCount <= n)]\n  y <- readIDs[as.numeric(names(passed))]\n  y <- names(table(y)[which(table(y) == 2)])\n  return(y)\n}\nI tried to debug this error and checked the BAM file for the presence of NM tags which are there in the file. I noticed that the editDistance variable is NULL and indelTotals variable has List class.\nCan anyone please help me with this issue?",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9609844/",
    "title": "Impact of the number of PCs on the clustering in scRNA seq",
    "question": "Hello all,\nI am working with a scRNA-seq dataset. I apply a PCA and select two different number of PCs (10 and 20). Then I apply a Louvain clustering on the reduced space with a fixed resolution (0.3) and I compare the two clusterings. I get more clusters when I selected 10 PCs than when I selected 20 PCs.\nI wonder why is that and I would therefore appreciate any hint!\nThank you very much :)\n(I join the scree plot showing the percentage of variance explained by each PC)\nAlong with the two UMAP showing the clustering.",
    "answers": [
      "As much as it might feel like an unsatisfactory answer, this totally depends on your downstream needs and what meaningful biology you can attach to the clustering. Some of these clusters will be based on technical variation, some will be QC-associated, some will associate with cell cycle while others will be driven by some effect based on a set of differential genes. Find out what markers are driving the clusters and you'll get a much better understanding of the clustering 'success' yourself.\nWhen gauging the success of graphical clustering the first question to ask is how meaningful are the clusters and you do that by looking at the marker genes for that cluster and see if you can make sense of it.\nFor your larger UMAP (bottom right), for example, I'd be quite interested in finding out the set of genes that would resolve cluster 2 (green) and cluster 8 (blue). I would equally be interested in understanding the stripe running down the middle of cluster 1 (orange), cluster 0 (dark blue) and cluster 5 (brown).\nSelecting the optimal number of PCs is not a simple question and typically the most practical solution is to iterate a few times and see if you get better resolution with a larger input set.\nSince you're already on python, you can look into tools like cNMF which allow you to optimise feature selection more readily than standard graphical clustering approaches. However, I would storngly recommend exploring your data a bit more before moving on with extra technical tools. Once you've convinced yourself you understand what is going on, you'll be in a much better position to judge these kinds of things yourself."
    ]
  },
  {
    "url": "https://www.biostars.org/p/482158/",
    "title": "ATAC-seq +4 -5 shift",
    "question": "Dear all,\nI saw having the mapped reads have +4 and -5 shift in ATAC-seq is a common practice.\nSome place says \"reads should be shifted + 4 bp and − 5 bp for positive and negative strand respectively, to account for the 9-bp duplication created by DNA repair of the nick by Tn5 transposase and achieve base-pair resolution of TF footprint and motif-related analyses\"\nSome place says:\" When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site.\"\nI'm a little bit confused. Are shifting mainly to center the peak or avoid the duplication?\nDoes anyone have a good illustration on this? What will happen to the peak calls if this step is skipped?\nThank you!",
    "answers": [
      "I illustrated the molecular biology here: http://guertinlab.org/wp-content/uploads/2021/01/Tn5_illumina_adapters_mjg_2.pdf\nwe skip this step for peak calling--it really only matters for looking at the data at single nucleotide resolution composite profiles.",
      "The shifting isn't for any real purpose unless you want to plot the exact cut location (e.g., when searching for motifs), it simply harkens back to one of the first ATAC-seq papers where they performed this adjustment to account for the 9-base single-stranded over-hang on each end of the fragment. Papers since have simply followed suite. A vastly more sensible strategy would be to use the 9 bases on each end of the fragment, since these are bases that are necessarily open.",
      "Turns out +4/-4 is the correct shift. You can see that when you make a bigwig separately from the + and - strands, then the reads align with each other for +4/-4 and it's off by one for +4/-5. The main idea is that transposition events should be mapped to the same base pair regardless of strand."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612994/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612991/",
    "title": "Snakemake Module Error: string indices must be integers",
    "question": "Hey, everyone! I'm having struggles importing a module in Snakemake. When I run Snakemake -np in the directory outside of the following snakefile:\nimport subprocess\nfrom pathlib import Path\nfrom snakemake.utils import min_version\nmin_version(\"6.0\")\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nmain_dir = \"results\"\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\nmodule haplo_call:\n    snakefile: github(\"alemanac/GATK_haplotype_module\", path=\"workflow/snakefile\", tag=\"main\")\n    config: config_path\n\nuse rule all from haplo_call as haplo_call with:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample'])\nI get the following error:\nTypeError in file https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile, line 7:\nstring indices must be integers\n  File \"/home/ac.aleman/Bacterial_GATK_SNPs_aleman/workflow/snakefile\", line 36, in <module>\n  File \"https://raw.githubusercontent.com/alemanac/GATK_haplotype_module/main/workflow/snakefile\", line 7, in <module>\nI'm unsure how to resolve this. Here's the snakefile of the imported module:\nimport subprocess\nfrom pathlib import Path\n\nconfig_path = \"workflow/config/config.yaml\"\nconfigfile: config_path\n\nprint(config)\nreference_fasta = Path(config['ref_fna']).name\nref_base = Path(reference_fasta).with_suffix('')\nmain_dir = f\"results/{config['run_name']}\"\n\nwildcard_constraints:\n    SRR = r\"[R-S0-9]{10}\"\n\ninclude: \"rules/alignment.smk\"\ninclude: \"rules/alignment_shifted.smk\"\ninclude: \"rules/ref_processing.smk\"\ninclude: \"rules/haplotype_caller.smk\"\n\nrule all:\n    input:\n        expand(\"{main_dir}/{SRR}/lifted_over_and_combined_vcfs/variants.final.vcf\", main_dir=main_dir, SRR=config['test_sample']),\n        expand(\"{main_dir}/copied_config.yaml\", main_dir=main_dir)\n\nrule copy_config:\n    output:\n        copied_config = \"{main_dir}/copied_config.yaml\"\n    params:\n        config_path = config_path\n    shell:\n        \"\"\"\n        cp {params.config_path} {output.copied_config}\n        \"\"\"\nThe module runs fine on its own. Unsure what to do. I would appreciate any help! TY!",
    "answers": [
      "I've resolved this by passing the config file via\nmodule haplo_call:\nsnakefile: \"/home/ac.aleman/git/GATK_haplotype_module/workflow/snakefile\"\nconfig: yaml.safe_load(open(config[\"other_workflow\"]))\nI feel like this isn't intended, though? Is there a better way of going about this?"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612989/",
    "title": "Where to find expected copy number for a gene",
    "question": "Is there a reference database where I can find the expected copy number of a given gene in the human germline genome?\nIt is my understanding that most human genes have 1 copy on each chromosome. However, there are genes that are commonly deleted or duplicated.\nIn other organisms I know elephants have many copies of TP53.",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612988/",
    "title": "ATAC seq: from peaks to differential analysis ??",
    "question": "Hi,\nI’m fairly new to ATAC-seq and have successfully run MACS2 separately for each of my samples. I now have individual *.narrowPeak files as output.\nMy experimental design looks like this:\nSample_ID     Cell_type     Condition     Donor\n\nSample_1      T_cells       Tumor         Donor_1\nSample_2      T_cells       Normal        Donor_1\nSample_3      T_cells       Tumor         Donor_2\nSample_4      T_cells       Normal        Donor_2\n...\nSample_11     Dendritics    Tumor         Donor_10\nSample_12     Dendritics    Normal        Donor_10\nSample_13     Dendritics    Tumor         Donor_11\nSample_14     Dendritics    Normal        Donor_11\nAs you can see, I have two cell types (T_cells and Dendritics), and for each donor, I have paired Tumor and Normal samples.\nMy goal is to perform a differential accessibility analysis (Tumor vs Normal), accounting for both Donor and Cell_type. I’m also interested in comparing Tumor (T_cells) vs Tumor (Dendritics).\nI heard that it is possible to use DESeq2 for ATAC-seq data, so my design will look like this: https://bioconductor.org/packages/3.21/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#group-specific-condition-effects-individuals-nested-within-groups\nI have a few questions:\n1) Can I use the exact same code from the vignette, as typically done for RNA-seq data? Or are there any parameters or steps specific to ATAC-seq that I should consider?\n2) I’m struggling with how to convert my individual *.narrowPeak files into a count matrix. Do you have any recommendations or tools to help with this step?\n3) Are there alternative methods to DESeq2 that would be better suited for this kind of analysis? I guess limma should work the same no ?\nThank you in advance for your help !",
    "answers": [
      "2:\n# make saf file:\nawk 'OFS=\"\\t\" {print $1\":\"$2+1\"-\"$3, $1, $2+1, $3, \"+\"}' ${sample}_peaks.narrowPeak > featureCounts_peaks.saf\n\n$featureCounts -a featureCounts_peaks.saf \\\n    -F SAF \\\n    --read2pos 5 \\\n    -p \\\n    -o ${peak_all_dir}${sample}_countMatrix.txt ${rmBL_dir}*.bam\n${rmBL_dir}*.bam: all bam files for generating narrowPeaks\n3: DEseq2 is oke. design matrix and param depends on research question, not on the ATAC-seq or RNA-seq as they are all readcount type",
      "I also recommend converting the peak to SAF file as QX shows, although I just count fragments centered on cut site. There's little difference either way though.\nFor more guidance on creating a count matrix, you do need to create a common set of peaks first. Two main ways to do this, either take the intersection of peaks present in all samples or the union.\nI use bedtools, e.g.\n# N just represents number of total samples.\nbedtools mutliinter -i ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | awk '$4 == N' | bedtools sort -i - | bedtools merge -i - > All_Samples_Intersection.bed\nOr,\ncat  ${PEAKS_1} ${PEAKS_2} ... ${PEAKS_N} | bedtools sort -i - | bedtools merge -i - > All_Samples_Union.bed\nThen you can convert that bed into SAF and use featurecounts to count reads from each sample and treat similar to RNA-seq.\nOne thing I would suggest, I usually use a low count filter before moving forward with DE analysis. With ATAC-seq, there's usually more noise (I use relatively lax peak calling), so I tend to use a higher count threshold."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612987/",
    "title": "CUT&RUN analysis pipeline",
    "question": "Hello, My lab has generated data using the CUT&RUN protocol. Unfortunately, Its single-end data. I tried the CUT&RUN tools and the henipipe tool. Both are for paired end data.\nAny suggestion how I can analyse these data ? Any idea how to set the parameters in case I use the classical Trimmomatic and bowtie2 ? Any other pipelines to try ?\nThanks and Regards. (Loosing my mind)",
    "answers": [
      "You don't need to do anything too special. Trim adapters with your favorite software, align to genome using BWA-MEM or bowtie2, and call peaks using MACS2.\nMost of the fancy stuff with CUT&RUN is when you have paired end data. With paired end data you get information on insert/fragment size, which can be used to infer nucleosomal and subnucleosomal fragment, which in turn lets you guess whether afragment was from nucleosome protection or TF binding."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612985/",
    "title": "Cut&Run replicates handling",
    "question": "Is there a CUT&RUN peakcaller that supports replicates? If not, what is your recommended approach for handling replicates?\nIt appears that all available peak callers, such as SEACR, are designed to call peaks from single pull-down experiments.\nOur Experiment: We have two conditions, Untreated (UTR) and Treatment (TREAT), each with two biological replicates. Each condition also has respective INPUT data for normalization.\nWe have completed:\nQuality Control (QC)\nAlignment\nDuplicate marking/removal\nSpike-in Calibration\nNext, we would like to proceed to peak calling and differential analysis. We would appreciate your recommendations on suitable peak callers and the best practices for handling replicates.",
    "answers": [
      "You can use nf-core cutandrun pipeline, it supports replicates in the step of consensus peaks calculations."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612984/",
    "title": "Unable to extract count matrices from multi-layered Seurat object",
    "question": "Hello, I have a merged R object with 18 samples\nall_combined <- merge(\n  sample_1MI1_so,\n  y = c(\n    sample_1MI3_so,\n    sample_2MI1_so,\n    sample_2MI3_so,\n    sample_3C_MI2_so,\n    sample_3MI1_so,\n    sample_3MI3_so,\n    sample_4D_MI2_so,\n    sample_4MI1_so,\n    sample_4MI3_so,\n    sample_5MI1_so,\n    sample_5MI3_so,\n    sample_6MI1_so,\n    sample_6MI3_so,\n    sample_7MI1_so,\n    sample_7MI3_so,\n    sample_8MI1_so,\n    sample_8MI3_so\n  ),\n  add.cell.ids = c(\n    \"sample_1MI1_so\",\n    \"sample_1MI3_so\",\n    \"sample_2MI1_so\",\n    \"sample_2MI3_so\",\n    \"sample_3C_MI2_so\",\n    \"sample_3MI1_so\",\n    \"sample_3MI3_so\",\n    \"sample_4D_MI2_so\",\n    \"sample_4MI1_so\",\n    \"sample_4MI3_so\",\n    \"sample_5MI1_so\",\n    \"sample_5MI3_so\",\n    \"sample_6MI1_so\",\n    \"sample_6MI3_so\",\n    \"sample_7MI1_so\",\n    \"sample_7MI3_so\",\n    \"sample_8MI1_so\",\n    \"sample_8MI3_so\"\n  ),\n  merge.data = TRUE\n)\nView object\nall_combined\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 55 layers present: counts.1, counts.2, counts.3, counts.4, counts.5, counts.6, counts.7, counts.8, counts.9, counts.10, counts.11, counts.12, counts.13, counts.14, counts.15, counts.16, counts.17, counts.18, data.1, scale.data.1, data.2, scale.data.2, data.3, scale.data.3, data.4, scale.data.4, data.5, scale.data.5, data.6, scale.data.6, data.7, scale.data.7, data.8, scale.data.8, data.9, scale.data.9, data.10, scale.data.10, data.11, scale.data.11, data.12, scale.data.12, data.13, scale.data.13, data.14, scale.data.14, data.15, scale.data.15, data.16, scale.data.16, data.17, scale.data.17, data.18, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nIn the all_combined object, I', trying to extract the counts matrices from it, but I'm not sure how to do this? Below is what I've tried\nall_combined_count_matrix <- LayerData(object = all_combined, assay = \"RNA\", layer = \"counts\")\nWarning: multiple layers are identified by counts.1 counts.2 counts.3 counts.4 counts.5 counts.6 counts.7 counts.8 counts.9 counts.10 counts.11 counts.12 counts.13 counts.14 counts.15 counts.16 counts.17 counts.18\n only the first layer is used\n\n# only 1st sample counts matrix is extracted....want to extract all 18 counts matrices...all_combined object contains 18 seurat objects merged into one\n\n`\nManually combine all 18 count matrices into one\n# List all layers that start with \"counts\"\nall_counts_layers <- Layers(all_combined[[\"RNA\"]])\nall_counts_layers <- counts_layers[grepl(\"^counts\", all_counts_layers)]\n\n# # view all_count_layers\n#  [1] \"counts.1\"  \"counts.2\"  \"counts.3\"  \"counts.4\"  \"counts.5\"  \"counts.6\"  \"counts.7\"  \"counts.8\"  \"counts.9\"  \"counts.10\" \"counts.11\" \"counts.12\" \"counts.13\" \"counts.14\"\n# [15] \"counts.15\" \"counts.16\" \"counts.17\" \"counts.18\"\n\n\n\n# Extract each layer and combine\nall_count_matrices <- lapply(all_counts_layers, function(layer) {\n  LayerData(all_combined, assay = \"RNA\", layer = layer)\n})\n\n\n# View all_count_matrices (prints out all 18 count matrices. NOTE: each matrices has different number of rows/genes and different number of columns. However, total rows adds up to 23,447 genes and total columns add to 135,704 cells)\n# all_count_matrices\n\n\n\n# Combine into one gene x cell matrix\nall_combined_counts <- do.call(cbind, all_count_matrices)\n\n# Error in cbind.Matrix(x, y, deparse.level = 0L) : \n#   number of rows of matrices must match\nAny advice on how to do this effectively would be greatly appreciated. I'm using Seurat v5.3.0",
    "answers": [
      "Alright, I fixed the issue by joining the counts layers in all_combined. I saved it to a new seurat object. Hopefully this helps someone else!\nall_combined_join_layers <- JoinLayers(all_combined)\nThis joins the 18 counts layers into one layer (and also the 18 normalized data layers)\nall_combined_join_layers\nAn object of class Seurat \n23477 features across 135704 samples within 1 assay \nActive assay: RNA (23477 features, 2000 variable features)\n 21 layers present: data, counts, scale.data.1, scale.data.2, scale.data.3, scale.data.4, scale.data.5, scale.data.6, scale.data.7, scale.data.8, scale.data.9, scale.data.10, scale.data.11, scale.data.12, scale.data.13, scale.data.14, scale.data.15, scale.data.16, scale.data.17, scale.data.18, scale.data\n 2 dimensional reductions calculated: pca, umap\nI then extracted the counts matrix from all_combined_join_layers\nall_combined_count_matrix_2 <- LayerData(object = all_combined_join_layers, assay = \"RNA\", layer = \"counts\")\n\nall_combined_count_matrix_2\n\n23477 x 135704 sparse Matrix of class \"dgCMatrix\"\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n  [[ suppressing 75 column names 'sample_1MI1_so_AAACCCAAGAGACAAG-1', 'sample_1MI1_so_AAACCCAAGTCCCGAC-1', 'sample_1MI1_so_AAACCCAAGTGAGGCT-1' ... ]]\n\nENSMMUG00000023296 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ......\nZNF692             . . . . . . . . . 1 . . . . . 1 2 . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . ."
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612983/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612982/",
    "title": "usage of ChromHMM and Segway",
    "question": "I want to use ChromHMM and Segway to identify chromatin states or genomic segmentation based on existing models. Where are these models stored and what form these models are, and how can I use them for direct calculations? ChromHMM(https://ernstlab.github.io/ChromHMM/); Segway(https://segway.hoffmanlab.org/)",
    "answers": []
  },
  {
    "url": "https://www.biostars.org/p/9612980/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612979/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612978/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612977/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612976/",
    "title": "Learning Nextflow",
    "question": "Hello and hope all is well. I would like to know if there is someone among us who can provide me with resources to learn Nextflow apart from what Nf-Core offers.\nI'm looking to write my bash scripts in Nextflow in order to run them with Nextflow for fast, reproducible and efficient analysis.\nThanks !",
    "answers": [
      "Hi there,\nHere is a good list of resources you can use:\nhttps://github.com/nextflow-io/awesome-nextflow",
      "I would recommend going directly to the Nextflow training: https://training.nextflow.io/ The hello nextflow series is I think pretty good (but I am slightly biased ;-))",
      "Here the nice ressources I have collected\nNextflow Training from Fundamentals to Advanced The offical trainings from nextflow.io.\nNextflow workshop for beginners - by vagkaratzas\nnf-core Training - by Zemzemfiras1\nSoftware Carpentry Nextflow training. High quality course made by the Software Carpentry.\nnextflow-training - by Aubin THOMAS IGH Montpellier"
    ]
  },
  {
    "url": "https://www.biostars.org/p/9612975/",
    "title": "phage sequence no alignment to genome",
    "question": "I am currently working with a bacterial strain (let’s call it Strain A) for which I have a complete genome assembly (StrainA_genome.fasta). From this strain, I obtained two sets of phage-related sequences using different experimental approaches:\n1. An assembled phage genome from lab experiments: StrainA_phage_gapfilled.fasta\n2. A pair of short-read sequencing files from another phage preparation: StrainA_phage_R1.fasta and StrainA_phage_R2.fasta\nSeparately, I predicted prophage regions on StrainA_genome.fasta using Cenote-Taker 3 and PhageBoost, and collected all the predicted prophage sequences into a file: StrainA_predicted_phages.fasta.\nMy goal is to verify whether the experimentally derived phage sequences correspond to any of the predicted prophage regions, i.e., to confirm that the experimentally recovered sequences truly originate from prophages within Strain A.\nWhat I tried\nI attempted various alignments, including:\nI used bwa mem to align both the raw phage reads and the gap-filled phage assembly against the predicted prophage regions, using commands like:\n # Align short reads to predicted phages \n  bwa mem -t 4 StrainA _predicted_phages.fasta StrainA_phage_R1.fasta StrainA_phage_R2.fasta > phage_reads_vs_predicted.sam\n\n # Align assembled phage genome to predicted phages\n  bwa mem -t 4 StrainA_predicted_phages.fasta StrainA_phage_gapfilled.fasta > phage_assembly_vs_predicted.sam\nnone of the reads or assemblies mapped to any of the predicted prophage sequences.\nI then aligned the same experimental phage sequences (both reads and the assembled genome) directly to the full bacterial genome (StrainA_genome.fasta), expecting at least partial matches in the prophage regions. However, there were still no alignments.\n  # Also tried aligning both to the host genome:\n  bwa mem -t 4 StrainA_genome.fasta StrainA_phage_gapfilled.fasta > phage_vs_host.sam\nThis was very unexpected — if the prophages are indeed part of the genome, I would expect at least some reads to map to those regions.\nI also tried the same comparisons using Minimap2/bowtie2/blast, even tuned parameters to allow more mismatches and gaps. Unfortunately, there were no alignments..\nTo verify the identity of the experimentally obtained phage sequences, I submitted StrainA_phage_gapfilled.fasta to NCBI ORF Finder, extracted several of the longer predicted protein sequences, and then ran BLASTp against the virus (taxid:10239) database. These searches returned high-confidence hits to known phage proteins, supporting the idea that the experimental sequences are indeed phage-derived.\nAdditionally, I extracted CRISPR spacer sequences from Strain A using CRISPRCasTyper, and tried to align them (using BLAST) to the gap-filled experimental phage genome. Only one spacer hit was found, with a relatively low score (e-value ~1e-2).\nI am now wondering:\n1. Could this complete lack of alignment be caused by my alignment strategy?\nAre there better tools or methods specifically suited to phage–prophage or phage–host genome comparisons?\n2. Could experimental artifacts explain this?\nI do not perform the experimental work myself. Could the phage DNA isolation,amplification, or assembly methods introduce chimerism or unrelated sequences that mislead mapping?\n3. Or is it genuinely common to see such divergence between experimentally isolated phages and prophage predictions from the same strain genome?\nAny thoughts or suggestions on improving the alignment strategy or interpreting these results would be greatly appreciated.",
    "answers": []
  }
]