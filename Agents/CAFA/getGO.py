import torch
from transformers import AutoTokenizer, AutoModel
from goatools.obo_parser import GODag
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
import re
from collections import defaultdict
import chromadb
from chromadb.config import Settings

# ====== 1. åŠ è½½ BioBERT ======
MODEL_NAME = "dmis-lab/biobert-base-cased-v1.1"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)

def encode_text(text: str):
    """è¿”å›æ–‡æœ¬çš„å¹³å‡æ± åŒ– embedding"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
    last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden]
    embedding = last_hidden.mean(dim=1).squeeze().numpy()
    return embedding

def split_sentences(text: str):
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    # ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦åˆ†å‰²å¥å­
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]+', text)
    # è¿‡æ»¤ç©ºå¥å­å’Œå¤ªçŸ­çš„å¥å­
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    return sentences

# ====== 2. åˆå§‹åŒ– ChromaDB ======
def init_chromadb():
    """åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ"""
    client = chromadb.Client(Settings())
    collection_name = "go_terms_embeddings"
    
    try:
        collection = client.get_collection(name=collection_name)
        print(f"å·²åŠ è½½ç°æœ‰é›†åˆ: {collection_name}")
        return collection
    except:
        print(f"åˆ›å»ºæ–°é›†åˆ: {collection_name}")
        collection = client.create_collection(name=collection_name)
        return collection

# ====== 3. è§£æ GO æœ¬ä½“ ======
import os

def load_cafa_go_terms(cafa_dir="Agents/CAFA"):
    """å°è¯•åŠ è½½CAFAæä¾›çš„GO termsæ–‡ä»¶"""
    # åªæ£€æŸ¥go.txtæ–‡ä»¶
    go_file = os.path.join(cafa_dir, "go.txt")
    
    if os.path.exists(go_file):
        print(f"å‘ç°CAFA GO termsæ–‡ä»¶: {go_file}")
        return parse_cafa_go_file(go_file)
    
    print("æœªæ‰¾åˆ°CAFA go.txtæ–‡ä»¶ï¼Œä½¿ç”¨go-basic.obo")
    return None

def parse_cafa_go_file(file_path):
    """è§£æCAFAçš„OBOæ ¼å¼GO termsæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        print("æ­£åœ¨è§£æOBOæ ¼å¼æ–‡ä»¶...")
        return parse_obo_format(lines)
        
    except Exception as e:
        print(f"è§£æCAFA GO termsæ–‡ä»¶å¤±è´¥: {e}")
        return None

def parse_obo_format(lines):
    """è§£æOBOæ ¼å¼çš„GO termsæ–‡ä»¶"""
    go_terms = {}
    current_term = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if line == '[Term]':
            # å¼€å§‹æ–°çš„term
            if current_term:
                # ä¿å­˜å‰ä¸€ä¸ªterm
                go_terms[current_term['id']] = create_go_term_object(current_term)
            current_term = {'id': '', 'name': '', 'namespace': '', 'definition': ''}
        elif line.startswith('id: '):
            current_term['id'] = line[4:].strip()
        elif line.startswith('name: '):
            current_term['name'] = line[6:].strip()
        elif line.startswith('namespace: '):
            current_term['namespace'] = line[11:].strip()
        elif line.startswith('def: '):
            current_term['definition'] = line[5:].strip()
    
    # ä¿å­˜æœ€åä¸€ä¸ªterm
    if current_term and current_term['id']:
        go_terms[current_term['id']] = create_go_term_object(current_term)
    
    print(f"æˆåŠŸè§£æ {len(go_terms)} ä¸ªOBOæ ¼å¼çš„CAFA GO terms")
    return go_terms



def create_go_term_object(term_data):
    """åˆ›å»ºGO termå¯¹è±¡"""
    class SimpleGOTerm:
        def __init__(self, go_id, name, namespace, definition=""):
            self.id = go_id
            self.name = name
            self.namespace = namespace
            self.definition = definition
            self.children = []  # ç®€åŒ–å¤„ç†ï¼Œä¸è§£æå±‚æ¬¡å…³ç³»
    
    return SimpleGOTerm(
        term_data['id'], 
        term_data['name'], 
        term_data['namespace'], 
        term_data.get('definition', '')
    )

# å°è¯•åŠ è½½CAFA GO termsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨go-basic.obo
cafa_go = load_cafa_go_terms()
if cafa_go:
    go = cafa_go
    print("ä½¿ç”¨CAFAæä¾›çš„GO terms")
else:
    from goatools.base import download_go_basic_obo
    go = GODag(download_go_basic_obo())
    print("ä½¿ç”¨go-basic.obo")

# ç­›é€‰GO terms
def filter_go_terms(go_dict, min_freq=1, exclude_iea=False, max_terms_per_namespace=1000):
    """ç­›é€‰GO termsï¼Œä½¿ç”¨åŸºäºé‡è¦æ€§å’Œè´¨é‡çš„ç­›é€‰ç­–ç•¥"""
    filtered_terms = {}
    namespace_counts = {'biological_process': 0, 'molecular_function': 0, 'cellular_component': 0}
    
    # æŒ‰namespaceåˆ†ç»„å¹¶è®¡ç®—ä¼˜å…ˆçº§
    namespace_terms = {
        'biological_process': [],
        'molecular_function': [], 
        'cellular_component': []
    }
    
    for go_id, term in go_dict.items():
        # 1. åªä¿ç•™BP, MF, CCä¸‰ç§ç±»å‹
        if term.namespace not in ['biological_process', 'molecular_function', 'cellular_component']:
            continue
            
        # 2. è®¡ç®—ç»¼åˆä¼˜å…ˆçº§åˆ†æ•°
        priority_score = 0
        if hasattr(term, 'definition') and term.definition.strip():
            priority_score += 2  # æœ‰definitionçš„+2åˆ†
        
        # 3. åŸºäºGO IDçš„å±‚æ¬¡æ·±åº¦è¿›è¡Œç­›é€‰ï¼ˆæµ…å±‚termsé€šå¸¸æ›´é‡è¦ï¼‰
        # GO:0000001 æ¯” GO:0000001.0000001 æ›´é‡è¦
        if '.' not in go_id:
            priority_score += 3  # é¡¶çº§terms +3åˆ†
        elif go_id.count('.') == 1:
            priority_score += 2  # äºŒçº§terms +2åˆ†
        elif go_id.count('.') == 2:
            priority_score += 1  # ä¸‰çº§terms +1åˆ†
        
        # 4. åŸºäºåç§°é•¿åº¦ï¼ˆåç§°å¤ªé•¿çš„å¯èƒ½è¿‡äºå…·ä½“ï¼‰
        name_length = len(term.name)
        if name_length < 50:
            priority_score += 1  # åç§°ç®€æ´çš„+1åˆ†
        
        namespace_terms[term.namespace].append((go_id, term, priority_score))
    
    # å¯¹æ¯ä¸ªnamespaceï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºå¹¶é€‰æ‹©å‰max_terms_per_namespaceä¸ªterms
    for namespace, terms_list in namespace_terms.items():
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        terms_list.sort(key=lambda x: x[2], reverse=True)
        # é€‰æ‹©å‰max_terms_per_namespaceä¸ªterms
        selected_terms = terms_list[:max_terms_per_namespace]
        
        for go_id, term, _ in selected_terms:
            filtered_terms[go_id] = term
            namespace_counts[namespace] += 1
    
    print(f"ç­›é€‰ç»“æœç»Ÿè®¡ï¼š")
    for namespace, count in namespace_counts.items():
        print(f"  {namespace}: {count} terms")
    
    return filtered_terms

# ====== 4. æ„å»ºå’Œå­˜å‚¨ GO terms embeddings ======
def build_go_embeddings(collection, filtered_go):
    """æ„å»ºGO terms embeddingså¹¶å­˜å‚¨åˆ°ChromaDB"""
    print("æ­£åœ¨è®¡ç®—GO termsçš„embeddings...")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    try:
        count = collection.count()
        if count > 0:
            print(f"ChromaDBä¸­å·²æœ‰ {count} ä¸ªGO terms embeddings")
            return
    except:
        pass
    
        # æ„å»ºembeddings
    documents = []
    embeddings = []
    ids = []
    metadatas = []
    
    count = 0
    for go_id, term in filtered_go.items():
        text = term.name
        if hasattr(term, 'definition') and term.definition:
            text += " " + term.definition
        
        try:
            embedding = encode_text(text)
            
            documents.append(text)
            embeddings.append(embedding.tolist())
            ids.append(go_id)
            metadatas.append({
                "name": term.name,
                "namespace": term.namespace,
                "definition": term.definition if hasattr(term, 'definition') else ""
            })
            
            count += 1
            if count % 100 == 0:
                print(f"å·²å¤„ç† {count}/{len(filtered_go)} ä¸ªGO terms...")
                
        except Exception as e:
            print(f"è·³è¿‡ {go_id}: {e}")
            continue
    
    # æ‰¹é‡æ·»åŠ åˆ°ChromaDB
    if documents:
        collection.add(
            documents=documents,
            embeddings=embeddings,
            ids=ids,
            metadatas=metadatas
        )
        print(f"æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªGO terms embeddingsåˆ°ChromaDB")

# ====== 5. å½’ä¸€åŒ–å‡½æ•° ======
def normalize_scores(scores_dict):
        min_score = min(scores_dict.values())
        max_score = max(scores_dict.values())
        if max_score == min_score:
            # å¦‚æœæ‰€æœ‰åˆ†æ•°ç›¸åŒï¼Œç»™ä¸€ä¸ªå°çš„éšæœºæ‰°åŠ¨
            import random
            return {go_id: 0.5 + random.uniform(-0.1, 0.1) for go_id in scores_dict.keys()}
        
        normalized = {}
        for go_id, score in scores_dict.items():
            norm_score = 0.1 + 0.8 * (score - min_score) / (max_score - min_score)
            normalized[go_id] = norm_score
        return normalized
    
   

# ====== 6. ä½¿ç”¨ChromaDBé¢„æµ‹GO terms ======
def predict_go_terms_chromadb(collection, description: str, top_k=5, balance_namespaces=True):
    """ä½¿ç”¨ChromaDBé¢„æµ‹GO termsï¼Œæ”¯æŒå¹³è¡¡ä¸‰å¤§ç±»è¦†ç›–"""
    # åˆ†å‰²å¥å­
    sentences = split_sentences(description)
    print(f"åˆ†å‰²å¾—åˆ° {len(sentences)} ä¸ªå¥å­:")
    for i, sent in enumerate(sentences, 1):
        print(f"  {i}. {sent[:80]}...")
    print()
    
    # å¯¹æ¯ä¸ªå¥å­é¢„æµ‹GO terms
    all_scores = defaultdict(list)
    
    for i, sentence in enumerate(sentences, 1):
        print(f"å¤„ç†å¥å­ {i}: {sentence[:60]}...")
        
        # è®¡ç®—å½“å‰å¥å­çš„embedding
        try:
            sent_emb = encode_text(sentence)
        except Exception as e:
            print(f"  è·³è¿‡å¥å­ {i} (ç¼–ç å¤±è´¥): {e}")
            continue
        
        # ä½¿ç”¨ChromaDBæŸ¥è¯¢ç›¸ä¼¼GO terms
        try:
            results = collection.query(
                query_embeddings=[sent_emb.tolist()],
                n_results=top_k * 3,  # æŸ¥è¯¢æ›´å¤šç»“æœä»¥ä¾¿åç»­å¹³è¡¡
                include=["documents", "metadatas", "distances"]
            )
            
            # æ£€æŸ¥æŸ¥è¯¢ç»“æœ
            if not results["ids"] or not results["ids"][0]:
                print(f"    è­¦å‘Šï¼šå¥å­ {i} æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„GO terms")
                continue
                
            print(f"  å¥å­ {i} çš„top {len(results['ids'][0])} GO terms:")
            for j, (doc, metadata, distance) in enumerate(zip(
                results["documents"][0], 
                results["metadatas"][0], 
                results["distances"][0]
            )):
                go_id = results["ids"][0][j]
                # è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼šè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
                # ä½¿ç”¨æ›´æ¸©å’Œçš„è½¬æ¢æ–¹å¼ï¼Œé¿å…è·ç¦»è¿‡å¤§æ—¶scoreä¸º0
                score = 1 / (1 + distance/10)  # é™¤ä»¥10è®©è·ç¦»èŒƒå›´æ›´åˆç†
                print(f"    {go_id}: {metadata['name']} (distance={distance:.3f}, score={score:.3f}) [{metadata['namespace']}]")
                all_scores[go_id].append(score)
        except Exception as e:
            print(f"    é”™è¯¯ï¼šå¥å­ {i} æŸ¥è¯¢å¤±è´¥: {e}")
            continue
        print()
    
    # æ±‡æ€»æ‰€æœ‰å¥å­çš„ç»“æœ
    final_scores = {}
    for go_id, scores in all_scores.items():
        # ä½¿ç”¨æœ€é«˜åˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
        final_scores[go_id] = max(scores)
    
    if balance_namespaces:
        # å¹³è¡¡ä¸‰å¤§ç±»è¦†ç›–
        balanced_results = balance_namespace_coverage(collection, final_scores, top_k)
        return balanced_results
    else:
        # ä¼ ç»Ÿæ–¹å¼ï¼šç›´æ¥å–top_k
        print(f"\næ‰€æœ‰å€™é€‰GO termsåˆ†æ•°èŒƒå›´: {min(final_scores.values()):.3f} - {max(final_scores.values()):.3f}")
        top_k_scores = dict(sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_k])
        
        # å¯¹å‰ top_k ä¸ªè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿åˆ†å¸ƒæ›´åˆç†
        print(f"å‰{top_k}ä¸ªGO termså½’ä¸€åŒ–å‰åˆ†æ•°èŒƒå›´: {min(top_k_scores.values()):.3f} - {max(top_k_scores.values()):.3f}")
        normalized_scores = normalize_scores(top_k_scores)
        print(f"å‰{top_k}ä¸ªGO termså½’ä¸€åŒ–ååˆ†æ•°èŒƒå›´: {min(normalized_scores.values()):.3f} - {max(normalized_scores.values()):.3f}")
        
        # è¿”å›å½’ä¸€åŒ–åçš„ç»“æœï¼ˆä¿æŒæ’åºï¼‰
        ranked = sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)
    return ranked

def balance_namespace_coverage(collection, final_scores, top_k):
    """å¹³è¡¡ä¸‰å¤§ç±»GO termsçš„è¦†ç›–ï¼Œä½¿ç”¨namespaceå†…éƒ¨åˆ†æ•°æ­£åˆ™åŒ–"""
    # è·å–æ¯ä¸ªGO termçš„namespaceä¿¡æ¯
    go_namespaces = {}
    for go_id in final_scores.keys():
        try:
            # ä»collectionä¸­è·å–metadata
            results = collection.get(ids=[go_id], include=["metadatas"])
            if results["metadatas"]:
                go_namespaces[go_id] = results["metadatas"][0]["namespace"]
        except:
            continue
    
    # æŒ‰namespaceåˆ†ç»„
    namespace_scores = {
        'biological_process': [],
        'molecular_function': [],
        'cellular_component': []
    }
    
    for go_id, score in final_scores.items():
        namespace = go_namespaces.get(go_id, 'unknown')
        if namespace in namespace_scores:
            namespace_scores[namespace].append((go_id, score))
    
    # å¯¹æ¯ä¸ªnamespaceå†…éƒ¨è¿›è¡Œåˆ†æ•°æ­£åˆ™åŒ–
    print("\nå„namespaceåŸå§‹åˆ†æ•°ç»Ÿè®¡ï¼š")
    for namespace, terms_list in namespace_scores.items():
        if terms_list:
            scores = [score for _, score in terms_list]
            print(f"  {namespace}: {len(terms_list)} terms, åˆ†æ•°èŒƒå›´ {min(scores):.3f} - {max(scores):.3f}")
    
    # åœ¨æ¯ä¸ªnamespaceå†…éƒ¨è¿›è¡Œæ­£åˆ™åŒ–
    normalized_namespace_scores = {}
    for namespace, terms_list in namespace_scores.items():
        if not terms_list:
            continue
            
        # æå–åˆ†æ•°
        scores = [score for _, score in terms_list]
        go_ids = [go_id for go_id, _ in terms_list]
        
        # åœ¨namespaceå†…éƒ¨è¿›è¡Œæ­£åˆ™åŒ–
        normalized_scores = normalize_scores_within_namespace(scores)
        
        # é‡æ–°ç»„åˆ
        normalized_namespace_scores[namespace] = list(zip(go_ids, normalized_scores))
    
    print("\nå„namespaceæ­£åˆ™åŒ–ååˆ†æ•°ç»Ÿè®¡ï¼š")
    for namespace, terms_list in normalized_namespace_scores.items():
        if terms_list:
            scores = [score for _, score in terms_list]
            print(f"  {namespace}: {len(terms_list)} terms, åˆ†æ•°èŒƒå›´ {min(scores):.3f} - {max(scores):.3f}")
    
    # å¯¹æ¯ä¸ªnamespaceæŒ‰æ­£åˆ™åŒ–åçš„åˆ†æ•°æ’åº
    for namespace in normalized_namespace_scores:
        normalized_namespace_scores[namespace].sort(key=lambda x: x[1], reverse=True)
    
    # å¹³è¡¡é€‰æ‹©ï¼šæ¯ä¸ªnamespaceé€‰æ‹©ä¸€å®šæ•°é‡çš„terms
    balanced_terms = {}
    terms_per_namespace = max(1, top_k // 3)  # æ¯ä¸ªnamespaceè‡³å°‘1ä¸ªï¼Œå¹³å‡åˆ†é…
    
    for namespace, terms_list in normalized_namespace_scores.items():
        selected_terms = terms_list[:terms_per_namespace]
        for go_id, score in selected_terms:
            balanced_terms[go_id] = score
    
    # å¦‚æœè¿˜ä¸å¤Ÿtop_kä¸ªï¼Œä»å‰©ä½™çš„é«˜åˆ†termsä¸­è¡¥å……
    remaining_slots = top_k - len(balanced_terms)
    if remaining_slots > 0:
        # æ”¶é›†æ‰€æœ‰æœªé€‰ä¸­çš„terms
        remaining_terms = []
        for namespace, terms_list in normalized_namespace_scores.items():
            for go_id, score in terms_list[terms_per_namespace:]:
                remaining_terms.append((go_id, score))
        
        # æŒ‰æ­£åˆ™åŒ–åçš„åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é«˜çš„
        remaining_terms.sort(key=lambda x: x[1], reverse=True)
        for go_id, score in remaining_terms[:remaining_slots]:
            balanced_terms[go_id] = score
    
    # æœ€ç»ˆå½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒæ•´æœ€ç»ˆåˆ†æ•°èŒƒå›´ï¼‰
    if balanced_terms:
        print(f"\nå¹³è¡¡åçš„GO termsåˆ†æ•°èŒƒå›´: {min(balanced_terms.values()):.3f} - {max(balanced_terms.values()):.3f}")
        final_normalized_scores = normalize_scores(balanced_terms)
        print(f"æœ€ç»ˆå½’ä¸€åŒ–ååˆ†æ•°èŒƒå›´: {min(final_normalized_scores.values()):.3f} - {max(final_normalized_scores.values()):.3f}")
        
        # è¿”å›æ’åºç»“æœ
        ranked = sorted(final_normalized_scores.items(), key=lambda x: x[1], reverse=True)
        return ranked
    else:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•GO termsï¼Œè¿”å›ç©ºç»“æœ")
        return []

def normalize_scores_within_namespace(scores):
    """åœ¨å•ä¸ªnamespaceå†…éƒ¨è¿›è¡Œåˆ†æ•°æ­£åˆ™åŒ–"""
    if not scores:
        return []
    
    min_score = min(scores)
    max_score = max(scores)
    
    if max_score == min_score:
        # å¦‚æœæ‰€æœ‰åˆ†æ•°ç›¸åŒï¼Œç»™ä¸€ä¸ªå°çš„éšæœºæ‰°åŠ¨
        import random
        return [0.5 + random.uniform(-0.1, 0.1) for _ in scores]
    
    # ä½¿ç”¨Min-Maxå½’ä¸€åŒ–åˆ°[0.1, 0.9]èŒƒå›´
    normalized = []
    for score in scores:
        norm_score = 0.1 + 0.8 * (score - min_score) / (max_score - min_score)
        normalized.append(norm_score)
    
    return normalized

# ====== 7. è¯»å–å¤šæ™ºèƒ½ä½“åˆ†æç»“æœ ======
def read_multi_agent_result(result_file="Agents/CAFA/analysis_result_with_confidence.txt"):
    """è¯»å–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç»¼åˆåˆ†æç»“æœ"""
    if not os.path.exists(result_file):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶ {result_file}")
        return None
    
    with open(result_file, "r", encoding="utf-8") as f:
        content = f.read()
    
    # æå–ç»¼åˆåˆ†æéƒ¨åˆ†
    if "COMPREHENSIVE ANALYSIS" in content:
        # æ‰¾åˆ°ç»¼åˆåˆ†æéƒ¨åˆ†
        start_idx = content.find("COMPREHENSIVE ANALYSIS")
        if start_idx != -1:
            # ä»ç»¼åˆåˆ†æå¼€å§‹åˆ°ä¸‹ä¸€ä¸ªéƒ¨åˆ†æˆ–æ–‡ä»¶ç»“æŸ
            comprehensive_part = content[start_idx:]
            
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªéƒ¨åˆ†çš„å¼€å§‹ï¼ˆå¦‚CONFIDENCE SUMMARYï¼‰
            next_section_markers = ["CONFIDENCE SUMMARY", "ğŸ“Š CONFIDENCE SUMMARY"]
            end_idx = len(comprehensive_part)
            for marker in next_section_markers:
                marker_idx = comprehensive_part.find(marker)
                if marker_idx != -1 and marker_idx < end_idx:
                    end_idx = marker_idx
            
            # æå–ç»¼åˆåˆ†æéƒ¨åˆ†
            comprehensive_text = comprehensive_part[:end_idx]
            
            # æå–ç½®ä¿¡åº¦è¡Œä¹‹åçš„å†…å®¹
            lines = comprehensive_text.split('\n')
            result_lines = []
            found_content = False
            for line in lines:
                if "Confidence:" in line and "[" in line:
                    found_content = True
                    continue
                if found_content and line.strip():
                    result_lines.append(line)
            
            if result_lines:
                return '\n'.join(result_lines)
    
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ç»¼åˆåˆ†æç»“æœï¼Œå°è¯•è¯»å–æ•´ä¸ªæ–‡ä»¶å†…å®¹")
    return content

# ====== 8. ä¸»å‡½æ•° ======
def main():
    # åˆå§‹åŒ–ChromaDB
    collection = init_chromadb()
    
    # æ£€æŸ¥ChromaDBçŠ¶æ€
    try:
        count = collection.count()
        print(f"ChromaDBé›†åˆä¸­å…±æœ‰ {count} ä¸ªGO terms")
        if count == 0:
            print("è­¦å‘Šï¼šChromaDBä¸ºç©ºï¼Œéœ€è¦é‡æ–°æ„å»ºembeddings")
    except Exception as e:
        print(f"ChromaDBçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
    
    # ç­›é€‰GO termsï¼Œä½¿ç”¨åŸºäºé‡è¦æ€§çš„ç­›é€‰ç­–ç•¥
    print("æ­£åœ¨ç­›é€‰GO terms...")
    filtered_go = filter_go_terms(go, min_freq=1, exclude_iea=False, max_terms_per_namespace=1000)
    print(f"ç­›é€‰åå‰©ä½™ {len(filtered_go)} ä¸ªGO terms")
    
    # æ„å»ºembeddingsï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
    build_go_embeddings(collection, filtered_go)
    
    # å†æ¬¡æ£€æŸ¥ChromaDBçŠ¶æ€
    try:
        final_count = collection.count()
        print(f"æ„å»ºå®Œæˆåï¼ŒChromaDBä¸­å…±æœ‰ {final_count} ä¸ªGO terms")
    except Exception as e:
        print(f"æœ€ç»ˆChromaDBçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
    
    # è¯»å–å¤šæ™ºèƒ½ä½“åˆ†æç»“æœ
    description = read_multi_agent_result()
    if not description:
        print("æ— æ³•è¯»å–åˆ†æç»“æœï¼Œä½¿ç”¨ç¤ºä¾‹æè¿°")
        description = "This protein is involved in ATP binding and located in the nucleus."
    
    print("=" * 60)
    print("å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç»¼åˆåˆ†æç»“æœï¼š")
    print("=" * 60)
    print(description)
    print("=" * 60)
    
    # é¢„æµ‹GO terms
    print("\né¢„æµ‹çš„GO termsï¼ˆå¹³è¡¡ä¸‰å¤§ç±»è¦†ç›–ï¼‰ï¼š")
    print("-" * 60)
    preds = predict_go_terms_chromadb(collection, description, top_k=20, balance_namespaces=True)
    
    # ç»Ÿè®¡namespaceåˆ†å¸ƒ
    namespace_stats = {'biological_process': 0, 'molecular_function': 0, 'cellular_component': 0}
    
    for i, (go_id, score) in enumerate(preds, 1):
        term = go[go_id]
        namespace = term.namespace
        namespace_stats[namespace] = namespace_stats.get(namespace, 0) + 1
        
        print(f"{i:2d}. {go_id:12s} | {term.name:50s} | score={score:.3f} | [{namespace}]")
        if hasattr(term, 'definition') and term.definition:
            print(f"     {term.definition[:80]}...")
        print()
    
    print("Namespaceåˆ†å¸ƒç»Ÿè®¡ï¼š")
    for namespace, count in namespace_stats.items():
        print(f"  {namespace}: {count} terms")

if __name__ == "__main__":
    main()
